{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/simon/.virtualenvs/termnn/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.7\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "from keras.layers import Input, Dense, Lambda, concatenate, Conv1D, Concatenate, Flatten, MaxPooling1D\n",
    "from keras.models import Model\n",
    "import pandas as pd\n",
    "from itertools import islice\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up variables for constants such as absolute datapaths and the desired valdiation fraction split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = '/Users/simon/Downloads/Substituent_Data'\n",
    "classyfire_datapath = datapath + os.sep + \"For Substituent GNPS ALL\" + os.sep + \"GNPS Python Master\" + os.sep + \"Final Data.txt\"\n",
    "substituents_path = datapath+os.sep+\"Classyfire Taxanomy\"+os.sep+\"GNPS_substituents.txt\"\n",
    "synced_substituents_path =datapath+os.sep+\"Classyfire Taxanomy\"+os.sep+\"GNPS_synced_substituents.txt\"\n",
    "\n",
    "substituents_names_path = datapath+os.sep+\"Classyfire Taxanomy\"+os.sep+\"GNPS_substituents_legend.txt\"\n",
    "filtered_substituents_names_path = datapath + os.sep + \"after_filtering_score_and_occurences\"+os.sep+\"filtered_top_substituents_after_parameters_legend.txt\"\n",
    "substituents_auc_results_path = datapath+os.sep+\"Classyfire Taxanomy\"+os.sep+\"substituents_auc_results.txt\"\n",
    "num_samples = 9238\n",
    "numSubstituents = 381\n",
    "val_fraction = 0.1\n",
    "default_dpi = plt.rcParamsDefault['figure.dpi']\n",
    "plt.rcParams['figure.dpi'] = default_dpi*1.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below are methods used to load in fragment spectra and fingerprint data from files stored in the absolute paths specified above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads a master file containing peak intensities for all molecules.\n",
    "# Each molecule's spectrum is added as a 1000 element row to a Pandas dataframe\n",
    "# The dataframe is then converted into a numpy array for use as Keras Input.\n",
    "# Include the option of adding additonal features to each molecule (mass_shifts variable)\n",
    "def load_master_file(path, mass_shifts = 0, number_of_bins = 1000):\n",
    "    MAX_MASS = 1000\n",
    "    BIN_SIZE = 1\n",
    "    NUM_FEATURES = mass_shifts\n",
    "    mol_all = np.loadtxt(path, dtype=\"U30\") # Get master file in as numpy array\n",
    "    \n",
    "    mol_ids = np.unique(mol_all[:, 0])  # Trim duplicate filename rows, store unique filenames\n",
    "    # Construct empty Pandas dataframe of correct size.\n",
    "    # Number of rows is equal to the number of unique molecules (found in mol_ids).\n",
    "    intensities = pd.DataFrame(0.0, index = mol_ids, columns=range((number_of_bins//BIN_SIZE)+NUM_FEATURES), dtype=float)\n",
    "    \n",
    "    # Populate the dataframe using each molecule's filename to place data in the correct row.\n",
    "    for row in mol_all:\n",
    "        intensities.at[row[0], float(row[1])-1] = float(row[2])\n",
    "    \n",
    "    # Convert populated dataframe into a numpy array for use by neural networks.\n",
    "    np_matrix = intensities.values\n",
    "    return np_matrix\n",
    "\n",
    "# Load a master file containing CDK fingerprints for all molecules.\n",
    "# Each molecules CDK bit set is added as a 320 element array to a Pandas dataframe.\n",
    "def load_substituents_master(path):\n",
    "    BITS = 444  # Total number of bits in fingerprint\n",
    "\n",
    "    fp_all = np.loadtxt(path, dtype=\"U30\") # Get master file as numpy array of Strings\n",
    "    fp_ids = np.unique(fp_all[:, 0]) # Trim duplicate filename rows, store unique filenames\n",
    "\n",
    "    # Construct empty Pandas dataframe of correct size.\n",
    "    # Number of rows is equal to the number of unique molecules (found in fp_ids).\n",
    "    substituents = pd.DataFrame(0, index = fp_ids, columns=range(BITS), dtype=int)\n",
    "\n",
    "    # Populate the dataframe using each molecule's filename to place data in the correct row.\n",
    "    for row in fp_all:\n",
    "        substituents.at[row[0], int(row[1])] = int(row[2])\n",
    "\n",
    "    # Convert populated dataframe into a numpy array for use as output by neural networks.\n",
    "    \n",
    "    np_matrix = substituents\n",
    "    return np_matrix\n",
    "\n",
    "# Load the names of all substituents included in the correct order\n",
    "# This is used for boxplots, when performance metrics for individual substituent are calculated.\n",
    "def load_substituent_legend(path):\n",
    "    substituent_legend = []\n",
    "    # Open file containing substituent names.\n",
    "    with open(path, 'r') as f:\n",
    "        # Add each name to the list of substituent names.\n",
    "        lines = list(islice(f, 0, None))\n",
    "        for line in lines:\n",
    "            substituent_legend.append(line[:-1])\n",
    "    return substituent_legend\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below methods create and train various neural networks when provided with valid input and output data. They allow for specifying the number of epochs the network is to be trained for and, in some cases, the learning rate. Trained models are returned and can be used to predict on test data and thereby be evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Sotchastic Gradient Descent object from Keras to allow for tweaking its learning rate.\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# A simplified spectrum-fingeprrint encoder.\n",
    "# Structure: (Input)1000-500-200-2098(Output)\n",
    "def simplified_substituent_model(x_train_spectra, x_train_substituents, epochs=100):\n",
    "    # Create input based on the provided x_train data structure.\n",
    "    input_layer = Input(shape=(x_train_spectra.shape[1],))\n",
    "    # Since output is not the same as input, we obtain its shape separately.\n",
    "    output_dims = x_train_substituents.shape[1]\n",
    "    print(output_dims)\n",
    "    # Create the encoding layers using functional API.\n",
    "    l = input_layer\n",
    "    l = Dense(500, activation='relu')(l)\n",
    "    \n",
    "    # Linear activation ensures that values can be negative (necessary for sigmoid to function)\n",
    "    l = Dense(200, activation='linear')(l)\n",
    "    \n",
    "    # Save reference to latent space\n",
    "    latent_space = l\n",
    "    \n",
    "    # Sigmoid activation to get outputs between 0 and 1. This is done because the output fingerprint is a set of bits (0 or 1).\n",
    "    l2 = Dense(output_dims, activation='sigmoid')(l)\n",
    "    \n",
    "    #Reference for output layer\n",
    "    out_layer = l2\n",
    "\n",
    "    auto_model = Model(input=input_layer, output=out_layer)\n",
    "    \n",
    "    # Set SGD learning rate = 0.05 and compile model with binary_crossentropy as loss function.\n",
    "    sgd = SGD(lr=0.05)\n",
    "    auto_model.compile(loss='binary_crossentropy', optimizer=sgd)\n",
    "    \n",
    "    # Train the model for the specified number of epochs, using the specified validation fraction.\n",
    "    autoencoder_train = auto_model.fit(x_train_spectra, x_train_substituents, shuffle=False, validation_split = 0.1, epochs=epochs)\n",
    "    \n",
    "    # Loss Plots\n",
    "    plot_loss(autoencoder_train, epochs)\n",
    "    \n",
    "    return auto_model # Return model, now trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes as input a trained neural network model and extracts its history variable.\n",
    "# It then uses it to graph the model's loss and validation loss over the training epochs\n",
    "# The epochs paramter is used for plotting the x axis.\n",
    "def plot_loss(fitted_model, epochs):\n",
    "    # Extract loss values for the training and validation sets.\n",
    "    loss = fitted_model.history['loss']\n",
    "    val_loss = fitted_model.history['val_loss']\n",
    "    # Create x axis variables.\n",
    "    epochs_label = epochs\n",
    "    epochs = range(epochs)\n",
    "\n",
    "    #Plot both losses.\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "    plt.plot(epochs, val_loss,'r', label='Validation loss')\n",
    "    plt.title('Training and Validation Loss for ' + str(epochs_label) + ' epochs')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "# Takes a actual and predicted fingerprint values and computes the area under the Roc curve for each substructure.\n",
    "# For each subtructure, also calculates AUC when the actual values are scrambled.\n",
    "# Return two numpy arrays: one containing AUC metrics for all susbtructures, one containing each permutation's\n",
    "# AUC scores for each susbtructure.\n",
    "from sympy.utilities.iterables import multiset_permutations\n",
    "def compute_auc(bits, true, pred, num_samples=0, permutations=500):\n",
    "    val_start_index = int(num_samples-(num_samples*val_fraction)-1) # Index where validation samples begin.\n",
    "    \n",
    "    num_permutations = permutations  # Number of permutations to compute AUC scores for. \n",
    "    \n",
    "    # Create structured array to hold statistics for each fingerprint.\n",
    "    dtype = [('fp_id', int),('nonzeros', int), ('auc', float), ('auc_percent', float)]\n",
    "    mol_stats = np.zeros((bits,), dtype=dtype)\n",
    "\n",
    "    # Create array to hold permutation AUC scores for plotting.\n",
    "    perm_scores = np.zeros((bits, num_permutations))\n",
    "    val_start_index = 0\n",
    "    for fp_id in range(true.shape[1]): # For every substructure\n",
    "        nonzero_vals = np.count_nonzero(true[val_start_index:, fp_id]) # Count number of nonzero values\n",
    "        if nonzero_vals > 0 and nonzero_vals < true[val_start_index:, fp_id].size:  # If there are no 1s or no 0s, can't compute.\n",
    "            # Compute actual AUC score using only the validation fraction of the dataset.\n",
    "            fp_true = true[val_start_index:, fp_id]\n",
    "            fp_pred = pred[val_start_index:, fp_id]\n",
    "            score = metrics.roc_auc_score(fp_true, fp_pred)\n",
    "\n",
    "            # Compute AUC scores for permutations and compare to actual.\n",
    "            counter = 0         \n",
    "            for i in range(num_permutations):\n",
    "                permutation = np.random.permutation(fp_true)\n",
    "                perm_score = metrics.roc_auc_score(permutation, fp_pred)\n",
    "                perm_scores[fp_id, i] = perm_score\n",
    "                # Count how many permutations have a higer AUC score than actual data.\n",
    "                if perm_score >= score:\n",
    "                    counter = counter + 1\n",
    "            # Calculate % of scrambled values with higher AUC score than actual AUC\n",
    "            percentage = (counter/num_permutations)*100\n",
    "        # Update structured array with data or non values if no AUC could be calculated.\n",
    "            mol_stats[fp_id] = fp_id, nonzero_vals, score, percentage\n",
    "        else:\n",
    "            mol_stats[fp_id] = (fp_id, nonzero_vals, 0, 100)\n",
    "        \n",
    "    # Permutations take a while, print statement to say when finished.\n",
    "    print(\"Compute AUC Done\")\n",
    "    return mol_stats, perm_scores\n",
    "\n",
    "\n",
    "# Takes a set of AUC scores and permutation AUC scores (normally output by compute_auc above) an uses them\n",
    "# to draw boxplots for specified susbtructures. Actual AUC is plotted as a coloured dot.\n",
    "plt.rcParams['figure.dpi'] = default_dpi*2.2\n",
    "def boxplots(real_stats, perm_stats, sample_fps):\n",
    "    index = sample_fps['fp_id']  # Grab id of each substructure to be plotted, used as index in parallel arrays\n",
    "    names = np.array(substituent_names)[index]  # Grab name of each susbtructure to be plotted.\n",
    "\n",
    "    plt.rcParams.update({'font.size': 6})\n",
    "    plt.figure()\n",
    "    plt.boxplot(perm_stats[index].T, vert=False, labels = names) # Boxplot permutation AUC scores\n",
    "    plt.scatter(real_stats[index]['auc'], range(1, len(index)+1)) # Scatter plot actual AUC scores for substructures in colour.\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "# Takes a set of AUC scores and permutation AUC scores and uses them to draw boxplots for specified substructures\n",
    "# Actual AUC is plotted as a coloured dot. A separate set of AUC scores\n",
    "# computed for prediction from a different model is also plotted for comparison\n",
    "def tandem_boxplots(real_stats, perm_stats, exp_stats, sample_fps):\n",
    "    index = sample_fps['fp_id']  # Grab id of each substructure to be plotted, used as index in parallel arrays\n",
    "    names = np.array(substituent_names)[index]  # Grab name of each susbtructure to be plotted.\n",
    "  \n",
    "    plt.rcParams.update({'font.size': 6})\n",
    "    plt.figure()\n",
    "    plt.boxplot(perm_stats[index].T, vert=False, labels = names) # Boxplot permutation AUC scores\n",
    "    plt.scatter(real_stats[index]['auc'], range(1, len(index)+1)) # Scatter plot actual AUC scores for substructures\n",
    "    plt.scatter(exp_stats[index]['auc'], range(1, len(index)+1), color = 'r') # Scatter plot AUC scores to be compared to.\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Given the AUC statistics derived from two separate models, it comapres the two models' performance\n",
    "# Creates a bar chart comparing substructures above an AUC threshold and draws boxplots for each model's best and worst\n",
    "# performing substructures.\n",
    "# Usually compares an experimental model's AUC to a baseline (e.g. the basic fingerprint encoder)\n",
    "def evaluate(base_stats, base_perm_scores, exp_stats, exp_perm_scores):\n",
    "    # Sort molecules in ascending order of baseline AUC score, keeping only molecules with AUC scores above 0.5\n",
    "    normal_auc = np.where((base_stats['auc'] > 0.5))\n",
    "    abnormal_auc = np.where((base_stats['auc']) < 0.5)\n",
    "    ordered_base = np.sort(base_stats[normal_auc], order='auc', axis=0)[::-1]\n",
    "    \n",
    "    # Take top 30 and bottom 5 substructures by AUC score to use for boxplots.\n",
    "    sample_fps = ordered_base[:30]\n",
    "    sample_fps = np.append(sample_fps, ordered_base[-5:])\n",
    "    \n",
    "    # Plot number of substructures with AUC scores above 0.7 and above 0.5 for both data sets\n",
    "    base_above_07 = len(np.where((base_stats['auc'] >= 0.7))[0])\n",
    "    exp_above_07 = len(np.where((exp_stats['auc'] >= 0.7))[0])\n",
    "    base_above_05 = len(np.where((base_stats['auc'] >= 0.5))[0])\n",
    "    exp_above_05 = len(np.where((exp_stats['auc'] >= 0.5))[0])\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    index = np.arange(2)\n",
    "    bar_width = 0.35\n",
    "    opacity = 0.5\n",
    "    ax.bar(index, (base_above_05, base_above_07), bar_width, alpha=opacity, color='b', label='Baseline')\n",
    "    ax.bar(index+bar_width, (exp_above_05, exp_above_07), bar_width, alpha=opacity, color='r', label='Experiment')\n",
    "    \n",
    "    ax.set_xlabel('AUC Threshold')\n",
    "    ax.set_ylabel('Number of Substituents')\n",
    "    ax.set_title('AUC Score Comparison')\n",
    "    ax.set_xticks(index + bar_width / 2)\n",
    "    ax.set_xticklabels(('Above 0.5', 'Above 0.7'))\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Boxplots of sample substructures for both data sets\n",
    "    boxplots(base_stats, base_perm_scores, sample_fps)\n",
    "    boxplots(exp_stats, exp_perm_scores, sample_fps)\n",
    "    tandem_boxplots(base_stats, base_perm_scores, exp_stats, sample_fps)\n",
    "    \n",
    "     # Sort molecules in ascending order of experimental AUC score, keeping only molecules with AUC scores above 0.5\n",
    "    normal_auc = np.where((exp_stats['auc'] > 0.5))\n",
    "    abnormal_auc = np.where((exp_stats['auc']) < 0.5)\n",
    "    ordered_exp = np.sort(exp_stats[normal_auc], order='auc', axis=0)[::-1]\n",
    "    \n",
    "    # Take top 30 and bottom 5 substructures by AUC score to use for boxplots.\n",
    "    sample_fps = ordered_exp[:30]\n",
    "    sample_fps = np.append(sample_fps, ordered_exp[-5:])\n",
    "    \n",
    "    boxplots(base_stats, base_perm_scores, sample_fps)\n",
    "    boxplots(exp_stats, exp_perm_scores, sample_fps)\n",
    "    tandem_boxplots(base_stats, exp_perm_scores, exp_stats, sample_fps)\n",
    "    \n",
    "\n",
    "# Given a matrix of layer weights, plots them in a Hinton diagram: each weight is a box\n",
    "# Box size is indicates absolute value, box colour indicates sign (white for positive, black for negative)\n",
    "# Adapted from matplotlib documentation.\n",
    "def hinton(matrix, max_weight=None, ax=None):\n",
    "    ax = ax if ax is not None else plt.gca()\n",
    "    # Find maximum weight in matrix.\n",
    "    if not max_weight:\n",
    "        max_weight = 2 ** np.ceil(np.log(np.abs(matrix).max()) / np.log(2))\n",
    "\n",
    "    ax.patch.set_facecolor('gray')\n",
    "    ax.set_aspect('equal', 'box')\n",
    "    ax.xaxis.set_major_locator(plt.NullLocator())\n",
    "    ax.yaxis.set_major_locator(plt.NullLocator())\n",
    "\n",
    "    # Plot weights as black or white boxes.\n",
    "    for (x, y), w in np.ndenumerate(matrix):\n",
    "        color = 'white' if w > 0 else 'black'\n",
    "        size = np.sqrt(np.abs(w) / max_weight)\n",
    "        rect = plt.Rectangle([x - size / 2, y - size / 2], size, size,\n",
    "                             facecolor=color, edgecolor=color)\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    ax.autoscale_view()\n",
    "    ax.invert_yaxis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following methods are quick ways to train multiple times using different training-validation splits. Used when we want means, error bars and statistical tests for model comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trains the basic substituent encoder using 10 different validation-training splits.\n",
    "# Computes AUC scores for each split and stores them as a separate file.\n",
    "# Takes path to store files in as a parameter, as well as the name of the test.\n",
    "def train_diff_splits(path, name, splits=10):\n",
    "    # Extract permuted indices for dataset.\n",
    "    index_path = \"G:\\\\Dev\\\\Data\\\\GNPS ALL Substituent Validation Split Permutations.txt\"\n",
    "    permuted_indices = np.loadtxt(index_path, dtype=int, delimiter=',')\n",
    "    epochs = 100\n",
    "    path = path + name + \" \"\n",
    "    #List to store AUC scores if we want to use them right away instead of loading from files.\n",
    "    experiment_stats = []\n",
    "    for i in range(splits):\n",
    "        # Create filepath for this training session.\n",
    "        curr_path = path + str(i) + \".txt\"\n",
    "        # Use permuted indices to create permuted array of input data.\n",
    "        x_train_dense = spectra[permuted_indices[:, i]]\n",
    "        x_train_dense = np.log(x_train_dense+1)\n",
    "        x_train_substituents = substituents.values[permuted_indices[:, i]]\n",
    "        # Train a basic model.\n",
    "        enc_basic = simplified_substituent_model(x_train_dense, x_train_substituents, epochs=100)\n",
    "        # Use trained model to compute AUC scores for substructures and save them to disc.\n",
    "        actual = x_train_substituents\n",
    "        predicted = enc_basic.predict(x_train_dense)\n",
    "        base_stats, base_perm_scores = compute_auc(2098, actual, predicted, num_samples=10038)\n",
    "        \n",
    "        np.savetxt(curr_path, base_stats, fmt=['%d', '%d', '%f', '%f'])\n",
    "        experiment_stats.append(base_stats)\n",
    "    \n",
    "    evaluate(base_stats, base_perm_scores, base_stats, base_perm_scores)\n",
    "    \n",
    "    return experiment_stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The below is the basic set up for running any of the methods. It loads the fragment spectra and subtituent terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "spectra = load_master_file(path=classyfire_datapath)\n",
    "substituents = load_substituents_master(synced_substituents_path)\n",
    "substituent_names = load_substituent_legend(filtered_substituents_names_path)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CherWei\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:28: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9034 samples, validate on 1004 samples\n",
      "Epoch 1/400\n",
      "9034/9034 [==============================] - 3s 347us/step - loss: 0.4713 - val_loss: 0.1576\n",
      "Epoch 2/400\n",
      "9034/9034 [==============================] - 3s 309us/step - loss: 0.1287 - val_loss: 0.0621\n",
      "Epoch 3/400\n",
      "9034/9034 [==============================] - 3s 341us/step - loss: 0.0859 - val_loss: 0.0535\n",
      "Epoch 4/400\n",
      "9034/9034 [==============================] - 3s 316us/step - loss: 0.0748 - val_loss: 0.0511\n",
      "Epoch 5/400\n",
      "9034/9034 [==============================] - 3s 375us/step - loss: 0.0694 - val_loss: 0.0501\n",
      "Epoch 6/400\n",
      "9034/9034 [==============================] - 3s 328us/step - loss: 0.0662 - val_loss: 0.0495\n",
      "Epoch 7/400\n",
      "9034/9034 [==============================] - 3s 328us/step - loss: 0.0640 - val_loss: 0.0491\n",
      "Epoch 8/400\n",
      "9034/9034 [==============================] - 3s 317us/step - loss: 0.0624 - val_loss: 0.0488\n",
      "Epoch 9/400\n",
      "9034/9034 [==============================] - 3s 315us/step - loss: 0.0611 - val_loss: 0.0485\n",
      "Epoch 10/400\n",
      "9034/9034 [==============================] - 3s 319us/step - loss: 0.0601 - val_loss: 0.0482\n",
      "Epoch 11/400\n",
      "9034/9034 [==============================] - 3s 315us/step - loss: 0.0592 - val_loss: 0.0480\n",
      "Epoch 12/400\n",
      "9034/9034 [==============================] - 3s 314us/step - loss: 0.0584 - val_loss: 0.0478\n",
      "Epoch 13/400\n",
      "9034/9034 [==============================] - 3s 341us/step - loss: 0.0577 - val_loss: 0.0476\n",
      "Epoch 14/400\n",
      "9034/9034 [==============================] - 4s 419us/step - loss: 0.0571 - val_loss: 0.0473\n",
      "Epoch 15/400\n",
      "9034/9034 [==============================] - 5s 584us/step - loss: 0.0565 - val_loss: 0.0471\n",
      "Epoch 16/400\n",
      "9034/9034 [==============================] - 6s 626us/step - loss: 0.0560 - val_loss: 0.0469\n",
      "Epoch 17/400\n",
      "9034/9034 [==============================] - 3s 342us/step - loss: 0.0555 - val_loss: 0.0467\n",
      "Epoch 18/400\n",
      "9034/9034 [==============================] - 3s 329us/step - loss: 0.0550 - val_loss: 0.0465\n",
      "Epoch 19/400\n",
      "9034/9034 [==============================] - 3s 335us/step - loss: 0.0545 - val_loss: 0.0463\n",
      "Epoch 20/400\n",
      "9034/9034 [==============================] - 3s 335us/step - loss: 0.0541 - val_loss: 0.0461\n",
      "Epoch 21/400\n",
      "9034/9034 [==============================] - 3s 358us/step - loss: 0.0537 - val_loss: 0.0459\n",
      "Epoch 22/400\n",
      "9034/9034 [==============================] - 3s 363us/step - loss: 0.0533 - val_loss: 0.0457\n",
      "Epoch 23/400\n",
      "9034/9034 [==============================] - 3s 356us/step - loss: 0.0529 - val_loss: 0.0455\n",
      "Epoch 24/400\n",
      "9034/9034 [==============================] - 3s 361us/step - loss: 0.0526 - val_loss: 0.0454\n",
      "Epoch 25/400\n",
      "9034/9034 [==============================] - 3s 338us/step - loss: 0.0522 - val_loss: 0.0452\n",
      "Epoch 26/400\n",
      "9034/9034 [==============================] - 3s 342us/step - loss: 0.0519 - val_loss: 0.0450\n",
      "Epoch 27/400\n",
      "9034/9034 [==============================] - 3s 339us/step - loss: 0.0516 - val_loss: 0.0448\n",
      "Epoch 28/400\n",
      "9034/9034 [==============================] - 3s 350us/step - loss: 0.0513 - val_loss: 0.0447\n",
      "Epoch 29/400\n",
      "9034/9034 [==============================] - 3s 334us/step - loss: 0.0510 - val_loss: 0.0445\n",
      "Epoch 30/400\n",
      "9034/9034 [==============================] - 3s 340us/step - loss: 0.0507 - val_loss: 0.0444\n",
      "Epoch 31/400\n",
      "9034/9034 [==============================] - 4s 461us/step - loss: 0.0505 - val_loss: 0.0442\n",
      "Epoch 32/400\n",
      "9034/9034 [==============================] - 4s 405us/step - loss: 0.0502 - val_loss: 0.0441\n",
      "Epoch 33/400\n",
      "9034/9034 [==============================] - 3s 376us/step - loss: 0.0500 - val_loss: 0.0439\n",
      "Epoch 34/400\n",
      "9034/9034 [==============================] - 3s 364us/step - loss: 0.0497 - val_loss: 0.0438\n",
      "Epoch 35/400\n",
      "9034/9034 [==============================] - 3s 359us/step - loss: 0.0495 - val_loss: 0.0437\n",
      "Epoch 36/400\n",
      "9034/9034 [==============================] - 3s 362us/step - loss: 0.0493 - val_loss: 0.0436\n",
      "Epoch 37/400\n",
      "9034/9034 [==============================] - 3s 365us/step - loss: 0.0491 - val_loss: 0.0434\n",
      "Epoch 38/400\n",
      "9034/9034 [==============================] - 3s 359us/step - loss: 0.0489 - val_loss: 0.0433\n",
      "Epoch 39/400\n",
      "9034/9034 [==============================] - 3s 363us/step - loss: 0.0487 - val_loss: 0.0432\n",
      "Epoch 40/400\n",
      "9034/9034 [==============================] - 3s 363us/step - loss: 0.0485 - val_loss: 0.0431\n",
      "Epoch 41/400\n",
      "9034/9034 [==============================] - 3s 377us/step - loss: 0.0483 - val_loss: 0.0430\n",
      "Epoch 42/400\n",
      "9034/9034 [==============================] - 4s 394us/step - loss: 0.0481 - val_loss: 0.0429\n",
      "Epoch 43/400\n",
      "9034/9034 [==============================] - 3s 369us/step - loss: 0.0480 - val_loss: 0.0428\n",
      "Epoch 44/400\n",
      "9034/9034 [==============================] - 3s 368us/step - loss: 0.0478 - val_loss: 0.0427\n",
      "Epoch 45/400\n",
      "9034/9034 [==============================] - 3s 362us/step - loss: 0.0476 - val_loss: 0.0426\n",
      "Epoch 46/400\n",
      "9034/9034 [==============================] - 3s 369us/step - loss: 0.0475 - val_loss: 0.0425\n",
      "Epoch 47/400\n",
      "9034/9034 [==============================] - 3s 360us/step - loss: 0.0473 - val_loss: 0.0424\n",
      "Epoch 48/400\n",
      "9034/9034 [==============================] - 3s 366us/step - loss: 0.0472 - val_loss: 0.0423\n",
      "Epoch 49/400\n",
      "9034/9034 [==============================] - 3s 364us/step - loss: 0.0470 - val_loss: 0.0423\n",
      "Epoch 50/400\n",
      "9034/9034 [==============================] - 3s 367us/step - loss: 0.0469 - val_loss: 0.0422\n",
      "Epoch 51/400\n",
      "9034/9034 [==============================] - 3s 385us/step - loss: 0.0468 - val_loss: 0.0421\n",
      "Epoch 52/400\n",
      "9034/9034 [==============================] - 3s 373us/step - loss: 0.0466 - val_loss: 0.0420\n",
      "Epoch 53/400\n",
      "9034/9034 [==============================] - 3s 374us/step - loss: 0.0465 - val_loss: 0.0419\n",
      "Epoch 54/400\n",
      "9034/9034 [==============================] - 3s 379us/step - loss: 0.0464 - val_loss: 0.0419\n",
      "Epoch 55/400\n",
      "9034/9034 [==============================] - 3s 379us/step - loss: 0.0462 - val_loss: 0.0418\n",
      "Epoch 56/400\n",
      "9034/9034 [==============================] - 3s 369us/step - loss: 0.0461 - val_loss: 0.0417\n",
      "Epoch 57/400\n",
      "9034/9034 [==============================] - ETA: 0s - loss: 0.046 - 3s 367us/step - loss: 0.0460 - val_loss: 0.0417\n",
      "Epoch 58/400\n",
      "9034/9034 [==============================] - 3s 368us/step - loss: 0.0459 - val_loss: 0.0416\n",
      "Epoch 59/400\n",
      "9034/9034 [==============================] - 4s 389us/step - loss: 0.0457 - val_loss: 0.0415\n",
      "Epoch 60/400\n",
      "9034/9034 [==============================] - 3s 385us/step - loss: 0.0456 - val_loss: 0.0415\n",
      "Epoch 61/400\n",
      "9034/9034 [==============================] - 4s 396us/step - loss: 0.0455 - val_loss: 0.0414\n",
      "Epoch 62/400\n",
      "9034/9034 [==============================] - 3s 378us/step - loss: 0.0454 - val_loss: 0.0413\n",
      "Epoch 63/400\n",
      "9034/9034 [==============================] - 3s 383us/step - loss: 0.0453 - val_loss: 0.0413\n",
      "Epoch 64/400\n",
      "9034/9034 [==============================] - 4s 390us/step - loss: 0.0452 - val_loss: 0.0412\n",
      "Epoch 65/400\n",
      "9034/9034 [==============================] - 4s 394us/step - loss: 0.0451 - val_loss: 0.0411A: 0s - \n",
      "Epoch 66/400\n",
      "9034/9034 [==============================] - 4s 401us/step - loss: 0.0450 - val_loss: 0.0411\n",
      "Epoch 67/400\n",
      "9034/9034 [==============================] - 3s 376us/step - loss: 0.0449 - val_loss: 0.0410\n",
      "Epoch 68/400\n",
      "9034/9034 [==============================] - 4s 414us/step - loss: 0.0448 - val_loss: 0.0410\n",
      "Epoch 69/400\n",
      "9034/9034 [==============================] - 3s 385us/step - loss: 0.0447 - val_loss: 0.0409\n",
      "Epoch 70/400\n",
      "9034/9034 [==============================] - 3s 367us/step - loss: 0.0446 - val_loss: 0.0409\n",
      "Epoch 71/400\n",
      "9034/9034 [==============================] - 3s 379us/step - loss: 0.0445 - val_loss: 0.0408\n",
      "Epoch 72/400\n",
      "9034/9034 [==============================] - 4s 398us/step - loss: 0.0444 - val_loss: 0.0408\n",
      "Epoch 73/400\n",
      "9034/9034 [==============================] - 3s 383us/step - loss: 0.0443 - val_loss: 0.0407\n",
      "Epoch 74/400\n",
      "9034/9034 [==============================] - 3s 374us/step - loss: 0.0442 - val_loss: 0.0407\n",
      "Epoch 75/400\n",
      "9034/9034 [==============================] - 3s 380us/step - loss: 0.0441 - val_loss: 0.0406\n",
      "Epoch 76/400\n",
      "9034/9034 [==============================] - 3s 385us/step - loss: 0.0440 - val_loss: 0.0406\n",
      "Epoch 77/400\n",
      "9034/9034 [==============================] - 3s 326us/step - loss: 0.0439 - val_loss: 0.0405\n",
      "Epoch 78/400\n",
      "9034/9034 [==============================] - 3s 314us/step - loss: 0.0438 - val_loss: 0.0405\n",
      "Epoch 79/400\n",
      "9034/9034 [==============================] - 3s 318us/step - loss: 0.0437 - val_loss: 0.0404\n",
      "Epoch 80/400\n",
      "9034/9034 [==============================] - 3s 326us/step - loss: 0.0437 - val_loss: 0.0404\n",
      "Epoch 81/400\n",
      "9034/9034 [==============================] - 3s 323us/step - loss: 0.0436 - val_loss: 0.0403\n",
      "Epoch 82/400\n",
      "9034/9034 [==============================] - 3s 327us/step - loss: 0.0435 - val_loss: 0.0403\n",
      "Epoch 83/400\n",
      "9034/9034 [==============================] - 3s 318us/step - loss: 0.0434 - val_loss: 0.0403\n",
      "Epoch 84/400\n",
      "9034/9034 [==============================] - 3s 315us/step - loss: 0.0433 - val_loss: 0.0402\n",
      "Epoch 85/400\n",
      "9034/9034 [==============================] - 3s 314us/step - loss: 0.0433 - val_loss: 0.0402\n",
      "Epoch 86/400\n",
      "9034/9034 [==============================] - 3s 315us/step - loss: 0.0432 - val_loss: 0.0401\n",
      "Epoch 87/400\n",
      "9034/9034 [==============================] - 3s 335us/step - loss: 0.0431 - val_loss: 0.0401\n",
      "Epoch 88/400\n",
      "9034/9034 [==============================] - 3s 317us/step - loss: 0.0430 - val_loss: 0.0401\n",
      "Epoch 89/400\n",
      "9034/9034 [==============================] - 3s 320us/step - loss: 0.0430 - val_loss: 0.0400\n",
      "Epoch 90/400\n",
      "9034/9034 [==============================] - 3s 317us/step - loss: 0.0429 - val_loss: 0.0400\n",
      "Epoch 91/400\n",
      "9034/9034 [==============================] - 3s 316us/step - loss: 0.0428 - val_loss: 0.0400\n",
      "Epoch 92/400\n",
      "9034/9034 [==============================] - 3s 327us/step - loss: 0.0427 - val_loss: 0.0399\n",
      "Epoch 93/400\n",
      "9034/9034 [==============================] - 3s 328us/step - loss: 0.0427 - val_loss: 0.0399\n",
      "Epoch 94/400\n",
      "9034/9034 [==============================] - 3s 319us/step - loss: 0.0426 - val_loss: 0.0398\n",
      "Epoch 95/400\n",
      "9034/9034 [==============================] - 3s 322us/step - loss: 0.0425 - val_loss: 0.0398\n",
      "Epoch 96/400\n",
      "9034/9034 [==============================] - 3s 317us/step - loss: 0.0425 - val_loss: 0.0398\n",
      "Epoch 97/400\n",
      "9034/9034 [==============================] - 3s 333us/step - loss: 0.0424 - val_loss: 0.0398\n",
      "Epoch 98/400\n",
      "9034/9034 [==============================] - 4s 400us/step - loss: 0.0423 - val_loss: 0.0397\n",
      "Epoch 99/400\n",
      "9034/9034 [==============================] - 4s 441us/step - loss: 0.0422 - val_loss: 0.0397\n",
      "Epoch 100/400\n",
      "9034/9034 [==============================] - 4s 417us/step - loss: 0.0422 - val_loss: 0.0397\n",
      "Epoch 101/400\n",
      "9034/9034 [==============================] - 3s 342us/step - loss: 0.0421 - val_loss: 0.0396\n",
      "Epoch 102/400\n",
      "9034/9034 [==============================] - 3s 330us/step - loss: 0.0421 - val_loss: 0.0396\n",
      "Epoch 103/400\n",
      "9034/9034 [==============================] - 3s 321us/step - loss: 0.0420 - val_loss: 0.0396\n",
      "Epoch 104/400\n",
      "9034/9034 [==============================] - 3s 342us/step - loss: 0.0419 - val_loss: 0.0395\n",
      "Epoch 105/400\n",
      "9034/9034 [==============================] - 3s 376us/step - loss: 0.0419 - val_loss: 0.0395\n",
      "Epoch 106/400\n",
      "9034/9034 [==============================] - 4s 435us/step - loss: 0.0418 - val_loss: 0.0395\n",
      "Epoch 107/400\n",
      "9034/9034 [==============================] - 4s 409us/step - loss: 0.0417 - val_loss: 0.0395\n",
      "Epoch 108/400\n",
      "9034/9034 [==============================] - 3s 380us/step - loss: 0.0417 - val_loss: 0.0394\n",
      "Epoch 109/400\n",
      "9034/9034 [==============================] - 3s 353us/step - loss: 0.0416 - val_loss: 0.0394\n",
      "Epoch 110/400\n",
      "9034/9034 [==============================] - 3s 351us/step - loss: 0.0416 - val_loss: 0.0394\n",
      "Epoch 111/400\n",
      "9034/9034 [==============================] - 3s 338us/step - loss: 0.0415 - val_loss: 0.0394\n",
      "Epoch 112/400\n",
      "9034/9034 [==============================] - 3s 327us/step - loss: 0.0414 - val_loss: 0.0393\n",
      "Epoch 113/400\n",
      "9034/9034 [==============================] - 3s 337us/step - loss: 0.0414 - val_loss: 0.0393\n",
      "Epoch 114/400\n",
      "9034/9034 [==============================] - 3s 328us/step - loss: 0.0413 - val_loss: 0.0393\n",
      "Epoch 115/400\n",
      "9034/9034 [==============================] - 3s 330us/step - loss: 0.0413 - val_loss: 0.0393\n",
      "Epoch 116/400\n",
      "9034/9034 [==============================] - 3s 342us/step - loss: 0.0412 - val_loss: 0.0392\n",
      "Epoch 117/400\n",
      "9034/9034 [==============================] - 3s 329us/step - loss: 0.0411 - val_loss: 0.0392\n",
      "Epoch 118/400\n",
      "9034/9034 [==============================] - 3s 355us/step - loss: 0.0411 - val_loss: 0.0392\n",
      "Epoch 119/400\n",
      "9034/9034 [==============================] - 3s 322us/step - loss: 0.0410 - val_loss: 0.0392\n",
      "Epoch 120/400\n",
      "9034/9034 [==============================] - 3s 331us/step - loss: 0.0410 - val_loss: 0.0392\n",
      "Epoch 121/400\n",
      "9034/9034 [==============================] - 3s 361us/step - loss: 0.0409 - val_loss: 0.0391\n",
      "Epoch 122/400\n",
      "9034/9034 [==============================] - 3s 326us/step - loss: 0.0409 - val_loss: 0.0391\n",
      "Epoch 123/400\n",
      "9034/9034 [==============================] - 3s 330us/step - loss: 0.0408 - val_loss: 0.0391\n",
      "Epoch 124/400\n",
      "9034/9034 [==============================] - 3s 325us/step - loss: 0.0408 - val_loss: 0.0391\n",
      "Epoch 125/400\n",
      "9034/9034 [==============================] - 3s 347us/step - loss: 0.0407 - val_loss: 0.0391\n",
      "Epoch 126/400\n",
      "9034/9034 [==============================] - 3s 330us/step - loss: 0.0406 - val_loss: 0.0390\n",
      "Epoch 127/400\n",
      "9034/9034 [==============================] - 3s 342us/step - loss: 0.0406 - val_loss: 0.0390\n",
      "Epoch 128/400\n",
      "9034/9034 [==============================] - 3s 332us/step - loss: 0.0405 - val_loss: 0.0390\n",
      "Epoch 129/400\n",
      "9034/9034 [==============================] - 4s 397us/step - loss: 0.0405 - val_loss: 0.0390\n",
      "Epoch 130/400\n",
      "9034/9034 [==============================] - 4s 455us/step - loss: 0.0404 - val_loss: 0.0390\n",
      "Epoch 131/400\n",
      "9034/9034 [==============================] - 3s 333us/step - loss: 0.0404 - val_loss: 0.0389\n",
      "Epoch 132/400\n",
      "9034/9034 [==============================] - 3s 323us/step - loss: 0.0403 - val_loss: 0.0389\n",
      "Epoch 133/400\n",
      "9034/9034 [==============================] - 3s 330us/step - loss: 0.0403 - val_loss: 0.0389\n",
      "Epoch 134/400\n",
      "9034/9034 [==============================] - 3s 344us/step - loss: 0.0402 - val_loss: 0.0389\n",
      "Epoch 135/400\n",
      "9034/9034 [==============================] - 3s 347us/step - loss: 0.0402 - val_loss: 0.0389\n",
      "Epoch 136/400\n",
      "9034/9034 [==============================] - 3s 337us/step - loss: 0.0401 - val_loss: 0.0389\n",
      "Epoch 137/400\n",
      "9034/9034 [==============================] - 3s 337us/step - loss: 0.0401 - val_loss: 0.0388\n",
      "Epoch 138/400\n",
      "9034/9034 [==============================] - 3s 336us/step - loss: 0.0400 - val_loss: 0.0388\n",
      "Epoch 139/400\n",
      "9034/9034 [==============================] - 3s 333us/step - loss: 0.0400 - val_loss: 0.0388\n",
      "Epoch 140/400\n",
      "9034/9034 [==============================] - 3s 343us/step - loss: 0.0399 - val_loss: 0.0388\n",
      "Epoch 141/400\n",
      "9034/9034 [==============================] - 3s 329us/step - loss: 0.0399 - val_loss: 0.0388\n",
      "Epoch 142/400\n",
      "9034/9034 [==============================] - 3s 331us/step - loss: 0.0398 - val_loss: 0.0388\n",
      "Epoch 143/400\n",
      "9034/9034 [==============================] - 3s 327us/step - loss: 0.0398 - val_loss: 0.0387\n",
      "Epoch 144/400\n",
      "9034/9034 [==============================] - 3s 333us/step - loss: 0.0397 - val_loss: 0.0387\n",
      "Epoch 145/400\n",
      "9034/9034 [==============================] - 3s 366us/step - loss: 0.0397 - val_loss: 0.0387\n",
      "Epoch 146/400\n",
      "9034/9034 [==============================] - 3s 333us/step - loss: 0.0396 - val_loss: 0.0387\n",
      "Epoch 147/400\n",
      "9034/9034 [==============================] - 3s 376us/step - loss: 0.0396 - val_loss: 0.0387\n",
      "Epoch 148/400\n",
      "9034/9034 [==============================] - 3s 330us/step - loss: 0.0396 - val_loss: 0.0387\n",
      "Epoch 149/400\n",
      "9034/9034 [==============================] - 3s 340us/step - loss: 0.0395 - val_loss: 0.0387\n",
      "Epoch 150/400\n",
      "9034/9034 [==============================] - 3s 365us/step - loss: 0.0395 - val_loss: 0.0386\n",
      "Epoch 151/400\n",
      "9034/9034 [==============================] - 3s 367us/step - loss: 0.0394 - val_loss: 0.0386\n",
      "Epoch 152/400\n",
      "9034/9034 [==============================] - 3s 338us/step - loss: 0.0394 - val_loss: 0.0386\n",
      "Epoch 153/400\n",
      "9034/9034 [==============================] - 3s 336us/step - loss: 0.0393 - val_loss: 0.0386\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 154/400\n",
      "9034/9034 [==============================] - 3s 327us/step - loss: 0.0393 - val_loss: 0.0386\n",
      "Epoch 155/400\n",
      "9034/9034 [==============================] - 3s 322us/step - loss: 0.0392 - val_loss: 0.0386\n",
      "Epoch 156/400\n",
      "9034/9034 [==============================] - 3s 319us/step - loss: 0.0392 - val_loss: 0.0386\n",
      "Epoch 157/400\n",
      "9034/9034 [==============================] - 3s 319us/step - loss: 0.0391 - val_loss: 0.0386\n",
      "Epoch 158/400\n",
      "9034/9034 [==============================] - 3s 315us/step - loss: 0.0391 - val_loss: 0.0385\n",
      "Epoch 159/400\n",
      "9034/9034 [==============================] - 3s 321us/step - loss: 0.0390 - val_loss: 0.0385\n",
      "Epoch 160/400\n",
      "9034/9034 [==============================] - 3s 317us/step - loss: 0.0390 - val_loss: 0.0385\n",
      "Epoch 161/400\n",
      "9034/9034 [==============================] - 3s 312us/step - loss: 0.0390 - val_loss: 0.0385\n",
      "Epoch 162/400\n",
      "9034/9034 [==============================] - 3s 341us/step - loss: 0.0389 - val_loss: 0.0385\n",
      "Epoch 163/400\n",
      "9034/9034 [==============================] - 3s 323us/step - loss: 0.0389 - val_loss: 0.0385\n",
      "Epoch 164/400\n",
      "9034/9034 [==============================] - 3s 320us/step - loss: 0.0388 - val_loss: 0.0385\n",
      "Epoch 165/400\n",
      "9034/9034 [==============================] - 3s 333us/step - loss: 0.0388 - val_loss: 0.0385\n",
      "Epoch 166/400\n",
      "9034/9034 [==============================] - 3s 318us/step - loss: 0.0387 - val_loss: 0.0384\n",
      "Epoch 167/400\n",
      "9034/9034 [==============================] - 3s 320us/step - loss: 0.0387 - val_loss: 0.0384\n",
      "Epoch 168/400\n",
      "9034/9034 [==============================] - 3s 323us/step - loss: 0.0386 - val_loss: 0.0384\n",
      "Epoch 169/400\n",
      "9034/9034 [==============================] - 3s 314us/step - loss: 0.0386 - val_loss: 0.0384\n",
      "Epoch 170/400\n",
      "9034/9034 [==============================] - 3s 321us/step - loss: 0.0386 - val_loss: 0.0384\n",
      "Epoch 171/400\n",
      "9034/9034 [==============================] - 3s 319us/step - loss: 0.0385 - val_loss: 0.0384\n",
      "Epoch 172/400\n",
      "9034/9034 [==============================] - 3s 343us/step - loss: 0.0385 - val_loss: 0.0384\n",
      "Epoch 173/400\n",
      "9034/9034 [==============================] - 3s 314us/step - loss: 0.0384 - val_loss: 0.0384\n",
      "Epoch 174/400\n",
      "9034/9034 [==============================] - 3s 354us/step - loss: 0.0384 - val_loss: 0.0384\n",
      "Epoch 175/400\n",
      "9034/9034 [==============================] - 3s 327us/step - loss: 0.0383 - val_loss: 0.0383\n",
      "Epoch 176/400\n",
      "9034/9034 [==============================] - 3s 323us/step - loss: 0.0383 - val_loss: 0.0383\n",
      "Epoch 177/400\n",
      "9034/9034 [==============================] - 3s 320us/step - loss: 0.0383 - val_loss: 0.0383\n",
      "Epoch 178/400\n",
      "9034/9034 [==============================] - 3s 320us/step - loss: 0.0382 - val_loss: 0.0383\n",
      "Epoch 179/400\n",
      "9034/9034 [==============================] - 3s 323us/step - loss: 0.0382 - val_loss: 0.0383\n",
      "Epoch 180/400\n",
      "9034/9034 [==============================] - 3s 337us/step - loss: 0.0381 - val_loss: 0.0383\n",
      "Epoch 181/400\n",
      "9034/9034 [==============================] - 3s 317us/step - loss: 0.0381 - val_loss: 0.0383\n",
      "Epoch 182/400\n",
      "9034/9034 [==============================] - 3s 322us/step - loss: 0.0381 - val_loss: 0.0383\n",
      "Epoch 183/400\n",
      "9034/9034 [==============================] - 3s 317us/step - loss: 0.0380 - val_loss: 0.0383\n",
      "Epoch 184/400\n",
      "9034/9034 [==============================] - 3s 325us/step - loss: 0.0380 - val_loss: 0.0383\n",
      "Epoch 185/400\n",
      "9034/9034 [==============================] - 3s 355us/step - loss: 0.0379 - val_loss: 0.0383\n",
      "Epoch 186/400\n",
      "9034/9034 [==============================] - 3s 322us/step - loss: 0.0379 - val_loss: 0.0382\n",
      "Epoch 187/400\n",
      "9034/9034 [==============================] - 3s 323us/step - loss: 0.0378 - val_loss: 0.0382\n",
      "Epoch 188/400\n",
      "9034/9034 [==============================] - 3s 328us/step - loss: 0.0378 - val_loss: 0.0382\n",
      "Epoch 189/400\n",
      "9034/9034 [==============================] - 3s 327us/step - loss: 0.0378 - val_loss: 0.0382\n",
      "Epoch 190/400\n",
      "9034/9034 [==============================] - 3s 327us/step - loss: 0.0377 - val_loss: 0.0382\n",
      "Epoch 191/400\n",
      "9034/9034 [==============================] - 3s 332us/step - loss: 0.0377 - val_loss: 0.0382\n",
      "Epoch 192/400\n",
      "9034/9034 [==============================] - 3s 314us/step - loss: 0.0376 - val_loss: 0.0382\n",
      "Epoch 193/400\n",
      "9034/9034 [==============================] - 3s 321us/step - loss: 0.0376 - val_loss: 0.0382\n",
      "Epoch 194/400\n",
      "9034/9034 [==============================] - 3s 325us/step - loss: 0.0376 - val_loss: 0.0382\n",
      "Epoch 195/400\n",
      "9034/9034 [==============================] - 3s 354us/step - loss: 0.0375 - val_loss: 0.0382\n",
      "Epoch 196/400\n",
      "9034/9034 [==============================] - 3s 334us/step - loss: 0.0375 - val_loss: 0.0382\n",
      "Epoch 197/400\n",
      "9034/9034 [==============================] - 3s 323us/step - loss: 0.0374 - val_loss: 0.0381\n",
      "Epoch 198/400\n",
      "9034/9034 [==============================] - 3s 346us/step - loss: 0.0374 - val_loss: 0.0381\n",
      "Epoch 199/400\n",
      "9034/9034 [==============================] - 3s 325us/step - loss: 0.0374 - val_loss: 0.0381\n",
      "Epoch 200/400\n",
      "9034/9034 [==============================] - 3s 331us/step - loss: 0.0373 - val_loss: 0.0381\n",
      "Epoch 201/400\n",
      "9034/9034 [==============================] - 3s 322us/step - loss: 0.0373 - val_loss: 0.0381\n",
      "Epoch 202/400\n",
      "9034/9034 [==============================] - 3s 320us/step - loss: 0.0372 - val_loss: 0.0381\n",
      "Epoch 203/400\n",
      "9034/9034 [==============================] - 3s 318us/step - loss: 0.0372 - val_loss: 0.0381\n",
      "Epoch 204/400\n",
      "9034/9034 [==============================] - 3s 330us/step - loss: 0.0372 - val_loss: 0.0381\n",
      "Epoch 205/400\n",
      "9034/9034 [==============================] - 3s 345us/step - loss: 0.0371 - val_loss: 0.0381\n",
      "Epoch 206/400\n",
      "9034/9034 [==============================] - 3s 334us/step - loss: 0.0371 - val_loss: 0.0381\n",
      "Epoch 207/400\n",
      "9034/9034 [==============================] - 3s 339us/step - loss: 0.0370 - val_loss: 0.0381\n",
      "Epoch 208/400\n",
      "9034/9034 [==============================] - 3s 337us/step - loss: 0.0370 - val_loss: 0.0381\n",
      "Epoch 209/400\n",
      "9034/9034 [==============================] - 3s 334us/step - loss: 0.0370 - val_loss: 0.0381\n",
      "Epoch 210/400\n",
      "9034/9034 [==============================] - 3s 340us/step - loss: 0.0369 - val_loss: 0.0381\n",
      "Epoch 211/400\n",
      "9034/9034 [==============================] - 3s 337us/step - loss: 0.0369 - val_loss: 0.0380\n",
      "Epoch 212/400\n",
      "9034/9034 [==============================] - 3s 323us/step - loss: 0.0368 - val_loss: 0.0380\n",
      "Epoch 213/400\n",
      "9034/9034 [==============================] - 3s 339us/step - loss: 0.0368 - val_loss: 0.0380\n",
      "Epoch 214/400\n",
      "9034/9034 [==============================] - 3s 329us/step - loss: 0.0368 - val_loss: 0.0380\n",
      "Epoch 215/400\n",
      "9034/9034 [==============================] - 3s 349us/step - loss: 0.0367 - val_loss: 0.0380\n",
      "Epoch 216/400\n",
      "9034/9034 [==============================] - 3s 329us/step - loss: 0.0367 - val_loss: 0.0380\n",
      "Epoch 217/400\n",
      "9034/9034 [==============================] - 3s 348us/step - loss: 0.0366 - val_loss: 0.0380\n",
      "Epoch 218/400\n",
      "9034/9034 [==============================] - 3s 333us/step - loss: 0.0366 - val_loss: 0.0380\n",
      "Epoch 219/400\n",
      "9034/9034 [==============================] - 3s 336us/step - loss: 0.0366 - val_loss: 0.0380\n",
      "Epoch 220/400\n",
      "9034/9034 [==============================] - 3s 351us/step - loss: 0.0365 - val_loss: 0.0380\n",
      "Epoch 221/400\n",
      "9034/9034 [==============================] - 3s 336us/step - loss: 0.0365 - val_loss: 0.0380\n",
      "Epoch 222/400\n",
      "9034/9034 [==============================] - 3s 336us/step - loss: 0.0364 - val_loss: 0.0380\n",
      "Epoch 223/400\n",
      "9034/9034 [==============================] - 3s 337us/step - loss: 0.0364 - val_loss: 0.0380\n",
      "Epoch 224/400\n",
      "9034/9034 [==============================] - 3s 355us/step - loss: 0.0364 - val_loss: 0.0380\n",
      "Epoch 225/400\n",
      "9034/9034 [==============================] - 3s 340us/step - loss: 0.0363 - val_loss: 0.0380\n",
      "Epoch 226/400\n",
      "9034/9034 [==============================] - 3s 345us/step - loss: 0.0363 - val_loss: 0.0380\n",
      "Epoch 227/400\n",
      "9034/9034 [==============================] - 3s 332us/step - loss: 0.0362 - val_loss: 0.0380\n",
      "Epoch 228/400\n",
      "9034/9034 [==============================] - 3s 342us/step - loss: 0.0362 - val_loss: 0.0379\n",
      "Epoch 229/400\n",
      "9034/9034 [==============================] - 3s 341us/step - loss: 0.0362 - val_loss: 0.0379\n",
      "Epoch 230/400\n",
      "9034/9034 [==============================] - 3s 329us/step - loss: 0.0361 - val_loss: 0.0379\n",
      "Epoch 231/400\n",
      "9034/9034 [==============================] - 3s 354us/step - loss: 0.0361 - val_loss: 0.0379\n",
      "Epoch 232/400\n",
      "9034/9034 [==============================] - 3s 349us/step - loss: 0.0360 - val_loss: 0.0379\n",
      "Epoch 233/400\n",
      "9034/9034 [==============================] - 3s 316us/step - loss: 0.0360 - val_loss: 0.0379\n",
      "Epoch 234/400\n",
      "9034/9034 [==============================] - 3s 321us/step - loss: 0.0360 - val_loss: 0.0379\n",
      "Epoch 235/400\n",
      "9034/9034 [==============================] - 3s 337us/step - loss: 0.0359 - val_loss: 0.0379\n",
      "Epoch 236/400\n",
      "9034/9034 [==============================] - 3s 326us/step - loss: 0.0359 - val_loss: 0.0379\n",
      "Epoch 237/400\n",
      "9034/9034 [==============================] - 3s 314us/step - loss: 0.0359 - val_loss: 0.0379\n",
      "Epoch 238/400\n",
      "9034/9034 [==============================] - 3s 318us/step - loss: 0.0358 - val_loss: 0.0379\n",
      "Epoch 239/400\n",
      "9034/9034 [==============================] - 3s 316us/step - loss: 0.0358 - val_loss: 0.0379\n",
      "Epoch 240/400\n",
      "9034/9034 [==============================] - 3s 322us/step - loss: 0.0357 - val_loss: 0.0379\n",
      "Epoch 241/400\n",
      "9034/9034 [==============================] - 3s 322us/step - loss: 0.0357 - val_loss: 0.0379\n",
      "Epoch 242/400\n",
      "9034/9034 [==============================] - 3s 325us/step - loss: 0.0357 - val_loss: 0.0379\n",
      "Epoch 243/400\n",
      "9034/9034 [==============================] - 3s 315us/step - loss: 0.0356 - val_loss: 0.0379\n",
      "Epoch 244/400\n",
      "9034/9034 [==============================] - 3s 320us/step - loss: 0.0356 - val_loss: 0.0379\n",
      "Epoch 245/400\n",
      "9034/9034 [==============================] - 3s 324us/step - loss: 0.0355 - val_loss: 0.0379\n",
      "Epoch 246/400\n",
      "9034/9034 [==============================] - 3s 316us/step - loss: 0.0355 - val_loss: 0.0379\n",
      "Epoch 247/400\n",
      "9034/9034 [==============================] - 3s 323us/step - loss: 0.0355 - val_loss: 0.0379\n",
      "Epoch 248/400\n",
      "9034/9034 [==============================] - 3s 329us/step - loss: 0.0354 - val_loss: 0.0379\n",
      "Epoch 249/400\n",
      "9034/9034 [==============================] - 3s 317us/step - loss: 0.0354 - val_loss: 0.0378\n",
      "Epoch 250/400\n",
      "9034/9034 [==============================] - 3s 335us/step - loss: 0.0354 - val_loss: 0.0378\n",
      "Epoch 251/400\n",
      "9034/9034 [==============================] - 3s 321us/step - loss: 0.0353 - val_loss: 0.0378\n",
      "Epoch 252/400\n",
      "9034/9034 [==============================] - 3s 314us/step - loss: 0.0353 - val_loss: 0.0378\n",
      "Epoch 253/400\n",
      "9034/9034 [==============================] - 3s 320us/step - loss: 0.0352 - val_loss: 0.0378\n",
      "Epoch 254/400\n",
      "9034/9034 [==============================] - 3s 319us/step - loss: 0.0352 - val_loss: 0.0378\n",
      "Epoch 255/400\n",
      "9034/9034 [==============================] - 3s 331us/step - loss: 0.0352 - val_loss: 0.0378\n",
      "Epoch 256/400\n",
      "9034/9034 [==============================] - 4s 437us/step - loss: 0.0351 - val_loss: 0.0378\n",
      "Epoch 257/400\n",
      "9034/9034 [==============================] - 3s 321us/step - loss: 0.0351 - val_loss: 0.0378\n",
      "Epoch 258/400\n",
      "9034/9034 [==============================] - 3s 321us/step - loss: 0.0350 - val_loss: 0.0378\n",
      "Epoch 259/400\n",
      "9034/9034 [==============================] - 3s 327us/step - loss: 0.0350 - val_loss: 0.0378\n",
      "Epoch 260/400\n",
      "9034/9034 [==============================] - 3s 329us/step - loss: 0.0350 - val_loss: 0.0378\n",
      "Epoch 261/400\n",
      "9034/9034 [==============================] - 3s 320us/step - loss: 0.0349 - val_loss: 0.0378\n",
      "Epoch 262/400\n",
      "9034/9034 [==============================] - 3s 323us/step - loss: 0.0349 - val_loss: 0.0378\n",
      "Epoch 263/400\n",
      "9034/9034 [==============================] - 3s 334us/step - loss: 0.0349 - val_loss: 0.0378\n",
      "Epoch 264/400\n",
      "9034/9034 [==============================] - 3s 322us/step - loss: 0.0348 - val_loss: 0.0378\n",
      "Epoch 265/400\n",
      "9034/9034 [==============================] - 3s 348us/step - loss: 0.0348 - val_loss: 0.0378\n",
      "Epoch 266/400\n",
      "9034/9034 [==============================] - 3s 326us/step - loss: 0.0347 - val_loss: 0.0378\n",
      "Epoch 267/400\n",
      "9034/9034 [==============================] - 3s 325us/step - loss: 0.0347 - val_loss: 0.0378\n",
      "Epoch 268/400\n",
      "9034/9034 [==============================] - 3s 324us/step - loss: 0.0347 - val_loss: 0.0378\n",
      "Epoch 269/400\n",
      "9034/9034 [==============================] - 3s 322us/step - loss: 0.0346 - val_loss: 0.0378\n",
      "Epoch 270/400\n",
      "9034/9034 [==============================] - 3s 326us/step - loss: 0.0346 - val_loss: 0.0378\n",
      "Epoch 271/400\n",
      "9034/9034 [==============================] - 3s 331us/step - loss: 0.0345 - val_loss: 0.0378\n",
      "Epoch 272/400\n",
      "9034/9034 [==============================] - 3s 327us/step - loss: 0.0345 - val_loss: 0.0378\n",
      "Epoch 273/400\n",
      "9034/9034 [==============================] - 3s 325us/step - loss: 0.0345 - val_loss: 0.0378\n",
      "Epoch 274/400\n",
      "9034/9034 [==============================] - 3s 328us/step - loss: 0.0344 - val_loss: 0.0378\n",
      "Epoch 275/400\n",
      "9034/9034 [==============================] - 3s 340us/step - loss: 0.0344 - val_loss: 0.0378\n",
      "Epoch 276/400\n",
      "9034/9034 [==============================] - 3s 329us/step - loss: 0.0344 - val_loss: 0.0378\n",
      "Epoch 277/400\n",
      "9034/9034 [==============================] - 3s 325us/step - loss: 0.0343 - val_loss: 0.0378\n",
      "Epoch 278/400\n",
      "9034/9034 [==============================] - 3s 325us/step - loss: 0.0343 - val_loss: 0.0378\n",
      "Epoch 279/400\n",
      "9034/9034 [==============================] - 3s 340us/step - loss: 0.0342 - val_loss: 0.0378\n",
      "Epoch 280/400\n",
      "9034/9034 [==============================] - 3s 366us/step - loss: 0.0342 - val_loss: 0.0377\n",
      "Epoch 281/400\n",
      "9034/9034 [==============================] - 3s 327us/step - loss: 0.0342 - val_loss: 0.0377\n",
      "Epoch 282/400\n",
      "9034/9034 [==============================] - 3s 329us/step - loss: 0.0341 - val_loss: 0.0377\n",
      "Epoch 283/400\n",
      "9034/9034 [==============================] - 3s 330us/step - loss: 0.0341 - val_loss: 0.0377\n",
      "Epoch 284/400\n",
      "9034/9034 [==============================] - 3s 328us/step - loss: 0.0341 - val_loss: 0.0377\n",
      "Epoch 285/400\n",
      "9034/9034 [==============================] - 3s 349us/step - loss: 0.0340 - val_loss: 0.0377\n",
      "Epoch 286/400\n",
      "9034/9034 [==============================] - 3s 328us/step - loss: 0.0340 - val_loss: 0.0377\n",
      "Epoch 287/400\n",
      "9034/9034 [==============================] - 3s 320us/step - loss: 0.0339 - val_loss: 0.0377\n",
      "Epoch 288/400\n",
      "9034/9034 [==============================] - 3s 324us/step - loss: 0.0339 - val_loss: 0.0377\n",
      "Epoch 289/400\n",
      "9034/9034 [==============================] - 3s 335us/step - loss: 0.0339 - val_loss: 0.0377\n",
      "Epoch 290/400\n",
      "9034/9034 [==============================] - 3s 334us/step - loss: 0.0338 - val_loss: 0.0377\n",
      "Epoch 291/400\n",
      "9034/9034 [==============================] - 3s 330us/step - loss: 0.0338 - val_loss: 0.0377\n",
      "Epoch 292/400\n",
      "9034/9034 [==============================] - 3s 333us/step - loss: 0.0338 - val_loss: 0.0377\n",
      "Epoch 293/400\n",
      "9034/9034 [==============================] - 3s 332us/step - loss: 0.0337 - val_loss: 0.0377\n",
      "Epoch 294/400\n",
      "9034/9034 [==============================] - 3s 326us/step - loss: 0.0337 - val_loss: 0.0377\n",
      "Epoch 295/400\n",
      "9034/9034 [==============================] - 3s 343us/step - loss: 0.0336 - val_loss: 0.0377\n",
      "Epoch 296/400\n",
      "9034/9034 [==============================] - 3s 341us/step - loss: 0.0336 - val_loss: 0.0377\n",
      "Epoch 297/400\n",
      "9034/9034 [==============================] - 3s 334us/step - loss: 0.0336 - val_loss: 0.0377\n",
      "Epoch 298/400\n",
      "9034/9034 [==============================] - 3s 332us/step - loss: 0.0335 - val_loss: 0.0377\n",
      "Epoch 299/400\n",
      "9034/9034 [==============================] - 3s 338us/step - loss: 0.0335 - val_loss: 0.0377\n",
      "Epoch 300/400\n",
      "9034/9034 [==============================] - 3s 353us/step - loss: 0.0334 - val_loss: 0.0377\n",
      "Epoch 301/400\n",
      "9034/9034 [==============================] - 3s 334us/step - loss: 0.0334 - val_loss: 0.0377\n",
      "Epoch 302/400\n",
      "9034/9034 [==============================] - 3s 339us/step - loss: 0.0334 - val_loss: 0.0377\n",
      "Epoch 303/400\n",
      "9034/9034 [==============================] - 3s 326us/step - loss: 0.0333 - val_loss: 0.0377\n",
      "Epoch 304/400\n",
      "9034/9034 [==============================] - 3s 331us/step - loss: 0.0333 - val_loss: 0.0377\n",
      "Epoch 305/400\n",
      "9034/9034 [==============================] - 3s 382us/step - loss: 0.0333 - val_loss: 0.0377\n",
      "Epoch 306/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9034/9034 [==============================] - 3s 347us/step - loss: 0.0332 - val_loss: 0.0377\n",
      "Epoch 307/400\n",
      "9034/9034 [==============================] - 3s 316us/step - loss: 0.0332 - val_loss: 0.0377\n",
      "Epoch 308/400\n",
      "9034/9034 [==============================] - 3s 329us/step - loss: 0.0331 - val_loss: 0.0377\n",
      "Epoch 309/400\n",
      "9034/9034 [==============================] - 3s 319us/step - loss: 0.0331 - val_loss: 0.0377\n",
      "Epoch 310/400\n",
      "9034/9034 [==============================] - 3s 319us/step - loss: 0.0331 - val_loss: 0.0377\n",
      "Epoch 311/400\n",
      "9034/9034 [==============================] - 3s 320us/step - loss: 0.0330 - val_loss: 0.0377\n",
      "Epoch 312/400\n",
      "9034/9034 [==============================] - 3s 314us/step - loss: 0.0330 - val_loss: 0.0377\n",
      "Epoch 313/400\n",
      "9034/9034 [==============================] - 3s 318us/step - loss: 0.0330 - val_loss: 0.0377\n",
      "Epoch 314/400\n",
      "9034/9034 [==============================] - 3s 333us/step - loss: 0.0329 - val_loss: 0.0377\n",
      "Epoch 315/400\n",
      "9034/9034 [==============================] - 3s 325us/step - loss: 0.0329 - val_loss: 0.0377\n",
      "Epoch 316/400\n",
      "9034/9034 [==============================] - 3s 321us/step - loss: 0.0328 - val_loss: 0.0377\n",
      "Epoch 317/400\n",
      "9034/9034 [==============================] - 3s 321us/step - loss: 0.0328 - val_loss: 0.0377\n",
      "Epoch 318/400\n",
      "9034/9034 [==============================] - 3s 318us/step - loss: 0.0328 - val_loss: 0.0377- ETA: 0s - loss: 0.03\n",
      "Epoch 319/400\n",
      "9034/9034 [==============================] - 3s 325us/step - loss: 0.0327 - val_loss: 0.0377\n",
      "Epoch 320/400\n",
      "9034/9034 [==============================] - 3s 319us/step - loss: 0.0327 - val_loss: 0.0377\n",
      "Epoch 321/400\n",
      "9034/9034 [==============================] - 3s 319us/step - loss: 0.0327 - val_loss: 0.0377\n",
      "Epoch 322/400\n",
      "9034/9034 [==============================] - 3s 313us/step - loss: 0.0326 - val_loss: 0.0377\n",
      "Epoch 323/400\n",
      "9034/9034 [==============================] - 3s 303us/step - loss: 0.0326 - val_loss: 0.0377\n",
      "Epoch 324/400\n",
      "9034/9034 [==============================] - 3s 338us/step - loss: 0.0325 - val_loss: 0.0377\n",
      "Epoch 325/400\n",
      "9034/9034 [==============================] - 4s 422us/step - loss: 0.0325 - val_loss: 0.0377\n",
      "Epoch 326/400\n",
      "9034/9034 [==============================] - 3s 306us/step - loss: 0.0325 - val_loss: 0.0377\n",
      "Epoch 327/400\n",
      "9034/9034 [==============================] - 3s 305us/step - loss: 0.0324 - val_loss: 0.0377\n",
      "Epoch 328/400\n",
      "9034/9034 [==============================] - 3s 305us/step - loss: 0.0324 - val_loss: 0.0377\n",
      "Epoch 329/400\n",
      "9034/9034 [==============================] - 3s 301us/step - loss: 0.0324 - val_loss: 0.0377\n",
      "Epoch 330/400\n",
      "9034/9034 [==============================] - 3s 307us/step - loss: 0.0323 - val_loss: 0.0377\n",
      "Epoch 331/400\n",
      "9034/9034 [==============================] - 3s 304us/step - loss: 0.0323 - val_loss: 0.0377\n",
      "Epoch 332/400\n",
      "9034/9034 [==============================] - 3s 303us/step - loss: 0.0322 - val_loss: 0.0377\n",
      "Epoch 333/400\n",
      "9034/9034 [==============================] - 3s 312us/step - loss: 0.0322 - val_loss: 0.0377\n",
      "Epoch 334/400\n",
      "9034/9034 [==============================] - 3s 306us/step - loss: 0.0322 - val_loss: 0.0377\n",
      "Epoch 335/400\n",
      "9034/9034 [==============================] - 3s 326us/step - loss: 0.0321 - val_loss: 0.0377\n",
      "Epoch 336/400\n",
      "9034/9034 [==============================] - 3s 323us/step - loss: 0.0321 - val_loss: 0.0377\n",
      "Epoch 337/400\n",
      "9034/9034 [==============================] - 3s 309us/step - loss: 0.0321 - val_loss: 0.0377\n",
      "Epoch 338/400\n",
      "9034/9034 [==============================] - 3s 353us/step - loss: 0.0320 - val_loss: 0.0377\n",
      "Epoch 339/400\n",
      "9034/9034 [==============================] - 3s 306us/step - loss: 0.0320 - val_loss: 0.0377\n",
      "Epoch 340/400\n",
      "9034/9034 [==============================] - 3s 310us/step - loss: 0.0320 - val_loss: 0.0377\n",
      "Epoch 341/400\n",
      "9034/9034 [==============================] - 3s 308us/step - loss: 0.0319 - val_loss: 0.0377\n",
      "Epoch 342/400\n",
      "9034/9034 [==============================] - 3s 312us/step - loss: 0.0319 - val_loss: 0.0377\n",
      "Epoch 343/400\n",
      "9034/9034 [==============================] - 3s 303us/step - loss: 0.0318 - val_loss: 0.0377\n",
      "Epoch 344/400\n",
      "9034/9034 [==============================] - 3s 308us/step - loss: 0.0318 - val_loss: 0.0377\n",
      "Epoch 345/400\n",
      "9034/9034 [==============================] - 3s 311us/step - loss: 0.0318 - val_loss: 0.0377\n",
      "Epoch 346/400\n",
      "9034/9034 [==============================] - 3s 328us/step - loss: 0.0317 - val_loss: 0.0377\n",
      "Epoch 347/400\n",
      "9034/9034 [==============================] - 3s 316us/step - loss: 0.0317 - val_loss: 0.0377\n",
      "Epoch 348/400\n",
      "9034/9034 [==============================] - 3s 308us/step - loss: 0.0317 - val_loss: 0.0377\n",
      "Epoch 349/400\n",
      "9034/9034 [==============================] - 3s 308us/step - loss: 0.0316 - val_loss: 0.0377\n",
      "Epoch 350/400\n",
      "9034/9034 [==============================] - 3s 312us/step - loss: 0.0316 - val_loss: 0.0377\n",
      "Epoch 351/400\n",
      "9034/9034 [==============================] - 3s 311us/step - loss: 0.0315 - val_loss: 0.0377\n",
      "Epoch 352/400\n",
      "9034/9034 [==============================] - 3s 310us/step - loss: 0.0315 - val_loss: 0.0377\n",
      "Epoch 353/400\n",
      "9034/9034 [==============================] - 3s 310us/step - loss: 0.0315 - val_loss: 0.0377\n",
      "Epoch 354/400\n",
      "9034/9034 [==============================] - 3s 314us/step - loss: 0.0314 - val_loss: 0.0377\n",
      "Epoch 355/400\n",
      "9034/9034 [==============================] - 3s 310us/step - loss: 0.0314 - val_loss: 0.0377\n",
      "Epoch 356/400\n",
      "9034/9034 [==============================] - 3s 322us/step - loss: 0.0314 - val_loss: 0.0377\n",
      "Epoch 357/400\n",
      "9034/9034 [==============================] - 3s 328us/step - loss: 0.0313 - val_loss: 0.0377\n",
      "Epoch 358/400\n",
      "9034/9034 [==============================] - 3s 308us/step - loss: 0.0313 - val_loss: 0.0378\n",
      "Epoch 359/400\n",
      "9034/9034 [==============================] - 3s 311us/step - loss: 0.0313 - val_loss: 0.0378\n",
      "Epoch 360/400\n",
      "9034/9034 [==============================] - 3s 312us/step - loss: 0.0312 - val_loss: 0.0378\n",
      "Epoch 361/400\n",
      "9034/9034 [==============================] - 3s 309us/step - loss: 0.0312 - val_loss: 0.0378\n",
      "Epoch 362/400\n",
      "9034/9034 [==============================] - 3s 316us/step - loss: 0.0311 - val_loss: 0.0378\n",
      "Epoch 363/400\n",
      "9034/9034 [==============================] - 3s 312us/step - loss: 0.0311 - val_loss: 0.0378\n",
      "Epoch 364/400\n",
      "9034/9034 [==============================] - 3s 326us/step - loss: 0.0311 - val_loss: 0.0378\n",
      "Epoch 365/400\n",
      "9034/9034 [==============================] - 3s 310us/step - loss: 0.0310 - val_loss: 0.0378\n",
      "Epoch 366/400\n",
      "9034/9034 [==============================] - 3s 326us/step - loss: 0.0310 - val_loss: 0.0378\n",
      "Epoch 367/400\n",
      "9034/9034 [==============================] - 3s 312us/step - loss: 0.0310 - val_loss: 0.0378\n",
      "Epoch 368/400\n",
      "9034/9034 [==============================] - 3s 339us/step - loss: 0.0309 - val_loss: 0.0378\n",
      "Epoch 369/400\n",
      "9034/9034 [==============================] - 3s 309us/step - loss: 0.0309 - val_loss: 0.0378\n",
      "Epoch 370/400\n",
      "9034/9034 [==============================] - 3s 311us/step - loss: 0.0309 - val_loss: 0.0378\n",
      "Epoch 371/400\n",
      "9034/9034 [==============================] - 3s 309us/step - loss: 0.0308 - val_loss: 0.0378\n",
      "Epoch 372/400\n",
      "9034/9034 [==============================] - 3s 320us/step - loss: 0.0308 - val_loss: 0.0378\n",
      "Epoch 373/400\n",
      "9034/9034 [==============================] - 3s 315us/step - loss: 0.0307 - val_loss: 0.0378\n",
      "Epoch 374/400\n",
      "9034/9034 [==============================] - 3s 323us/step - loss: 0.0307 - val_loss: 0.0378\n",
      "Epoch 375/400\n",
      "9034/9034 [==============================] - 3s 308us/step - loss: 0.0307 - val_loss: 0.0378\n",
      "Epoch 376/400\n",
      "9034/9034 [==============================] - 3s 315us/step - loss: 0.0306 - val_loss: 0.0378\n",
      "Epoch 377/400\n",
      "9034/9034 [==============================] - 3s 348us/step - loss: 0.0306 - val_loss: 0.0378\n",
      "Epoch 378/400\n",
      "9034/9034 [==============================] - 3s 352us/step - loss: 0.0306 - val_loss: 0.0378\n",
      "Epoch 379/400\n",
      "9034/9034 [==============================] - 3s 320us/step - loss: 0.0305 - val_loss: 0.0378\n",
      "Epoch 380/400\n",
      "9034/9034 [==============================] - 3s 315us/step - loss: 0.0305 - val_loss: 0.0378\n",
      "Epoch 381/400\n",
      "9034/9034 [==============================] - 3s 308us/step - loss: 0.0305 - val_loss: 0.0378\n",
      "Epoch 382/400\n",
      "9034/9034 [==============================] - 3s 303us/step - loss: 0.0304 - val_loss: 0.0378\n",
      "Epoch 383/400\n",
      "9034/9034 [==============================] - 3s 307us/step - loss: 0.0304 - val_loss: 0.0378\n",
      "Epoch 384/400\n",
      "9034/9034 [==============================] - 3s 306us/step - loss: 0.0304 - val_loss: 0.0378\n",
      "Epoch 385/400\n",
      "9034/9034 [==============================] - 3s 302us/step - loss: 0.0303 - val_loss: 0.0378\n",
      "Epoch 386/400\n",
      "9034/9034 [==============================] - 3s 297us/step - loss: 0.0303 - val_loss: 0.0378\n",
      "Epoch 387/400\n",
      "9034/9034 [==============================] - 3s 317us/step - loss: 0.0302 - val_loss: 0.0378\n",
      "Epoch 388/400\n",
      "9034/9034 [==============================] - 3s 310us/step - loss: 0.0302 - val_loss: 0.0379\n",
      "Epoch 389/400\n",
      "9034/9034 [==============================] - 3s 324us/step - loss: 0.0302 - val_loss: 0.0379\n",
      "Epoch 390/400\n",
      "9034/9034 [==============================] - 3s 302us/step - loss: 0.0301 - val_loss: 0.0379\n",
      "Epoch 391/400\n",
      "9034/9034 [==============================] - 3s 295us/step - loss: 0.0301 - val_loss: 0.0379\n",
      "Epoch 392/400\n",
      "9034/9034 [==============================] - 3s 300us/step - loss: 0.0301 - val_loss: 0.0379\n",
      "Epoch 393/400\n",
      "9034/9034 [==============================] - 3s 297us/step - loss: 0.0300 - val_loss: 0.0379\n",
      "Epoch 394/400\n",
      "9034/9034 [==============================] - 3s 315us/step - loss: 0.0300 - val_loss: 0.0379\n",
      "Epoch 395/400\n",
      "9034/9034 [==============================] - 3s 303us/step - loss: 0.0300 - val_loss: 0.0379\n",
      "Epoch 396/400\n",
      "9034/9034 [==============================] - 3s 305us/step - loss: 0.0299 - val_loss: 0.0379\n",
      "Epoch 397/400\n",
      "9034/9034 [==============================] - 3s 303us/step - loss: 0.0299 - val_loss: 0.0379\n",
      "Epoch 398/400\n",
      "9034/9034 [==============================] - 3s 301us/step - loss: 0.0299 - val_loss: 0.0379\n",
      "Epoch 399/400\n",
      "9034/9034 [==============================] - 3s 310us/step - loss: 0.0298 - val_loss: 0.0379\n",
      "Epoch 400/400\n",
      "9034/9034 [==============================] - 3s 320us/step - loss: 0.0298 - val_loss: 0.0379\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1320x880 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Train 1 time\n",
    "\n",
    "epochs = 400\n",
    "x_train_spectra = np.log(spectra+1)\n",
    "x_train_substituents = substituents\n",
    "\n",
    "enc = simplified_substituent_model(x_train_spectra, x_train_substituents.values, epochs=epochs)\n",
    "\n",
    "actual = x_train_substituents.values\n",
    "predicted = enc.predict(x_train_spectra)\n",
    "\n",
    "# print(compute_auc(444, actual, predicted, num_samples=10038))\n",
    "    \n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing base scores with experiment scores (to get the bar chart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(baseline_stats, baseline_perm_scores, exp_stats, exp_perm_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the results in (substituent_term auc_score) format to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(substituents_auc_results_path, 'w') as f:\n",
    "    for index, auc in enumerate(exp_stats):\n",
    "        f.write(substituent_names[index] + \" \" + str(auc[2]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method to calculate how many matches are there i.e. true label == predicted label. Write to result to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "substituent_dict = {}\n",
    "true_label_counter = {}\n",
    "substituent_correct_counter = {}\n",
    "\n",
    "for name in substituent_names:\n",
    "    true_label_counter[name] = 0\n",
    "    substituent_correct_counter[name] = 0\n",
    "\n",
    "# save the substituent terms indexes to a dict for each id\n",
    "# e.g. {CCMSLIB0000001 : [10, 15, 103], ...}\n",
    "with open(substituents_path, 'r') as f:\n",
    "    for line in f:\n",
    "        id, index, value = line.split(\" \")\n",
    "        if id not in substituent_dict:\n",
    "            substituent_dict[id] = []\n",
    "        substituent_dict[id].append(index)\n",
    "\n",
    "# get the index of element that has highest probability\n",
    "# for each substituent terms in the gnps id, store the max\n",
    "# in the end, go through each and see if the index is in the true labels\n",
    "# if matches, that term's counter increase\n",
    "for index, probabilities in enumerate(predicted):\n",
    "    temp = copy.deepcopy(probabilities).tolist()\n",
    "    true_labels = []\n",
    "    max_indexes = []\n",
    "    for substituent_index in substituent_dict[substituents.index[index]]:\n",
    "        true_labels.append(substituent_index)\n",
    "        max_index = temp.index(max(temp))\n",
    "        max_indexes.append(max_index)\n",
    "        temp.remove(temp[max_index])\n",
    "        true_label_counter[substituent_names[int(substituent_index)]] += 1\n",
    "    for index in max_indexes:\n",
    "        if index in true_labels:\n",
    "            substituent_correct_counter[substituent_names[int(index)]] += 1\n",
    "                    \n",
    "prediction_comparison_report_path = \"G:\\\\Dev\\\\Data\\\\substituent_prediction_comparison_report.txt\"\n",
    "with open(prediction_comparison_report_path, 'w') as f:\n",
    "    f.write(\"substituent,matched,actual,proportion\\n\")\n",
    "\n",
    "with open(prediction_comparison_report_path, 'a') as f:\n",
    "    for substituent in substituent_correct_counter:\n",
    "        matched = substituent_correct_counter[substituent]\n",
    "        actual = true_label_counter[substituent]\n",
    "        proportion = 0\n",
    "        if actual != 0:\n",
    "            proportion = matched / actual\n",
    "        f.write(substituent + \",\" + str(matched) + \",\" + str(actual) + \",\" + str(proportion*100) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method to generate validation split files to be used when training different splits 10 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnps_all_substituent_perm_file_path = \"G:\\\\Dev\\\\Data\\\\GNPS ALL Substituent Validation Split Permutations.txt\"\n",
    "\n",
    "# Create and store 10 randomly permuted indices for 10038 samples\n",
    "sample_size = 10038\n",
    "att = np.arange(sample_size)\n",
    "\n",
    "att = np.random.permutation(att)\n",
    "\n",
    "index_permutations = np.zeros((sample_size, 0), dtype=int)\n",
    "# Add each permutation to a numpy array of indices\n",
    "for i in range(10):\n",
    "    perm = np.random.permutation(np.arange(sample_size, dtype=int))\n",
    "    index_permutations = np.column_stack((index_permutations, perm))\n",
    "\n",
    "# Verify numpy array has correct shape (should be 5770 for each column)\n",
    "for i in range(10):\n",
    "    print(np.unique(index_permutations[:, i]).size)\n",
    "\n",
    "# Save index permutations to file\n",
    "np.savetxt(gnps_all_substituent_perm_file_path, index_permutations, delimiter=',', fmt='%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of training 10 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HI\n"
     ]
    }
   ],
   "source": [
    "# Train 10 times (may take awhile)\n",
    "\n",
    "path = \"G:\\\\Dev\\\\Data\\\\Substituents Experiments\\\\\"\n",
    "train_diff_splits(path, \"GNPS ALL Substituents\")\n",
    "\n",
    "process_average_substituent_path = \"G:\\\\Dev\\\\Data\\\\Substituents Experiments\\\\filtered_average_result.txt\"\n",
    "substituents_legend_path = \"G:\\\\Dev\\\\Data\\\\Classyfire Taxanomy\\\\GNPS_substituents_legend.txt\"\n",
    "substituents_term_occurences_path = \"G:\\\\Dev\\\\Data\\\\Substituent Terms Occurences\\\\substituents_terms_occurences.txt\"\n",
    "variables = [\"GNPS ALL Substituents\"]\n",
    "data = []\n",
    "substituent_occurences_dict = {}\n",
    "\n",
    "# to calculate occurences\n",
    "with open(substituents_legend_path, 'r') as f:\n",
    "    content = f.readlines()\n",
    "\n",
    "with open(substituents_term_occurences_path, 'r') as f:\n",
    "    for line in f:\n",
    "        substituent, occurences = line.split(\"\\t\")\n",
    "        substituent_occurences_dict[substituent] = int(float(occurences[:-1]))\n",
    "\n",
    "for i in range(10):\n",
    "    filepath = path + variables[0] + \" \" + str(i) + \".txt\"\n",
    "    stats_one = np.loadtxt(filepath, dtype=float)\n",
    "    print(stats_one[:, 2])\n",
    "    data.append(stats_one[:, 2])\n",
    "\n",
    "print(\"Average\")\n",
    "result = np.mean(data, axis=0)\n",
    "with open(process_average_substituent_path, 'w') as f:\n",
    "    for index, probability in enumerate(result):\n",
    "        f.write(content[index][:-1] + \"\\t\" + str(probability) + \"\\t\" + str(substituent_occurences_dict[content[index][:-1]]) + \"%\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving model to h5 format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "filepath = \"G:\\\\Dev\\\\Data\\\\saved_substituents_classifier_after_parameters_400e_model.h5\"\n",
    "enc.save(filepath)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

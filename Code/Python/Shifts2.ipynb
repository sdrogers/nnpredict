{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â Notebook to play with convolution for networks\n",
    "\n",
    "- Setup a binary classification task\n",
    "- Have peak probabilities for each class\n",
    "- Then have some shifts also for the class and see if convolution can improve this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from matplotlib import pylab as plt\n",
    "\n",
    "import pandas as pd\n",
    "import math\n",
    "import random\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_random_ones = None\n",
    "training_random_zeros = None\n",
    "test_random_ones = None\n",
    "test_random_zeros = None\n",
    "\n",
    "all_ones = None\n",
    "all_zeros = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_number_of_training_set(has_substructure_path):\n",
    "    global all_ones\n",
    "    global all_zeros\n",
    "    substructure = np.loadtxt(has_substructure_path, np.int)\n",
    "    all_ones = np.where(substructure == 1)\n",
    "    all_zeros = np.where(substructure == 0)\n",
    "\n",
    "    number_of_training_set = int(len(all_ones[0]) * 0.7)\n",
    "    return number_of_training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_training_and_test_set(num_of_training_set):\n",
    "    global training_random_ones\n",
    "    global training_random_zeros\n",
    "    global test_random_ones\n",
    "    global test_random_zeros\n",
    "\n",
    "    training_random_ones = random.sample(list(all_ones[0]), num_of_training_set)\n",
    "    training_random_zeros = random.sample(list(all_zeros[0]), num_of_training_set)\n",
    "\n",
    "    test_random_ones = [ones for ones in all_ones[0] if ones not in training_random_ones]\n",
    "    test_random_zeros = [zeros for zeros in all_zeros[0] if zeros not in training_random_zeros]\n",
    "    test_random_zeros = random.sample(test_random_zeros, len(test_random_ones))\n",
    "    \n",
    "def get_list_of_ids(dataset):\n",
    "    return [\"GNPS_ALL_\" + str(index+1) for index in dataset]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building training and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_dataset_dir = \"G:\\\\Dev\\\\Data\\\\MSMS-NIST\\\\Python Filtered\"\n",
    "\n",
    "def load_training_spec(spec_path):\n",
    "    global training_random_ones\n",
    "    global training_random_zeros\n",
    "    training_random_ones_id = get_list_of_ids(training_random_ones)\n",
    "    training_random_zeros_id = get_list_of_ids(training_random_zeros)\n",
    "    training_set = training_random_ones_id + training_random_zeros_id\n",
    "    \n",
    "    file_list = os.listdir(spec_path)\n",
    "    \n",
    "    filtered_file_list = [file for file in file_list if file[:-13] in training_set]\n",
    "    filtered_name_list = [filename[:-13] for filename in filtered_file_list]\n",
    "\n",
    "    intensities = pd.DataFrame(0.0, index = filtered_name_list, columns=range(1000), dtype=float)\n",
    "\n",
    "    for file in filtered_file_list:\n",
    "        filepath = os.path.join(filtered_dataset_dir, file)\n",
    "        mol_name = file[:-13]\n",
    "        with open(filepath, 'r') as f:\n",
    "            for index, line in enumerate(f):\n",
    "                mass, intensity = line.split(\" \")\n",
    "                if not math.isnan(float(intensity)):\n",
    "                    intensities.at[mol_name, int(mass)-1] = float(intensity)\n",
    "    \n",
    "    return intensities\n",
    "\n",
    "def load_test_spec(spec_path):\n",
    "    global test_random_ones\n",
    "    global test_random_zeros\n",
    "    test_random_ones_id = get_list_of_ids(test_random_ones)\n",
    "    test_random_zeros_id = get_list_of_ids(test_random_zeros)\n",
    "    test_set = test_random_ones_id + test_random_zeros_id\n",
    "    file_list = os.listdir(spec_path)\n",
    "    \n",
    "    filtered_file_list = [file for file in file_list if file[:-13] in test_set]\n",
    "    filtered_name_list = [filename[:-13] for filename in filtered_file_list]\n",
    "\n",
    "    test_intensities = pd.DataFrame(0.0, index = filtered_name_list, columns=range(1000), dtype=float)\n",
    "\n",
    "    for file in filtered_file_list:\n",
    "        filepath = os.path.join(filtered_dataset_dir, file)\n",
    "        mol_name = file[:-13]\n",
    "        with open(filepath, 'r') as f:\n",
    "            for index, line in enumerate(f):\n",
    "                mass, intensity = line.split(\" \")\n",
    "                if not math.isnan(float(intensity)):\n",
    "                    test_intensities.at[mol_name, int(mass)-1] = float(intensity)\n",
    "    \n",
    "    return test_intensities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_has_substructure(content, intensities):\n",
    "    has_substructure_truth_values = []\n",
    "    for index in intensities.index:\n",
    "        has_substructure_truth_values.append(int(content[int(index.split('_')[2]) - 1][:-1]))\n",
    "        \n",
    "    return has_substructure_truth_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a simple keras model to classify this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model,Sequential\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.optimizers import SGD\n",
    "def baseline_model(input_to_network):\n",
    "    class_model = Sequential()\n",
    "    class_model.add(Dense(50, input_dim=input_to_network.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "    class_model.add(Dense(10,kernel_initializer='normal',activation = 'relu'))\n",
    "    class_model.add(Dense(1,kernel_initializer='normal',activation = 'sigmoid'))\n",
    "    class_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return class_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following model is a convolutional model for this data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These methods generate spectra that include these shifts and convert the data into the necessary tensor format\n",
    "\n",
    "- To generate a spectrum, we choose a number of peaks from a Poisson\n",
    "- We then sample a shift from the class-specific shift distribution\n",
    "- Sample a starting point\n",
    "- Sample an intensity for the start and end\n",
    "- Increment the spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_spec_into_tensor(spec_array,shift=[10,20]):\n",
    "    n_spec,n_bins = spec_array.shape\n",
    "    spec_tensor = np.zeros((n_spec,n_bins,len(shift)+1),np.double)\n",
    "    for i in range(n_spec):\n",
    "        spec = spec_array[i,:]\n",
    "        shift_spec = np.copy(spec)\n",
    "        for s in shift:\n",
    "            shift_spec = np.vstack((shift_spec,np.hstack((spec[s:],np.zeros(s)))))\n",
    "        spec_tensor[i,:,:] = shift_spec.T\n",
    "    return spec_tensor[:,:,:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(X,n_noise = 5000):\n",
    "    N,M = X.shape\n",
    "    for n in range(N):\n",
    "        X[n,:] += np.random.multinomial(np.random.poisson(n_noise),[1.0/M for m in range(M)])\n",
    "    return X\n",
    "def normalise(X):\n",
    "    for i,row in enumerate(X):\n",
    "        tot = row.sum()\n",
    "        if tot > 0:\n",
    "            for j in range(len(row)):\n",
    "                X[i,j] = (1.0*row[j])/tot\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Generate data for the two classes, add some noise and normalise\n",
    "- Note that `use_shifts` is what the model uses.\n",
    "- We don't need all the possible shifts to be in `use_shifts` as long as the kernel has sufficient width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D,Flatten,MaxPooling2D,AveragePooling2D\n",
    "def conv_model(n_bins = 1000, n_kernels = 2,kernel_width=1,use_shifts=[15]):\n",
    "    class_model = Sequential()\n",
    "    input_shape = (n_bins,len(use_shifts)+1,1)\n",
    "    n_kernels = n_kernels\n",
    "    \n",
    "    pool_width = n_bins - (kernel_width+1)\n",
    "    pool_size = (pool_width,1)\n",
    "    \n",
    "    kernel_size = (kernel_width,len(use_shifts)+1)\n",
    "    class_model.add(Conv2D(n_kernels, input_shape=input_shape, \n",
    "                           kernel_initializer='normal', activation='relu',strides=1,\n",
    "                          kernel_size = kernel_size))\n",
    "\n",
    "#     class_model.add(AveragePooling2D(pool_size = pool_size))\n",
    "    class_model.add(MaxPooling2D(pool_size = pool_size))\n",
    "    class_model.add(Flatten())\n",
    "    class_model.add(Dense(1,kernel_initializer='normal',activation = 'sigmoid'))\n",
    "    class_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return class_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second model for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_peak_differences(spec):\n",
    "    non_zero_peaks = list(np.where(spec>0)[0])\n",
    "    peak_differences = [(abs(i-j), (spec[i]+spec[j]/2.0)) for i in non_zero_peaks for j in non_zero_peaks if i != j and j > i]\n",
    "    \n",
    "    return peak_differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_shift_bins(intensities):\n",
    "    shift_bins = np.zeros(intensities.shape, np.double)\n",
    "\n",
    "    for index, spec in enumerate(intensities.values):\n",
    "        peak_differences = calc_peak_differences(spec)\n",
    "        for shift, average_intensity in peak_differences:\n",
    "            shift_bins[index, shift - 1] += average_intensity\n",
    "\n",
    "    return shift_bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_conv_diff_splits(path, name, has_substruct_dataset, shift, splits=10):\n",
    "    global filtered_dataset_dir\n",
    "    global training_random_ones\n",
    "    global training_random_zeros\n",
    "    global test_random_ones\n",
    "    global test_random_zeros\n",
    "    epochs = 200\n",
    "    extra_epochs = 100\n",
    "    path = path + name\n",
    "    n_kernels = 3\n",
    "    kernel_width = 5\n",
    "    truth_values = []\n",
    "    test_truth_values = []\n",
    "\n",
    "    with open(has_substruct_dataset, 'r') as f:\n",
    "        content = f.readlines()\n",
    "\n",
    "    with open(path, 'w') as f:\n",
    "        for i in range(splits):\n",
    "            number_of_training_set = calculate_number_of_training_set(has_substruct_dataset)\n",
    "            build_training_and_test_set(number_of_training_set)\n",
    "\n",
    "            training_random_ones_id = get_list_of_ids(training_random_ones)\n",
    "            training_random_zeros_id = get_list_of_ids(training_random_zeros)\n",
    "            training_set = training_random_ones_id + training_random_zeros_id\n",
    "\n",
    "            test_random_ones_id = get_list_of_ids(test_random_ones)\n",
    "            test_random_zeros_id = get_list_of_ids(test_random_zeros)\n",
    "            test_set = test_random_ones_id + test_random_zeros_id\n",
    "            \n",
    "            intensities = load_training_spec(filtered_dataset_dir)\n",
    "            intensities = intensities.reindex(index=intensities.index.to_series().str.rsplit('_').str[-1].astype(int).sort_values().index)\n",
    "\n",
    "            test_intensities = load_test_spec(filtered_dataset_dir)\n",
    "            test_intensities = test_intensities.reindex(index=test_intensities.index.to_series().str.rsplit('_').str[-1].astype(int).sort_values().index)\n",
    "\n",
    "            truth_values = load_has_substructure(content, intensities)\n",
    "            test_truth_values = load_has_substructure(content, test_intensities)\n",
    "            \n",
    "            X = intensities.values\n",
    "            N,M = X.shape\n",
    "            shuffle_order = np.random.permutation(N)\n",
    "            labels = truth_values\n",
    "            labels = np.array(labels)[:,None]\n",
    "            test_labels = test_truth_values\n",
    "            test_labels = np.array(test_labels)[:,None]\n",
    "            \n",
    "            use_shifts = np.asarray([shift])\n",
    "\n",
    "            X_tensor = shift_spec_into_tensor(X,shift=use_shifts)\n",
    "            print(X_tensor.shape)\n",
    "            \n",
    "            mod = conv_model(n_kernels = n_kernels, kernel_width=kernel_width, use_shifts=use_shifts)\n",
    "            mod.fit(X_tensor[shuffle_order,:,:],labels[shuffle_order],epochs=epochs,validation_split=0.2,verbose=0)\n",
    "            \n",
    "            test_tensor = shift_spec_into_tensor(test_intensities.values,shift=use_shifts)\n",
    "            predicted = mod.predict(test_tensor)\n",
    "            \n",
    "            auc = roc_auc_score(test_labels, predicted)\n",
    "            f.write(str(auc) + \"\\n\")\n",
    "        \n",
    "def train_shifts_diff_splits(path, name, has_substruct_dataset, splits=10):\n",
    "    global filtered_dataset_dir\n",
    "    global training_random_ones\n",
    "    global training_random_zeros\n",
    "    global test_random_ones\n",
    "    global test_random_zeros\n",
    "    epochs = 200\n",
    "    extra_epochs = 100\n",
    "    path = path + name\n",
    "    truth_values = []\n",
    "    test_truth_values = []\n",
    "\n",
    "    with open(has_substruct_dataset, 'r') as f:\n",
    "        content = f.readlines()\n",
    "\n",
    "    with open(path, 'w') as f:\n",
    "        for i in range(splits):\n",
    "            number_of_training_set = calculate_number_of_training_set(has_substruct_dataset)\n",
    "            build_training_and_test_set(number_of_training_set)\n",
    "\n",
    "            training_random_ones_id = get_list_of_ids(training_random_ones)\n",
    "            training_random_zeros_id = get_list_of_ids(training_random_zeros)\n",
    "            training_set = training_random_ones_id + training_random_zeros_id\n",
    "\n",
    "            test_random_ones_id = get_list_of_ids(test_random_ones)\n",
    "            test_random_zeros_id = get_list_of_ids(test_random_zeros)\n",
    "            test_set = test_random_ones_id + test_random_zeros_id\n",
    "\n",
    "            intensities = load_training_spec(filtered_dataset_dir)\n",
    "            intensities = intensities.reindex(index=intensities.index.to_series().str.rsplit('_').str[-1].astype(int).sort_values().index)\n",
    "           \n",
    "            test_intensities = load_test_spec(filtered_dataset_dir)\n",
    "            test_intensities = test_intensities.reindex(index=test_intensities.index.to_series().str.rsplit('_').str[-1].astype(int).sort_values().index)\n",
    "\n",
    "            truth_values = load_has_substructure(content, intensities)\n",
    "            test_truth_values = load_has_substructure(content, test_intensities)\n",
    "\n",
    "            X = intensities.values\n",
    "            N,M = X.shape\n",
    "            shuffle_order = np.random.permutation(N)\n",
    "            labels = truth_values\n",
    "            labels = np.array(labels)[:,None]\n",
    "            test_labels = test_truth_values\n",
    "            test_labels = np.array(test_labels)[:,None]\n",
    "\n",
    "            shift_bins = load_shift_bins(intensities)\n",
    "\n",
    "            X_shifts = normalise(shift_bins)\n",
    "            print(X_shifts.shape)\n",
    "\n",
    "            mod = baseline_model(X_shifts)\n",
    "            mod.fit(X_shifts[shuffle_order,:],labels[shuffle_order],epochs=extra_epochs,validation_split=0.2,verbose=0)\n",
    "\n",
    "            test_shift_bins = load_shift_bins(test_intensities)\n",
    "            X_test_shifts = normalise(test_shift_bins)\n",
    "            print(X_test_shifts.shape)\n",
    "\n",
    "            predicted = mod.predict(X_test_shifts)\n",
    "\n",
    "            auc = roc_auc_score(test_labels, predicted)\n",
    "            f.write(str(auc) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Alanine', 71), ('Arginine', 156), ('Asparagine', 114), ('Aspartic Acid', 115), ('Cysteine', 103), ('Glutamic Acid', 129), ('Glutamine', 128), ('Glycine', 57), ('Histidine', 137), ('Isoleucine', 113), ('Leucine', 113), ('Lysine', 128), ('Methionine', 131), ('Phenylalanine', 147), ('Proline', 97), ('Serine', 87), ('Threonine', 101), ('Tryptophan', 186), ('Tyrosine', 163), ('Valine', 99)]\n"
     ]
    }
   ],
   "source": [
    "amino_acid_path = \"G:\\\\Dev\\\\Data\\\\Fragment Masses.txt\"\n",
    "amino_acids_with_shifts = []\n",
    "\n",
    "with open(amino_acid_path, 'r') as f:\n",
    "    for line in f:\n",
    "        amino_acid, shift = line.split(\", \")\n",
    "        amino_acids_with_shifts.append((amino_acid, int(float(shift[:-1]))))\n",
    "\n",
    "print(amino_acids_with_shifts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2934, 1000)\n",
      "(1260, 1000)\n",
      "(2934, 1000)\n",
      "(1260, 1000)\n",
      "(2934, 1000)\n",
      "(1260, 1000)\n",
      "(2934, 1000)\n",
      "(1260, 1000)\n",
      "(2934, 1000)\n",
      "(1260, 1000)\n",
      "(2934, 1000)\n",
      "(1260, 1000)\n",
      "(2934, 1000)\n",
      "(1260, 1000)\n",
      "(2934, 1000)\n",
      "(1260, 1000)\n",
      "(2934, 1000)\n",
      "(1260, 1000)\n",
      "(2934, 1000)\n",
      "(1260, 1000)\n",
      "(2934, 1000, 2, 1)\n",
      "(2934, 1000, 2, 1)\n",
      "(2934, 1000, 2, 1)\n",
      "(2934, 1000, 2, 1)\n",
      "(2934, 1000, 2, 1)\n",
      "(2934, 1000, 2, 1)\n",
      "(2934, 1000, 2, 1)\n",
      "(2934, 1000, 2, 1)\n",
      "(2934, 1000, 2, 1)\n",
      "(2934, 1000, 2, 1)\n",
      "(750, 1000)\n",
      "(324, 1000)\n",
      "(750, 1000)\n",
      "(324, 1000)\n",
      "(750, 1000)\n",
      "(324, 1000)\n",
      "(750, 1000)\n",
      "(324, 1000)\n",
      "(750, 1000)\n",
      "(324, 1000)\n",
      "(750, 1000)\n",
      "(324, 1000)\n",
      "(750, 1000)\n",
      "(324, 1000)\n",
      "(750, 1000)\n",
      "(324, 1000)\n",
      "(750, 1000)\n",
      "(324, 1000)\n",
      "(750, 1000)\n",
      "(324, 1000)\n",
      "(750, 1000, 2, 1)\n",
      "(750, 1000, 2, 1)\n",
      "(750, 1000, 2, 1)\n",
      "(750, 1000, 2, 1)\n",
      "(750, 1000, 2, 1)\n",
      "(750, 1000, 2, 1)\n",
      "(750, 1000, 2, 1)\n",
      "(750, 1000, 2, 1)\n",
      "(750, 1000, 2, 1)\n",
      "(750, 1000, 2, 1)\n",
      "(764, 1000)\n",
      "(330, 1000)\n",
      "(764, 1000)\n",
      "(330, 1000)\n",
      "(764, 1000)\n",
      "(330, 1000)\n",
      "(764, 1000)\n",
      "(330, 1000)\n",
      "(764, 1000)\n",
      "(330, 1000)\n",
      "(764, 1000)\n",
      "(330, 1000)\n",
      "(764, 1000)\n",
      "(330, 1000)\n",
      "(764, 1000)\n",
      "(330, 1000)\n",
      "(764, 1000)\n",
      "(330, 1000)\n",
      "(764, 1000)\n",
      "(330, 1000)\n",
      "(764, 1000, 2, 1)\n",
      "(764, 1000, 2, 1)\n",
      "(764, 1000, 2, 1)\n",
      "(764, 1000, 2, 1)\n",
      "(764, 1000, 2, 1)\n",
      "(764, 1000, 2, 1)\n",
      "(764, 1000, 2, 1)\n",
      "(764, 1000, 2, 1)\n",
      "(764, 1000, 2, 1)\n",
      "(764, 1000, 2, 1)\n",
      "(872, 1000)\n",
      "(376, 1000)\n",
      "(872, 1000)\n",
      "(376, 1000)\n",
      "(872, 1000)\n",
      "(376, 1000)\n",
      "(872, 1000)\n",
      "(376, 1000)\n",
      "(872, 1000)\n",
      "(376, 1000)\n",
      "(872, 1000)\n",
      "(376, 1000)\n",
      "(872, 1000)\n",
      "(376, 1000)\n",
      "(872, 1000)\n",
      "(376, 1000)\n",
      "(872, 1000)\n",
      "(376, 1000)\n",
      "(872, 1000)\n",
      "(376, 1000)\n",
      "(872, 1000, 2, 1)\n",
      "(872, 1000, 2, 1)\n",
      "(872, 1000, 2, 1)\n",
      "(872, 1000, 2, 1)\n",
      "(872, 1000, 2, 1)\n",
      "(872, 1000, 2, 1)\n",
      "(872, 1000, 2, 1)\n",
      "(872, 1000, 2, 1)\n",
      "(872, 1000, 2, 1)\n",
      "(872, 1000, 2, 1)\n",
      "(610, 1000)\n",
      "(264, 1000)\n",
      "(610, 1000)\n",
      "(264, 1000)\n",
      "(610, 1000)\n",
      "(264, 1000)\n",
      "(610, 1000)\n",
      "(264, 1000)\n",
      "(610, 1000)\n",
      "(264, 1000)\n",
      "(610, 1000)\n",
      "(264, 1000)\n",
      "(610, 1000)\n",
      "(264, 1000)\n",
      "(610, 1000)\n",
      "(264, 1000)\n",
      "(610, 1000)\n",
      "(264, 1000)\n",
      "(610, 1000)\n",
      "(264, 1000)\n",
      "(610, 1000, 2, 1)\n",
      "(610, 1000, 2, 1)\n",
      "(610, 1000, 2, 1)\n",
      "(610, 1000, 2, 1)\n",
      "(610, 1000, 2, 1)\n",
      "(610, 1000, 2, 1)\n",
      "(610, 1000, 2, 1)\n",
      "(610, 1000, 2, 1)\n",
      "(610, 1000, 2, 1)\n",
      "(610, 1000, 2, 1)\n",
      "(644, 1000)\n",
      "(276, 1000)\n",
      "(644, 1000)\n",
      "(276, 1000)\n",
      "(644, 1000)\n",
      "(276, 1000)\n",
      "(644, 1000)\n",
      "(276, 1000)\n",
      "(644, 1000)\n",
      "(276, 1000)\n",
      "(644, 1000)\n",
      "(276, 1000)\n",
      "(644, 1000)\n",
      "(276, 1000)\n",
      "(644, 1000)\n",
      "(276, 1000)\n",
      "(644, 1000)\n",
      "(276, 1000)\n",
      "(644, 1000)\n",
      "(276, 1000)\n",
      "(644, 1000, 2, 1)\n",
      "(644, 1000, 2, 1)\n",
      "(644, 1000, 2, 1)\n",
      "(644, 1000, 2, 1)\n",
      "(644, 1000, 2, 1)\n",
      "(644, 1000, 2, 1)\n",
      "(644, 1000, 2, 1)\n",
      "(644, 1000, 2, 1)\n",
      "(644, 1000, 2, 1)\n",
      "(644, 1000, 2, 1)\n",
      "(702, 1000)\n",
      "(302, 1000)\n",
      "(702, 1000)\n",
      "(302, 1000)\n",
      "(702, 1000)\n",
      "(302, 1000)\n",
      "(702, 1000)\n",
      "(302, 1000)\n",
      "(702, 1000)\n",
      "(302, 1000)\n",
      "(702, 1000)\n",
      "(302, 1000)\n",
      "(702, 1000)\n",
      "(302, 1000)\n",
      "(702, 1000)\n",
      "(302, 1000)\n",
      "(702, 1000)\n",
      "(302, 1000)\n",
      "(702, 1000)\n",
      "(302, 1000)\n",
      "(702, 1000, 2, 1)\n",
      "(702, 1000, 2, 1)\n",
      "(702, 1000, 2, 1)\n",
      "(702, 1000, 2, 1)\n",
      "(702, 1000, 2, 1)\n",
      "(702, 1000, 2, 1)\n",
      "(702, 1000, 2, 1)\n",
      "(702, 1000, 2, 1)\n",
      "(702, 1000, 2, 1)\n",
      "(702, 1000, 2, 1)\n",
      "(728, 1000)\n",
      "(314, 1000)\n",
      "(728, 1000)\n",
      "(314, 1000)\n",
      "(728, 1000)\n",
      "(314, 1000)\n",
      "(728, 1000)\n",
      "(314, 1000)\n",
      "(728, 1000)\n",
      "(314, 1000)\n",
      "(728, 1000)\n",
      "(314, 1000)\n",
      "(728, 1000)\n",
      "(314, 1000)\n",
      "(728, 1000)\n",
      "(314, 1000)\n",
      "(728, 1000)\n",
      "(314, 1000)\n",
      "(728, 1000)\n",
      "(314, 1000)\n",
      "(728, 1000, 2, 1)\n",
      "(728, 1000, 2, 1)\n",
      "(728, 1000, 2, 1)\n",
      "(728, 1000, 2, 1)\n",
      "(728, 1000, 2, 1)\n",
      "(728, 1000, 2, 1)\n",
      "(728, 1000, 2, 1)\n",
      "(728, 1000, 2, 1)\n",
      "(728, 1000, 2, 1)\n",
      "(728, 1000, 2, 1)\n",
      "(716, 1000)\n",
      "(308, 1000)\n",
      "(716, 1000)\n",
      "(308, 1000)\n",
      "(716, 1000)\n",
      "(308, 1000)\n",
      "(716, 1000)\n",
      "(308, 1000)\n",
      "(716, 1000)\n",
      "(308, 1000)\n",
      "(716, 1000)\n",
      "(308, 1000)\n",
      "(716, 1000)\n",
      "(308, 1000)\n",
      "(716, 1000)\n",
      "(308, 1000)\n",
      "(716, 1000)\n",
      "(308, 1000)\n",
      "(716, 1000)\n",
      "(308, 1000)\n",
      "(716, 1000, 2, 1)\n",
      "(716, 1000, 2, 1)\n",
      "(716, 1000, 2, 1)\n",
      "(716, 1000, 2, 1)\n",
      "(716, 1000, 2, 1)\n",
      "(716, 1000, 2, 1)\n",
      "(716, 1000, 2, 1)\n",
      "(716, 1000, 2, 1)\n",
      "(716, 1000, 2, 1)\n",
      "(716, 1000, 2, 1)\n"
     ]
    }
   ],
   "source": [
    "processed_amino_acid = [\"Alanine\", \"Arginine\", \"Glutamine\", \"Asparagine\", \"Aspartic Acid\", \"Cysteine\", \"Glutamic Acid\", \n",
    "                        \"Glycine\", \"Histidine\", \"Isoleucine\", \"Leucine\"]\n",
    "missing_amino_acid = \"Glutamine\"\n",
    "path = \"G:\\\\Dev\\\\Data\\\\Convolution vs Dense Experiments corrected\\\\\"\n",
    "\n",
    "for amino_acid, shift in amino_acids_with_shifts:\n",
    "    if amino_acid != missing_amino_acid and amino_acid not in processed_amino_acid:\n",
    "        dataset = \"G:\\\\Dev\\\\Data\\\\NIST Amino Acids\\\\NIST {} Has Substructure.txt\".format(amino_acid)\n",
    "        train_shifts_diff_splits(path, \"Shifts {} AUC.txt\".format(amino_acid), dataset)\n",
    "        train_conv_diff_splits(path, \"Conv {} AUC.txt\".format(amino_acid), dataset, shift, splits=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Shifts', 'Conv'], dtype='object')\n",
      "          0\n",
      "0  0.852761\n",
      "1  0.820384\n",
      "2  0.821391\n",
      "3  0.899595\n",
      "4  0.844519\n",
      "5  0.840184\n",
      "6  0.890185\n",
      "7  0.903554\n",
      "8  0.848412\n",
      "9  0.888131\n",
      "          0\n",
      "0  0.714301\n",
      "1  0.588177\n",
      "2  0.669562\n",
      "3  0.677857\n",
      "4  0.663588\n",
      "5  0.691817\n",
      "6  0.653387\n",
      "7  0.689710\n",
      "8  0.677173\n",
      "9  0.560512\n",
      "     Shifts      Conv\n",
      "0  0.852761  0.714301\n",
      "1  0.820384  0.588177\n",
      "2  0.821391  0.669562\n",
      "3  0.899595  0.677857\n",
      "4  0.844519  0.663588\n",
      "5  0.840184  0.691817\n",
      "6  0.890185  0.653387\n",
      "7  0.903554  0.689710\n",
      "8  0.848412  0.677173\n",
      "9  0.888131  0.560512\n"
     ]
    }
   ],
   "source": [
    "path = \"G:\\\\Dev\\\\Data\\\\Convolution vs Dense Experiments corrected\\\\\"\n",
    "df_shifts = pd.read_csv(path + \"Shifts Alanine AUC.txt\", header=None)\n",
    "df_conv = pd.read_csv(path + \"Conv Alanine AUC.txt\", header=None)\n",
    "df_combined = pd.concat([df_shifts, df_conv], axis=1)\n",
    "df_combined.columns = [\"Shifts\", \"Conv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2680fa11780>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_combined[['Shifts', 'Conv']].plot(kind='box')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_relResult(statistic=11.221065799556373, pvalue=1.360958273920665e-06)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.ttest_rel(df_combined['Shifts'], df_combined['Conv'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alanine: Ttest_relResult(statistic=11.221065799556373, pvalue=1.360958273920665e-06)\n",
      "Arginine: Ttest_relResult(statistic=28.5750086111355, pvalue=3.8328019853930337e-10)\n",
      "Asparagine: Ttest_relResult(statistic=16.725438280769435, pvalue=4.36717980880867e-08)\n",
      "Aspartic Acid: Ttest_relResult(statistic=10.596471853565722, pvalue=2.20475016639447e-06)\n",
      "Cysteine: Ttest_relResult(statistic=20.999878665699455, pvalue=5.902295392507168e-09)\n",
      "Glutamic Acid: Ttest_relResult(statistic=16.11574726396528, pvalue=6.04057600384934e-08)\n",
      "Glycine: Ttest_relResult(statistic=21.29469907408046, pvalue=5.21802415846084e-09)\n",
      "Histidine: Ttest_relResult(statistic=24.686189478071988, pvalue=1.4084292165671842e-09)\n",
      "Isoleucine: Ttest_relResult(statistic=22.404763799156985, pvalue=3.3283701743627767e-09)\n",
      "Leucine: Ttest_relResult(statistic=23.173613772424893, pvalue=2.468281924435496e-09)\n",
      "Lysine: Ttest_relResult(statistic=17.375865382206925, pvalue=3.127198069649469e-08)\n",
      "Methionine: Ttest_relResult(statistic=15.327817324443428, pvalue=9.348512085218057e-08)\n",
      "Phenylalanine: Ttest_relResult(statistic=26.276007564751552, pvalue=8.08769109802662e-10)\n",
      "Proline: Ttest_relResult(statistic=25.627502344919723, pvalue=1.0100222289747276e-09)\n",
      "Serine: Ttest_relResult(statistic=12.40721886217737, pvalue=5.791774484541126e-07)\n",
      "Threonine: Ttest_relResult(statistic=16.519758319259353, pvalue=4.866063644398779e-08)\n",
      "Tryptophan: Ttest_relResult(statistic=14.258275934854591, pvalue=1.7509872047212962e-07)\n",
      "Tyrosine: Ttest_relResult(statistic=14.81830887140648, pvalue=1.2539952191166017e-07)\n",
      "Valine: Ttest_relResult(statistic=21.626969016103633, pvalue=4.550337194863764e-09)\n"
     ]
    }
   ],
   "source": [
    "path = \"G:\\\\Dev\\\\Data\\\\Convolution vs Dense Experiments corrected\\\\\"\n",
    "missing_amino_acid = \"Glutamine\"\n",
    "\n",
    "for amino_acid, shift in amino_acids_with_shifts:\n",
    "    if amino_acid != missing_amino_acid:        \n",
    "        df_shifts = pd.read_csv(path + \"Shifts {} AUC.txt\".format(amino_acid), header=None)\n",
    "        df_conv = pd.read_csv(path + \"Conv {} AUC.txt\".format(amino_acid), header=None)\n",
    "        df_combined = pd.concat([df_shifts, df_conv], axis=1)\n",
    "        df_combined.columns = [\"Shifts\", \"Conv\"]\n",
    "        t_test = stats.ttest_rel(df_combined['Shifts'], df_combined['Conv'])\n",
    "        print(amino_acid + \": \" + str(t_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

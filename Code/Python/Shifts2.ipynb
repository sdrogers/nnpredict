{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â Notebook to play with convolution for networks\n",
    "\n",
    "- Setup a binary classification task\n",
    "- Have peak probabilities for each class\n",
    "- Then have some shifts also for the class and see if convolution can improve this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from matplotlib import pylab as plt\n",
    "\n",
    "import pandas as pd\n",
    "import math\n",
    "import random\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_random_ones = None\n",
    "training_random_zeros = None\n",
    "test_random_ones = None\n",
    "test_random_zeros = None\n",
    "\n",
    "all_ones = None\n",
    "all_zeros = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_number_of_training_set(has_substructure_path):\n",
    "    global all_ones\n",
    "    global all_zeros\n",
    "    substructure = np.loadtxt(has_substructure_path, np.int)\n",
    "    all_ones = np.where(substructure == 1)\n",
    "    all_zeros = np.where(substructure == 0)\n",
    "\n",
    "    number_of_training_set = int(len(all_ones[0]) * 0.7)\n",
    "    return number_of_training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_training_and_test_set(num_of_training_set):\n",
    "    global training_random_ones\n",
    "    global training_random_zeros\n",
    "    global test_random_ones\n",
    "    global test_random_zeros\n",
    "\n",
    "    training_random_ones = random.sample(list(all_ones[0]), num_of_training_set)\n",
    "    training_random_zeros = random.sample(list(all_zeros[0]), num_of_training_set)\n",
    "\n",
    "    test_random_ones = [ones for ones in all_ones[0] if ones not in training_random_ones]\n",
    "    test_random_zeros = [zeros for zeros in all_zeros[0] if zeros not in training_random_zeros]\n",
    "    test_random_zeros = random.sample(test_random_zeros, len(test_random_ones))\n",
    "    \n",
    "def get_list_of_ids(dataset):\n",
    "    return [\"GNPS_ALL_\" + str(index+1) for index in dataset]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building training and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_dataset_dir = \"G:\\\\Dev\\\\Data\\\\MSMS-NIST\\\\Python Filtered\"\n",
    "\n",
    "def load_training_spec(spec_path):\n",
    "    global training_random_ones\n",
    "    global training_random_zeros\n",
    "    training_random_ones_id = get_list_of_ids(training_random_ones)\n",
    "    training_random_zeros_id = get_list_of_ids(training_random_zeros)\n",
    "    training_set = training_random_ones_id + training_random_zeros_id\n",
    "    \n",
    "    file_list = os.listdir(spec_path)\n",
    "    \n",
    "    filtered_file_list = [file for file in file_list if file[:-13] in training_set]\n",
    "    filtered_name_list = [filename[:-13] for filename in filtered_file_list]\n",
    "\n",
    "    intensities = pd.DataFrame(0.0, index = filtered_name_list, columns=range(1000), dtype=float)\n",
    "\n",
    "    for file in filtered_file_list:\n",
    "        filepath = os.path.join(filtered_dataset_dir, file)\n",
    "        mol_name = file[:-13]\n",
    "        with open(filepath, 'r') as f:\n",
    "            for index, line in enumerate(f):\n",
    "                mass, intensity = line.split(\" \")\n",
    "                if not math.isnan(float(intensity)):\n",
    "                    intensities.at[mol_name, int(mass)-1] = float(intensity)\n",
    "    \n",
    "    return intensities\n",
    "\n",
    "def load_test_spec(spec_path):\n",
    "    global test_random_ones\n",
    "    global test_random_zeros\n",
    "    test_random_ones_id = get_list_of_ids(test_random_ones)\n",
    "    test_random_zeros_id = get_list_of_ids(test_random_zeros)\n",
    "    test_set = test_random_ones_id + test_random_zeros_id\n",
    "    file_list = os.listdir(spec_path)\n",
    "    \n",
    "    filtered_file_list = [file for file in file_list if file[:-13] in test_set]\n",
    "    filtered_name_list = [filename[:-13] for filename in filtered_file_list]\n",
    "\n",
    "    test_intensities = pd.DataFrame(0.0, index = filtered_name_list, columns=range(1000), dtype=float)\n",
    "\n",
    "    for file in filtered_file_list:\n",
    "        filepath = os.path.join(filtered_dataset_dir, file)\n",
    "        mol_name = file[:-13]\n",
    "        with open(filepath, 'r') as f:\n",
    "            for index, line in enumerate(f):\n",
    "                mass, intensity = line.split(\" \")\n",
    "                if not math.isnan(float(intensity)):\n",
    "                    test_intensities.at[mol_name, int(mass)-1] = float(intensity)\n",
    "    \n",
    "    return test_intensities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_has_substructure(content, intensities):\n",
    "    has_substructure_truth_values = []\n",
    "    for index in intensities.index:\n",
    "        has_substructure_truth_values.append(int(content[int(index.split('_')[2]) - 1][:-1]))\n",
    "        \n",
    "    return has_substructure_truth_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a simple keras model to classify this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model,Sequential\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.optimizers import SGD\n",
    "def baseline_model(input_to_network):\n",
    "    class_model = Sequential()\n",
    "    class_model.add(Dense(50, input_dim=input_to_network.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "    class_model.add(Dense(10,kernel_initializer='normal',activation = 'relu'))\n",
    "    class_model.add(Dense(1,kernel_initializer='normal',activation = 'sigmoid'))\n",
    "    class_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return class_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following model is a convolutional model for this data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These methods generate spectra that include these shifts and convert the data into the necessary tensor format\n",
    "\n",
    "- To generate a spectrum, we choose a number of peaks from a Poisson\n",
    "- We then sample a shift from the class-specific shift distribution\n",
    "- Sample a starting point\n",
    "- Sample an intensity for the start and end\n",
    "- Increment the spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_spec_into_tensor(spec_array,shift=[10,20]):\n",
    "    n_spec,n_bins = spec_array.shape\n",
    "    spec_tensor = np.zeros((n_spec,n_bins,len(shift)+1),np.double)\n",
    "    for i in range(n_spec):\n",
    "        spec = spec_array[i,:]\n",
    "        shift_spec = np.copy(spec)\n",
    "        for s in shift:\n",
    "            shift_spec = np.vstack((shift_spec,np.hstack((spec[s:],np.zeros(s)))))\n",
    "        spec_tensor[i,:,:] = shift_spec.T\n",
    "    return spec_tensor[:,:,:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(X,n_noise = 5000):\n",
    "    N,M = X.shape\n",
    "    for n in range(N):\n",
    "        X[n,:] += np.random.multinomial(np.random.poisson(n_noise),[1.0/M for m in range(M)])\n",
    "    return X\n",
    "def normalise(X):\n",
    "    for i,row in enumerate(X):\n",
    "        tot = row.sum()\n",
    "        if tot > 0:\n",
    "            for j in range(len(row)):\n",
    "                X[i,j] = (1.0*row[j])/tot\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Generate data for the two classes, add some noise and normalise\n",
    "- Note that `use_shifts` is what the model uses.\n",
    "- We don't need all the possible shifts to be in `use_shifts` as long as the kernel has sufficient width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D,Flatten,MaxPooling2D,AveragePooling2D\n",
    "def conv_model(n_bins = 1000, n_kernels = 2,kernel_width=1,use_shifts=[15]):\n",
    "    class_model = Sequential()\n",
    "    input_shape = (n_bins,len(use_shifts)+1,1)\n",
    "    n_kernels = n_kernels\n",
    "    \n",
    "    pool_width = n_bins - (kernel_width+1)\n",
    "    pool_size = (pool_width,1)\n",
    "    \n",
    "    kernel_size = (kernel_width,len(use_shifts)+1)\n",
    "    class_model.add(Conv2D(n_kernels, input_shape=input_shape, \n",
    "                           kernel_initializer='normal', activation='relu',strides=1,\n",
    "                          kernel_size = kernel_size))\n",
    "\n",
    "#     class_model.add(AveragePooling2D(pool_size = pool_size))\n",
    "    class_model.add(MaxPooling2D(pool_size = pool_size))\n",
    "    class_model.add(Flatten())\n",
    "    class_model.add(Dense(1,kernel_initializer='normal',activation = 'sigmoid'))\n",
    "    class_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return class_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second model for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_peak_differences(spec):\n",
    "    non_zero_peaks = list(np.where(spec>0)[0])\n",
    "    peak_differences = [(abs(i-j), (spec[i]+spec[j]/2.0)) for i in non_zero_peaks for j in non_zero_peaks if i != j and j > i]\n",
    "    \n",
    "    return peak_differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_shift_bins(intensities):\n",
    "    shift_bins = np.zeros(intensities.shape, np.double)\n",
    "\n",
    "    for index, spec in enumerate(intensities.values):\n",
    "        peak_differences = calc_peak_differences(spec)\n",
    "        for shift, average_intensity in peak_differences:\n",
    "            shift_bins[index, shift - 1] += average_intensity\n",
    "\n",
    "    return shift_bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_conv_diff_splits(path, name, has_substruct_dataset, shift, splits=10):\n",
    "    global filtered_dataset_dir\n",
    "    global training_random_ones\n",
    "    global training_random_zeros\n",
    "    global test_random_ones\n",
    "    global test_random_zeros\n",
    "    epochs = 200\n",
    "    extra_epochs = 100\n",
    "    path = path + name\n",
    "    n_kernels = 3\n",
    "    kernel_width = 5\n",
    "    truth_values = []\n",
    "    test_truth_values = []\n",
    "\n",
    "    with open(has_substruct_dataset, 'r') as f:\n",
    "        content = f.readlines()\n",
    "\n",
    "    with open(path, 'w') as f:\n",
    "        for i in range(splits):\n",
    "            number_of_training_set = calculate_number_of_training_set(has_substruct_dataset)\n",
    "            build_training_and_test_set(number_of_training_set)\n",
    "\n",
    "            training_random_ones_id = get_list_of_ids(training_random_ones)\n",
    "            training_random_zeros_id = get_list_of_ids(training_random_zeros)\n",
    "            training_set = training_random_ones_id + training_random_zeros_id\n",
    "\n",
    "            test_random_ones_id = get_list_of_ids(test_random_ones)\n",
    "            test_random_zeros_id = get_list_of_ids(test_random_zeros)\n",
    "            test_set = test_random_ones_id + test_random_zeros_id\n",
    "            \n",
    "            intensities = load_training_spec(filtered_dataset_dir)\n",
    "            intensities = intensities.reindex(index=intensities.index.to_series().str.rsplit('_').str[-1].astype(int).sort_values().index)\n",
    "\n",
    "            test_intensities = load_test_spec(filtered_dataset_dir)\n",
    "            test_intensities = test_intensities.reindex(index=test_intensities.index.to_series().str.rsplit('_').str[-1].astype(int).sort_values().index)\n",
    "\n",
    "            truth_values = load_has_substructure(content, intensities)\n",
    "            test_truth_values = load_has_substructure(content, test_intensities)\n",
    "            \n",
    "            X = intensities.values\n",
    "            N,M = X.shape\n",
    "            shuffle_order = np.random.permutation(N)\n",
    "            labels = truth_values\n",
    "            labels = np.array(labels)[:,None]\n",
    "            test_labels = test_truth_values\n",
    "            test_labels = np.array(test_labels)[:,None]\n",
    "            \n",
    "            use_shifts = np.asarray([shift])\n",
    "\n",
    "            X_tensor = shift_spec_into_tensor(X,shift=use_shifts)\n",
    "            print(X_tensor.shape)\n",
    "            \n",
    "            mod = conv_model(n_kernels = n_kernels, kernel_width=kernel_width, use_shifts=use_shifts)\n",
    "            mod.fit(X_tensor[shuffle_order,:,:],labels[shuffle_order],epochs=epochs,validation_split=0.2,verbose=0)\n",
    "            \n",
    "            test_tensor = shift_spec_into_tensor(test_intensities.values,shift=use_shifts)\n",
    "            predicted = mod.predict(test_tensor)\n",
    "            \n",
    "            auc = roc_auc_score(test_labels, predicted)\n",
    "            f.write(str(auc) + \"\\n\")\n",
    "        \n",
    "def train_shifts_diff_splits(path, name, has_substruct_dataset, splits=10):\n",
    "    global filtered_dataset_dir\n",
    "    global training_random_ones\n",
    "    global training_random_zeros\n",
    "    global test_random_ones\n",
    "    global test_random_zeros\n",
    "    epochs = 200\n",
    "    extra_epochs = 100\n",
    "    path = path + name\n",
    "    truth_values = []\n",
    "    test_truth_values = []\n",
    "\n",
    "    with open(has_substruct_dataset, 'r') as f:\n",
    "        content = f.readlines()\n",
    "\n",
    "    with open(path, 'w') as f:\n",
    "        for i in range(splits):\n",
    "            number_of_training_set = calculate_number_of_training_set(has_substruct_dataset)\n",
    "            build_training_and_test_set(number_of_training_set)\n",
    "\n",
    "            training_random_ones_id = get_list_of_ids(training_random_ones)\n",
    "            training_random_zeros_id = get_list_of_ids(training_random_zeros)\n",
    "            training_set = training_random_ones_id + training_random_zeros_id\n",
    "\n",
    "            test_random_ones_id = get_list_of_ids(test_random_ones)\n",
    "            test_random_zeros_id = get_list_of_ids(test_random_zeros)\n",
    "            test_set = test_random_ones_id + test_random_zeros_id\n",
    "\n",
    "            intensities = load_training_spec(filtered_dataset_dir)\n",
    "            intensities = intensities.reindex(index=intensities.index.to_series().str.rsplit('_').str[-1].astype(int).sort_values().index)\n",
    "           \n",
    "            test_intensities = load_test_spec(filtered_dataset_dir)\n",
    "            test_intensities = test_intensities.reindex(index=test_intensities.index.to_series().str.rsplit('_').str[-1].astype(int).sort_values().index)\n",
    "\n",
    "            truth_values = load_has_substructure(content, intensities)\n",
    "            test_truth_values = load_has_substructure(content, test_intensities)\n",
    "\n",
    "            X = intensities.values\n",
    "            N,M = X.shape\n",
    "            shuffle_order = np.random.permutation(N)\n",
    "            labels = truth_values\n",
    "            labels = np.array(labels)[:,None]\n",
    "            test_labels = test_truth_values\n",
    "            test_labels = np.array(test_labels)[:,None]\n",
    "\n",
    "            shift_bins = load_shift_bins(intensities)\n",
    "\n",
    "            X_shifts = normalise(shift_bins)\n",
    "            print(X_shifts.shape)\n",
    "\n",
    "            mod = baseline_model(X_shifts)\n",
    "            mod.fit(X_shifts[shuffle_order,:],labels[shuffle_order],epochs=extra_epochs,validation_split=0.2,verbose=0)\n",
    "\n",
    "            test_shift_bins = load_shift_bins(test_intensities)\n",
    "            X_test_shifts = normalise(test_shift_bins)\n",
    "            print(X_test_shifts.shape)\n",
    "\n",
    "            predicted = mod.predict(X_test_shifts)\n",
    "\n",
    "            auc = roc_auc_score(test_labels, predicted)\n",
    "            f.write(str(auc) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Alanine', 71), ('Arginine', 156), ('Asparagine', 114), ('Aspartic Acid', 115), ('Cysteine', 103), ('Glutamic Acid', 129), ('Glutamine', 128), ('Glycine', 57), ('Histidine', 137), ('Isoleucine', 113), ('Leucine', 113), ('Lysine', 128), ('Methionine', 131), ('Phenylalanine', 147), ('Proline', 97), ('Serine', 87), ('Threonine', 101), ('Tryptophan', 186), ('Tyrosine', 163), ('Valine', 99)]\n"
     ]
    }
   ],
   "source": [
    "amino_acid_path = \"G:\\\\Dev\\\\Data\\\\Fragment Masses.txt\"\n",
    "amino_acids_with_shifts = []\n",
    "\n",
    "with open(amino_acid_path, 'r') as f:\n",
    "    for line in f:\n",
    "        amino_acid, shift = line.split(\", \")\n",
    "        amino_acids_with_shifts.append((amino_acid, int(float(shift[:-1]))))\n",
    "\n",
    "print(amino_acids_with_shifts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2934, 1000)\n",
      "(1260, 1000)\n",
      "(2934, 1000)\n",
      "(1260, 1000)\n",
      "(2934, 1000)\n",
      "(1260, 1000)\n",
      "(2934, 1000)\n",
      "(1260, 1000)\n",
      "(2934, 1000)\n",
      "(1260, 1000)\n",
      "(2934, 1000)\n",
      "(1260, 1000)\n",
      "(2934, 1000)\n",
      "(1260, 1000)\n",
      "(2934, 1000)\n",
      "(1260, 1000)\n",
      "(2934, 1000)\n",
      "(1260, 1000)\n",
      "(2934, 1000)\n",
      "(1260, 1000)\n",
      "(2934, 1000, 2, 1)\n",
      "(2934, 1000, 2, 1)\n",
      "(2934, 1000, 2, 1)\n",
      "(2934, 1000, 2, 1)\n",
      "(2934, 1000, 2, 1)\n",
      "(2934, 1000, 2, 1)\n",
      "(2934, 1000, 2, 1)\n",
      "(2934, 1000, 2, 1)\n",
      "(2934, 1000, 2, 1)\n",
      "(2934, 1000, 2, 1)\n",
      "(750, 1000)\n",
      "(324, 1000)\n",
      "(750, 1000)\n",
      "(324, 1000)\n",
      "(750, 1000)\n",
      "(324, 1000)\n",
      "(750, 1000)\n",
      "(324, 1000)\n",
      "(750, 1000)\n",
      "(324, 1000)\n",
      "(750, 1000)\n",
      "(324, 1000)\n",
      "(750, 1000)\n",
      "(324, 1000)\n",
      "(750, 1000)\n",
      "(324, 1000)\n",
      "(750, 1000)\n",
      "(324, 1000)\n",
      "(750, 1000)\n",
      "(324, 1000)\n",
      "(750, 1000, 2, 1)\n",
      "(750, 1000, 2, 1)\n",
      "(750, 1000, 2, 1)\n",
      "(750, 1000, 2, 1)\n",
      "(750, 1000, 2, 1)\n",
      "(750, 1000, 2, 1)\n",
      "(750, 1000, 2, 1)\n",
      "(750, 1000, 2, 1)\n",
      "(750, 1000, 2, 1)\n",
      "(750, 1000, 2, 1)\n",
      "(764, 1000)\n",
      "(330, 1000)\n",
      "(764, 1000)\n",
      "(330, 1000)\n",
      "(764, 1000)\n",
      "(330, 1000)\n",
      "(764, 1000)\n",
      "(330, 1000)\n",
      "(764, 1000)\n",
      "(330, 1000)\n",
      "(764, 1000)\n",
      "(330, 1000)\n",
      "(764, 1000)\n",
      "(330, 1000)\n",
      "(764, 1000)\n",
      "(330, 1000)\n",
      "(764, 1000)\n",
      "(330, 1000)\n",
      "(764, 1000)\n",
      "(330, 1000)\n",
      "(764, 1000, 2, 1)\n",
      "(764, 1000, 2, 1)\n",
      "(764, 1000, 2, 1)\n",
      "(764, 1000, 2, 1)\n",
      "(764, 1000, 2, 1)\n",
      "(764, 1000, 2, 1)\n",
      "(764, 1000, 2, 1)\n",
      "(764, 1000, 2, 1)\n",
      "(764, 1000, 2, 1)\n",
      "(764, 1000, 2, 1)\n",
      "(872, 1000)\n",
      "(376, 1000)\n",
      "(872, 1000)\n",
      "(376, 1000)\n",
      "(872, 1000)\n",
      "(376, 1000)\n",
      "(872, 1000)\n",
      "(376, 1000)\n",
      "(872, 1000)\n",
      "(376, 1000)\n",
      "(872, 1000)\n",
      "(376, 1000)\n",
      "(872, 1000)\n",
      "(376, 1000)\n",
      "(872, 1000)\n",
      "(376, 1000)\n",
      "(872, 1000)\n",
      "(376, 1000)\n",
      "(872, 1000)\n",
      "(376, 1000)\n",
      "(872, 1000, 2, 1)\n",
      "(872, 1000, 2, 1)\n",
      "(872, 1000, 2, 1)\n",
      "(872, 1000, 2, 1)\n",
      "(872, 1000, 2, 1)\n",
      "(872, 1000, 2, 1)\n",
      "(872, 1000, 2, 1)\n",
      "(872, 1000, 2, 1)\n",
      "(872, 1000, 2, 1)\n",
      "(872, 1000, 2, 1)\n",
      "(610, 1000)\n",
      "(264, 1000)\n",
      "(610, 1000)\n",
      "(264, 1000)\n",
      "(610, 1000)\n",
      "(264, 1000)\n",
      "(610, 1000)\n",
      "(264, 1000)\n",
      "(610, 1000)\n",
      "(264, 1000)\n",
      "(610, 1000)\n",
      "(264, 1000)\n",
      "(610, 1000)\n",
      "(264, 1000)\n",
      "(610, 1000)\n",
      "(264, 1000)\n",
      "(610, 1000)\n",
      "(264, 1000)\n",
      "(610, 1000)\n",
      "(264, 1000)\n",
      "(610, 1000, 2, 1)\n",
      "(610, 1000, 2, 1)\n",
      "(610, 1000, 2, 1)\n",
      "(610, 1000, 2, 1)\n",
      "(610, 1000, 2, 1)\n",
      "(610, 1000, 2, 1)\n",
      "(610, 1000, 2, 1)\n",
      "(610, 1000, 2, 1)\n",
      "(610, 1000, 2, 1)\n",
      "(610, 1000, 2, 1)\n",
      "(644, 1000)\n",
      "(276, 1000)\n",
      "(644, 1000)\n",
      "(276, 1000)\n",
      "(644, 1000)\n",
      "(276, 1000)\n",
      "(644, 1000)\n",
      "(276, 1000)\n",
      "(644, 1000)\n",
      "(276, 1000)\n",
      "(644, 1000)\n",
      "(276, 1000)\n",
      "(644, 1000)\n",
      "(276, 1000)\n",
      "(644, 1000)\n",
      "(276, 1000)\n",
      "(644, 1000)\n",
      "(276, 1000)\n",
      "(644, 1000)\n",
      "(276, 1000)\n",
      "(644, 1000, 2, 1)\n",
      "(644, 1000, 2, 1)\n",
      "(644, 1000, 2, 1)\n",
      "(644, 1000, 2, 1)\n",
      "(644, 1000, 2, 1)\n",
      "(644, 1000, 2, 1)\n",
      "(644, 1000, 2, 1)\n",
      "(644, 1000, 2, 1)\n",
      "(644, 1000, 2, 1)\n",
      "(644, 1000, 2, 1)\n",
      "(702, 1000)\n",
      "(302, 1000)\n",
      "(702, 1000)\n",
      "(302, 1000)\n",
      "(702, 1000)\n",
      "(302, 1000)\n",
      "(702, 1000)\n",
      "(302, 1000)\n",
      "(702, 1000)\n",
      "(302, 1000)\n",
      "(702, 1000)\n",
      "(302, 1000)\n",
      "(702, 1000)\n",
      "(302, 1000)\n",
      "(702, 1000)\n",
      "(302, 1000)\n",
      "(702, 1000)\n",
      "(302, 1000)\n",
      "(702, 1000)\n",
      "(302, 1000)\n",
      "(702, 1000, 2, 1)\n",
      "(702, 1000, 2, 1)\n",
      "(702, 1000, 2, 1)\n",
      "(702, 1000, 2, 1)\n",
      "(702, 1000, 2, 1)\n",
      "(702, 1000, 2, 1)\n",
      "(702, 1000, 2, 1)\n",
      "(702, 1000, 2, 1)\n",
      "(702, 1000, 2, 1)\n",
      "(702, 1000, 2, 1)\n",
      "(728, 1000)\n",
      "(314, 1000)\n",
      "(728, 1000)\n",
      "(314, 1000)\n",
      "(728, 1000)\n",
      "(314, 1000)\n",
      "(728, 1000)\n",
      "(314, 1000)\n",
      "(728, 1000)\n",
      "(314, 1000)\n",
      "(728, 1000)\n",
      "(314, 1000)\n",
      "(728, 1000)\n",
      "(314, 1000)\n",
      "(728, 1000)\n",
      "(314, 1000)\n",
      "(728, 1000)\n",
      "(314, 1000)\n",
      "(728, 1000)\n",
      "(314, 1000)\n",
      "(728, 1000, 2, 1)\n",
      "(728, 1000, 2, 1)\n",
      "(728, 1000, 2, 1)\n",
      "(728, 1000, 2, 1)\n",
      "(728, 1000, 2, 1)\n",
      "(728, 1000, 2, 1)\n",
      "(728, 1000, 2, 1)\n",
      "(728, 1000, 2, 1)\n",
      "(728, 1000, 2, 1)\n",
      "(728, 1000, 2, 1)\n",
      "(716, 1000)\n",
      "(308, 1000)\n",
      "(716, 1000)\n",
      "(308, 1000)\n",
      "(716, 1000)\n",
      "(308, 1000)\n",
      "(716, 1000)\n",
      "(308, 1000)\n",
      "(716, 1000)\n",
      "(308, 1000)\n",
      "(716, 1000)\n",
      "(308, 1000)\n",
      "(716, 1000)\n",
      "(308, 1000)\n",
      "(716, 1000)\n",
      "(308, 1000)\n",
      "(716, 1000)\n",
      "(308, 1000)\n",
      "(716, 1000)\n",
      "(308, 1000)\n",
      "(716, 1000, 2, 1)\n",
      "(716, 1000, 2, 1)\n",
      "(716, 1000, 2, 1)\n",
      "(716, 1000, 2, 1)\n",
      "(716, 1000, 2, 1)\n",
      "(716, 1000, 2, 1)\n",
      "(716, 1000, 2, 1)\n",
      "(716, 1000, 2, 1)\n",
      "(716, 1000, 2, 1)\n",
      "(716, 1000, 2, 1)\n"
     ]
    }
   ],
   "source": [
    "processed_amino_acid = [\"Alanine\", \"Arginine\", \"Glutamine\", \"Asparagine\", \"Aspartic Acid\", \"Cysteine\", \"Glutamic Acid\", \n",
    "                        \"Glycine\", \"Histidine\", \"Isoleucine\", \"Leucine\"]\n",
    "missing_amino_acid = \"Glutamine\"\n",
    "path = \"G:\\\\Dev\\\\Data\\\\Convolution vs Dense Experiments corrected\\\\\"\n",
    "\n",
    "for amino_acid, shift in amino_acids_with_shifts:\n",
    "    if amino_acid != missing_amino_acid and amino_acid not in processed_amino_acid:\n",
    "        dataset = \"G:\\\\Dev\\\\Data\\\\NIST Amino Acids\\\\NIST {} Has Substructure.txt\".format(amino_acid)\n",
    "        train_shifts_diff_splits(path, \"Shifts {} AUC.txt\".format(amino_acid), dataset)\n",
    "        train_conv_diff_splits(path, \"Conv {} AUC.txt\".format(amino_acid), dataset, shift, splits=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Shifts', 'Conv'], dtype='object')\n",
      "          0\n",
      "0  0.852761\n",
      "1  0.820384\n",
      "2  0.821391\n",
      "3  0.899595\n",
      "4  0.844519\n",
      "5  0.840184\n",
      "6  0.890185\n",
      "7  0.903554\n",
      "8  0.848412\n",
      "9  0.888131\n",
      "          0\n",
      "0  0.714301\n",
      "1  0.588177\n",
      "2  0.669562\n",
      "3  0.677857\n",
      "4  0.663588\n",
      "5  0.691817\n",
      "6  0.653387\n",
      "7  0.689710\n",
      "8  0.677173\n",
      "9  0.560512\n",
      "     Shifts      Conv\n",
      "0  0.852761  0.714301\n",
      "1  0.820384  0.588177\n",
      "2  0.821391  0.669562\n",
      "3  0.899595  0.677857\n",
      "4  0.844519  0.663588\n",
      "5  0.840184  0.691817\n",
      "6  0.890185  0.653387\n",
      "7  0.903554  0.689710\n",
      "8  0.848412  0.677173\n",
      "9  0.888131  0.560512\n"
     ]
    }
   ],
   "source": [
    "path = \"G:\\\\Dev\\\\Data\\\\Convolution vs Dense Experiments corrected\\\\\"\n",
    "df_shifts = pd.read_csv(path + \"Shifts Alanine AUC.txt\", header=None)\n",
    "df_conv = pd.read_csv(path + \"Conv Alanine AUC.txt\", header=None)\n",
    "df_combined = pd.concat([df_shifts, df_conv], axis=1)\n",
    "df_combined.columns = [\"Shifts\", \"Conv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_relResult(statistic=11.221065799556373, pvalue=1.360958273920665e-06)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.ttest_rel(df_combined['Shifts'], df_combined['Conv'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alanine: Ttest_relResult(statistic=11.221065799556373, pvalue=1.360958273920665e-06)\n",
      "Arginine: Ttest_relResult(statistic=28.5750086111355, pvalue=3.8328019853930337e-10)\n",
      "Asparagine: Ttest_relResult(statistic=16.725438280769435, pvalue=4.36717980880867e-08)\n",
      "Aspartic Acid: Ttest_relResult(statistic=10.596471853565722, pvalue=2.20475016639447e-06)\n",
      "Cysteine: Ttest_relResult(statistic=20.999878665699455, pvalue=5.902295392507168e-09)\n",
      "Glutamic Acid: Ttest_relResult(statistic=16.11574726396528, pvalue=6.04057600384934e-08)\n",
      "Glycine: Ttest_relResult(statistic=21.29469907408046, pvalue=5.21802415846084e-09)\n",
      "Histidine: Ttest_relResult(statistic=24.686189478071988, pvalue=1.4084292165671842e-09)\n",
      "Isoleucine: Ttest_relResult(statistic=22.404763799156985, pvalue=3.3283701743627767e-09)\n",
      "Leucine: Ttest_relResult(statistic=23.173613772424893, pvalue=2.468281924435496e-09)\n",
      "Lysine: Ttest_relResult(statistic=17.375865382206925, pvalue=3.127198069649469e-08)\n",
      "Methionine: Ttest_relResult(statistic=15.327817324443428, pvalue=9.348512085218057e-08)\n",
      "Phenylalanine: Ttest_relResult(statistic=26.276007564751552, pvalue=8.08769109802662e-10)\n",
      "Proline: Ttest_relResult(statistic=25.627502344919723, pvalue=1.0100222289747276e-09)\n",
      "Serine: Ttest_relResult(statistic=12.40721886217737, pvalue=5.791774484541126e-07)\n",
      "Threonine: Ttest_relResult(statistic=16.519758319259353, pvalue=4.866063644398779e-08)\n",
      "Tryptophan: Ttest_relResult(statistic=14.258275934854591, pvalue=1.7509872047212962e-07)\n",
      "Tyrosine: Ttest_relResult(statistic=14.81830887140648, pvalue=1.2539952191166017e-07)\n",
      "Valine: Ttest_relResult(statistic=21.626969016103633, pvalue=4.550337194863764e-09)\n"
     ]
    }
   ],
   "source": [
    "path = \"G:\\\\Dev\\\\Data\\\\Convolution vs Dense Experiments corrected\\\\\"\n",
    "missing_amino_acid = \"Glutamine\"\n",
    "\n",
    "for amino_acid, shift in amino_acids_with_shifts:\n",
    "    if amino_acid != missing_amino_acid:        \n",
    "        df_shifts = pd.read_csv(path + \"Shifts {} AUC.txt\".format(amino_acid), header=None)\n",
    "        df_conv = pd.read_csv(path + \"Conv {} AUC.txt\".format(amino_acid), header=None)\n",
    "        df_combined = pd.concat([df_shifts, df_conv], axis=1)\n",
    "        df_combined.columns = [\"Shifts\", \"Conv\"]\n",
    "        t_test = stats.ttest_rel(df_combined['Shifts'], df_combined['Conv'])\n",
    "        print(amino_acid + \": \" + str(t_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

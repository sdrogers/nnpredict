{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from matplotlib import pylab as plt\n",
    "\n",
    "import pandas as pd\n",
    "import math\n",
    "import random\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_training_and_test_set(has_substructure_path):\n",
    "    substructure = np.loadtxt(has_substructure_path, np.int)\n",
    "    all_ones = np.where(substructure == 1)\n",
    "    all_zeros = np.where(substructure == 0)\n",
    "\n",
    "    number_of_training_set = int(len(all_ones[0]) * 0.7)\n",
    "    \n",
    "    training_random_ones = random.sample(list(all_ones[0]), number_of_training_set)\n",
    "    training_random_zeros = random.sample(list(all_zeros[0]), number_of_training_set)\n",
    "\n",
    "    test_random_ones = [ones for ones in all_ones[0] if ones not in training_random_ones]\n",
    "    test_random_zeros = [zeros for zeros in all_zeros[0] if zeros not in training_random_zeros]\n",
    "    test_random_zeros = random.sample(test_random_zeros, len(test_random_ones))\n",
    "    \n",
    "    return training_random_ones, training_random_zeros, test_random_ones, test_random_zeros\n",
    "    \n",
    "def get_list_of_ids(dataset):\n",
    "    return [\"GNPS_ALL_\" + str(index+1) for index in dataset]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building training and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training_spec(spec_path, training_set):\n",
    "    file_list = os.listdir(spec_path)\n",
    "    \n",
    "    filtered_file_list = [file for file in file_list if file[:-13] in training_set]\n",
    "    filtered_name_list = [filename[:-13] for filename in filtered_file_list]\n",
    "\n",
    "    intensities = pd.DataFrame(0.0, index = filtered_name_list, columns=range(1000), dtype=float)\n",
    "\n",
    "    for file in filtered_file_list:\n",
    "        filepath = os.path.join(spec_path, file)\n",
    "        mol_name = file[:-13]\n",
    "        with open(filepath, 'r') as f:\n",
    "            for index, line in enumerate(f):\n",
    "                mass, intensity = line.split(\" \")\n",
    "                if not math.isnan(float(intensity)):\n",
    "                    intensities.at[mol_name, int(mass)-1] = float(intensity)\n",
    "            intensities.loc[mol_name].div(np.amax(intensities.loc[mol_name].values)).mul(999)\n",
    "            \n",
    "    return intensities\n",
    "\n",
    "def load_test_spec(spec_path, test_set):\n",
    "    file_list = os.listdir(spec_path)\n",
    "    \n",
    "    filtered_file_list = [file for file in file_list if file[:-13] in test_set]\n",
    "    filtered_name_list = [filename[:-13] for filename in filtered_file_list]\n",
    "\n",
    "    test_intensities = pd.DataFrame(0.0, index = filtered_name_list, columns=range(1000), dtype=float)\n",
    "\n",
    "    for file in filtered_file_list:\n",
    "        filepath = os.path.join(spec_path, file)\n",
    "        mol_name = file[:-13]\n",
    "        with open(filepath, 'r') as f:\n",
    "            for index, line in enumerate(f):\n",
    "                mass, intensity = line.split(\" \")\n",
    "                if not math.isnan(float(intensity)):\n",
    "                    test_intensities.at[mol_name, int(mass)-1] = float(intensity)\n",
    "            test_intensities.loc[mol_name].div(np.amax(test_intensities.loc[mol_name].values)).mul(999)\n",
    "    return test_intensities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_has_substructure(content, intensities):\n",
    "    has_substructure_truth_values = []\n",
    "    for index in intensities.index:\n",
    "        has_substructure_truth_values.append(int(content[int(index.split('_')[2]) - 1][:-1]))\n",
    "        \n",
    "    return has_substructure_truth_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a simple keras model to classify this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model,Sequential\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "def just_fragments_model(input_to_network):\n",
    "    class_model = Sequential()\n",
    "    class_model.add(Dense(500, input_dim=input_to_network.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "    class_model.add(Dense(200,kernel_initializer='normal',activation = 'relu'))\n",
    "    class_model.add(Dense(100,kernel_initializer='normal',activation = 'relu'))\n",
    "    class_model.add(Dense(50,kernel_initializer='normal',activation = 'relu'))\n",
    "    class_model.add(Dense(1,kernel_initializer='normal',activation = 'sigmoid'))\n",
    "    class_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return class_model\n",
    "\n",
    "def just_shifts_model(input_to_network):\n",
    "    class_model = Sequential()\n",
    "    class_model.add(Dense(500, input_dim=input_to_network.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "    class_model.add(Dense(200,kernel_initializer='normal',activation = 'relu'))\n",
    "    class_model.add(Dense(100,kernel_initializer='normal',activation = 'relu'))\n",
    "    class_model.add(Dense(50,kernel_initializer='normal',activation = 'relu'))\n",
    "    class_model.add(Dense(1,kernel_initializer='normal',activation = 'sigmoid'))\n",
    "    class_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return class_model\n",
    "\n",
    "def fragments_shifts_model(input_to_network):\n",
    "    class_model = Sequential()\n",
    "    class_model.add(Dense(1000, input_dim=input_to_network.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "    class_model.add(Dense(500,kernel_initializer='normal',activation = 'relu'))\n",
    "    class_model.add(Dense(200,kernel_initializer='normal',activation = 'relu'))\n",
    "    class_model.add(Dense(100,kernel_initializer='normal',activation = 'relu'))\n",
    "    class_model.add(Dense(50,kernel_initializer='normal',activation = 'relu'))\n",
    "    class_model.add(Dense(1,kernel_initializer='normal',activation = 'sigmoid'))\n",
    "    class_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return class_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second model for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_peak_differences(spec):\n",
    "    non_zero_peaks = list(np.where(spec>0)[0])\n",
    "    peak_differences = [(abs(i-j), (spec[i]+spec[j]/2.0)) for i in non_zero_peaks for j in non_zero_peaks if i != j and j > i]\n",
    "    \n",
    "    return peak_differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_shift_bins(intensities):\n",
    "    shift_bins = pd.DataFrame(0.0, index = intensities.index, columns=range(intensities.shape[1]), dtype=float)\n",
    "    for index, spec in enumerate(intensities.values):\n",
    "        peak_differences = calc_peak_differences(spec)\n",
    "        mol_name = intensities.index[index]\n",
    "        for shift, average_intensity in peak_differences:\n",
    "            shift_bins.at[mol_name, shift-1] += average_intensity\n",
    "        shift_bins.loc[mol_name].div(np.amax(intensities.loc[mol_name].values)).mul(999)\n",
    "    return shift_bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise_training_test_set(has_substruct_path):\n",
    "    training_random_ones, training_random_zeros, test_random_ones, test_random_zeros = build_training_and_test_set(has_substruct_path)\n",
    "    training_random_ones_id = get_list_of_ids(training_random_ones)\n",
    "    training_random_zeros_id = get_list_of_ids(training_random_zeros)\n",
    "    training_set = training_random_ones_id + training_random_zeros_id\n",
    "\n",
    "    test_random_ones_id = get_list_of_ids(test_random_ones)\n",
    "    test_random_zeros_id = get_list_of_ids(test_random_zeros)\n",
    "    test_set = test_random_ones_id + test_random_zeros_id\n",
    "    return training_set, test_set\n",
    "\n",
    "def train_fragments_diff_splits(path, name, has_substruct_dataset, training_set, test_set, splits=10):\n",
    "    filtered_dataset_dir = \"G:\\\\Dev\\\\Data\\\\MSMS-NIST\\\\Python Filtered\"\n",
    "    epochs = 200\n",
    "    extra_epochs = 100\n",
    "    path = path + name\n",
    "    truth_values = []\n",
    "    test_truth_values = []\n",
    "\n",
    "    with open(has_substruct_dataset, 'r') as f:\n",
    "        content = f.readlines()\n",
    "\n",
    "    with open(path, 'w') as f:\n",
    "        for i in range(splits):\n",
    "            intensities = load_training_spec(filtered_dataset_dir, training_set)\n",
    "            intensities = intensities.reindex(index=intensities.index.to_series().str.rsplit('_').str[-1].astype(int).sort_values().index)\n",
    "           \n",
    "            test_intensities = load_test_spec(filtered_dataset_dir, test_set)\n",
    "            test_intensities = test_intensities.reindex(index=test_intensities.index.to_series().str.rsplit('_').str[-1].astype(int).sort_values().index)\n",
    "\n",
    "            truth_values = load_has_substructure(content, intensities)\n",
    "            test_truth_values = load_has_substructure(content, test_intensities)\n",
    "\n",
    "            X = intensities.values\n",
    "            N,M = X.shape\n",
    "            print(X.shape)\n",
    "            shuffle_order = np.random.permutation(N)\n",
    "            labels = truth_values\n",
    "            labels = np.array(labels)[:,None]\n",
    "            test_labels = test_truth_values\n",
    "            test_labels = np.array(test_labels)[:,None]\n",
    "\n",
    "            mod = just_fragments_model(X)\n",
    "            mod.fit(X[shuffle_order,:],labels[shuffle_order],epochs=extra_epochs,validation_split=0.2,verbose=0)\n",
    "\n",
    "            x_test_spec = test_intensities.values\n",
    "            predicted = mod.predict(x_test_spec)\n",
    "\n",
    "            auc = roc_auc_score(test_labels, predicted)\n",
    "            f.write(str(auc) + \"\\n\")\n",
    "            \n",
    "def train_shifts_diff_splits(path, name, has_substruct_dataset, training_set, test_set, splits=10):\n",
    "    filtered_dataset_dir = \"G:\\\\Dev\\\\Data\\\\MSMS-NIST\\\\Python Filtered\"\n",
    "    epochs = 200\n",
    "    extra_epochs = 100\n",
    "    path = path + name\n",
    "    truth_values = []\n",
    "    test_truth_values = []\n",
    "\n",
    "    with open(has_substruct_dataset, 'r') as f:\n",
    "        content = f.readlines()\n",
    "\n",
    "    with open(path, 'w') as f:\n",
    "        for i in range(splits):\n",
    "            intensities = load_training_spec(filtered_dataset_dir, training_set)\n",
    "            intensities = intensities.reindex(index=intensities.index.to_series().str.rsplit('_').str[-1].astype(int).sort_values().index)\n",
    "           \n",
    "            test_intensities = load_test_spec(filtered_dataset_dir, test_set)\n",
    "            test_intensities = test_intensities.reindex(index=test_intensities.index.to_series().str.rsplit('_').str[-1].astype(int).sort_values().index)\n",
    "\n",
    "            truth_values = load_has_substructure(content, intensities)\n",
    "            test_truth_values = load_has_substructure(content, test_intensities)\n",
    "\n",
    "            X = intensities.values\n",
    "            N,M = X.shape\n",
    "            shuffle_order = np.random.permutation(N)\n",
    "            labels = truth_values\n",
    "            labels = np.array(labels)[:,None]\n",
    "            test_labels = test_truth_values\n",
    "            test_labels = np.array(test_labels)[:,None]\n",
    "\n",
    "            shift_bins = load_shift_bins(intensities).values\n",
    "            print(shift_bins.shape)\n",
    "\n",
    "            mod = just_shifts_model(shift_bins)\n",
    "            mod.fit(shift_bins[shuffle_order,:],labels[shuffle_order],epochs=extra_epochs,validation_split=0.2,verbose=0)\n",
    "\n",
    "            test_shift_bins = load_shift_bins(test_intensities).values\n",
    "            print(test_shift_bins.shape)\n",
    "\n",
    "            predicted = mod.predict(test_shift_bins)\n",
    "\n",
    "            auc = roc_auc_score(test_labels, predicted)\n",
    "            f.write(str(auc) + \"\\n\")\n",
    "\n",
    "def train_fragments_and_shifts_diff_splits(path, name, has_substruct_dataset, training_set, test_set, splits=10):\n",
    "    filtered_dataset_dir = \"G:\\\\Dev\\\\Data\\\\MSMS-NIST\\\\Python Filtered\"\n",
    "    epochs = 200\n",
    "    extra_epochs = 100\n",
    "    path = path + name\n",
    "    truth_values = []\n",
    "    test_truth_values = []\n",
    "\n",
    "    with open(has_substruct_dataset, 'r') as f:\n",
    "        content = f.readlines()\n",
    "\n",
    "    with open(path, 'w') as f:\n",
    "        for i in range(splits):\n",
    "            intensities = load_training_spec(filtered_dataset_dir, training_set)\n",
    "            intensities = intensities.reindex(index=intensities.index.to_series().str.rsplit('_').str[-1].astype(int).sort_values().index)\n",
    "           \n",
    "            test_intensities = load_test_spec(filtered_dataset_dir, test_set)\n",
    "            test_intensities = test_intensities.reindex(index=test_intensities.index.to_series().str.rsplit('_').str[-1].astype(int).sort_values().index)\n",
    "\n",
    "            truth_values = load_has_substructure(content, intensities)\n",
    "            test_truth_values = load_has_substructure(content, test_intensities)\n",
    "\n",
    "            X = intensities.values\n",
    "            N,M = X.shape\n",
    "            shuffle_order = np.random.permutation(N)\n",
    "            labels = truth_values\n",
    "            labels = np.array(labels)[:,None]\n",
    "            test_labels = test_truth_values\n",
    "            test_labels = np.array(test_labels)[:,None]\n",
    "\n",
    "            shift_bins = load_shift_bins(intensities)\n",
    "            x_fragments_shifts = pd.concat([intensities, shift_bins], axis=1)\n",
    "            x_fragments_shifts.columns = range(2000)\n",
    "            print(x_fragments_shifts.shape)\n",
    "\n",
    "            mod = fragments_shifts_model(x_fragments_shifts.values)\n",
    "            mod.fit(x_fragments_shifts.values[shuffle_order,:],labels[shuffle_order],epochs=extra_epochs,validation_split=0.2,verbose=0)\n",
    "\n",
    "            test_shift_bins = load_shift_bins(test_intensities)            \n",
    "            x_fragments_shifts_test = pd.concat([test_intensities, test_shift_bins], axis=1)\n",
    "            x_fragments_shifts_test.columns = range(2000)\n",
    "            print(x_fragments_shifts_test.shape)\n",
    "\n",
    "            predicted = mod.predict(x_fragments_shifts_test.values)\n",
    "\n",
    "            auc = roc_auc_score(test_labels, predicted)\n",
    "            f.write(str(auc) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Alanine', 71), ('Arginine', 156), ('Asparagine', 114), ('Aspartic Acid', 115), ('Cysteine', 103), ('Glutamic Acid', 129), ('Glutamine', 128), ('Glycine', 57), ('Histidine', 137), ('Isoleucine', 113), ('Leucine', 113), ('Lysine', 128), ('Methionine', 131), ('Phenylalanine', 147), ('Proline', 97), ('Serine', 87), ('Threonine', 101), ('Tryptophan', 186), ('Tyrosine', 163), ('Valine', 99)]\n"
     ]
    }
   ],
   "source": [
    "amino_acid_path = \"G:\\\\Dev\\\\Data\\\\Fragment Masses.txt\"\n",
    "amino_acids_with_shifts = []\n",
    "\n",
    "with open(amino_acid_path, 'r') as f:\n",
    "    for line in f:\n",
    "        amino_acid, shift = line.split(\", \")\n",
    "        amino_acids_with_shifts.append((amino_acid, int(float(shift[:-1]))))\n",
    "\n",
    "print(amino_acids_with_shifts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(716, 1000)\n",
      "(716, 1000)\n",
      "(716, 1000)\n",
      "(716, 1000)\n",
      "(716, 1000)\n",
      "(716, 1000)\n",
      "(716, 1000)\n",
      "(716, 1000)\n",
      "(716, 1000)\n",
      "(716, 1000)\n",
      "(716, 1000)\n",
      "(308, 1000)\n",
      "(716, 1000)\n",
      "(308, 1000)\n",
      "(716, 1000)\n",
      "(308, 1000)\n",
      "(716, 1000)\n",
      "(308, 1000)\n",
      "(716, 1000)\n",
      "(308, 1000)\n",
      "(716, 1000)\n",
      "(308, 1000)\n",
      "(716, 1000)\n",
      "(308, 1000)\n",
      "(716, 1000)\n",
      "(308, 1000)\n",
      "(716, 1000)\n",
      "(308, 1000)\n",
      "(716, 1000)\n",
      "(308, 1000)\n",
      "(716, 2000)\n",
      "(308, 2000)\n",
      "(716, 2000)\n",
      "(308, 2000)\n",
      "(716, 2000)\n",
      "(308, 2000)\n",
      "(716, 2000)\n",
      "(308, 2000)\n",
      "(716, 2000)\n",
      "(308, 2000)\n",
      "(716, 2000)\n",
      "(308, 2000)\n",
      "(716, 2000)\n",
      "(308, 2000)\n",
      "(716, 2000)\n",
      "(308, 2000)\n",
      "(716, 2000)\n",
      "(308, 2000)\n",
      "(716, 2000)\n",
      "(308, 2000)\n"
     ]
    }
   ],
   "source": [
    "missing_amino_acid = \"Glutamine\"\n",
    "path = \"G:\\\\Dev\\\\Data\\\\Shift vs No Shifts Experiments\\\\\"\n",
    "\n",
    "for amino_acid, shift in amino_acids_with_shifts:\n",
    "    if amino_acid != missing_amino_acid and amino_acid not in processed_amino_acid:\n",
    "        dataset = \"G:\\\\Dev\\\\Data\\\\NIST Amino Acids\\\\NIST {} Has Substructure.txt\".format(amino_acid)\n",
    "        training_set, test_set = initialise_training_test_set(dataset)\n",
    "        train_fragments_diff_splits(path, \"Fragments {} AUC.txt\".format(amino_acid), dataset, training_set, test_set)\n",
    "        train_shifts_diff_splits(path, \"Shifts {} AUC.txt\".format(amino_acid), dataset, training_set, test_set)\n",
    "        train_fragments_and_shifts_diff_splits(path, \"Fragments and Shifts {} AUC.txt\".format(amino_acid), dataset, training_set, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(728, 2000)\n",
      "(314, 2000)\n",
      "(728, 2000)\n",
      "(314, 2000)\n",
      "(728, 2000)\n",
      "(314, 2000)\n",
      "(728, 2000)\n",
      "(314, 2000)\n",
      "(728, 2000)\n",
      "(314, 2000)\n",
      "(728, 2000)\n",
      "(314, 2000)\n",
      "(728, 2000)\n",
      "(314, 2000)\n",
      "(728, 2000)\n",
      "(314, 2000)\n",
      "(728, 2000)\n",
      "(314, 2000)\n",
      "(728, 2000)\n",
      "(314, 2000)\n"
     ]
    }
   ],
   "source": [
    "path = \"G:\\\\Dev\\\\Data\\\\Shift vs No Shifts Experiments\\\\\"\n",
    "amino_acid = \"Tyrosine\"\n",
    "dataset = \"G:\\\\Dev\\\\Data\\\\NIST Amino Acids\\\\NIST {} Has Substructure.txt\".format(amino_acid)\n",
    "training_set, test_set = initialise_training_test_set(dataset)\n",
    "train_fragments_and_shifts_diff_splits(path, \"Fragments and Shifts {} AUC.txt\".format(amino_acid), dataset, training_set, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

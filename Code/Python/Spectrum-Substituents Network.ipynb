{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/simon/.virtualenvs/termnn/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.7\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "from keras.layers import Input, Dense, Lambda, concatenate, Conv1D, Concatenate, Flatten, MaxPooling1D\n",
    "from keras.models import Model\n",
    "import pandas as pd\n",
    "from itertools import islice\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up variables for constants such as absolute datapaths and the desired valdiation fraction split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = '/Users/simon/Downloads/Substituent_Data'\n",
    "classyfire_datapath = datapath + os.sep + \"For Substituent GNPS ALL\" + os.sep + \"GNPS Python Master\" + os.sep + \"Final Data.txt\"\n",
    "substituents_path = datapath+os.sep+\"Classyfire Taxanomy\"+os.sep+\"GNPS_substituents.txt\"\n",
    "synced_substituents_path =datapath+os.sep+\"Classyfire Taxanomy\"+os.sep+\"GNPS_synced_substituents.txt\"\n",
    "\n",
    "substituents_names_path = datapath+os.sep+\"Classyfire Taxanomy\"+os.sep+\"GNPS_substituents_legend.txt\"\n",
    "filtered_substituents_names_path = datapath + os.sep + \"after_filtering_score_and_occurences\"+os.sep+\"filtered_top_substituents_after_parameters_legend.txt\"\n",
    "substituents_auc_results_path = datapath+os.sep+\"Classyfire Taxanomy\"+os.sep+\"substituents_auc_results.txt\"\n",
    "num_samples = 9238\n",
    "numSubstituents = 381\n",
    "val_fraction = 0.1\n",
    "default_dpi = plt.rcParamsDefault['figure.dpi']\n",
    "plt.rcParams['figure.dpi'] = default_dpi*1.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below are methods used to load in fragment spectra and fingerprint data from files stored in the absolute paths specified above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads a master file containing peak intensities for all molecules.\n",
    "# Each molecule's spectrum is added as a 1000 element row to a Pandas dataframe\n",
    "# The dataframe is then converted into a numpy array for use as Keras Input.\n",
    "# Include the option of adding additonal features to each molecule (mass_shifts variable)\n",
    "def load_master_file(path, mass_shifts = 0, number_of_bins = 1000):\n",
    "    MAX_MASS = 1000\n",
    "    BIN_SIZE = 1\n",
    "    NUM_FEATURES = mass_shifts\n",
    "    mol_all = np.loadtxt(path, dtype=\"U30\") # Get master file in as numpy array\n",
    "    \n",
    "    mol_ids = np.unique(mol_all[:, 0])  # Trim duplicate filename rows, store unique filenames\n",
    "    # Construct empty Pandas dataframe of correct size.\n",
    "    # Number of rows is equal to the number of unique molecules (found in mol_ids).\n",
    "    intensities = pd.DataFrame(0.0, index = mol_ids, columns=range((number_of_bins//BIN_SIZE)+NUM_FEATURES), dtype=float)\n",
    "    \n",
    "    # Populate the dataframe using each molecule's filename to place data in the correct row.\n",
    "    for row in mol_all:\n",
    "        intensities.at[row[0], float(row[1])-1] = float(row[2])\n",
    "    \n",
    "    # Convert populated dataframe into a numpy array for use by neural networks.\n",
    "    np_matrix = intensities.values\n",
    "    return np_matrix\n",
    "\n",
    "# Load a master file containing CDK fingerprints for all molecules.\n",
    "# Each molecules CDK bit set is added as a 320 element array to a Pandas dataframe.\n",
    "def load_substituents_master(path):\n",
    "    BITS = 444  # Total number of bits in fingerprint\n",
    "\n",
    "    fp_all = np.loadtxt(path, dtype=\"U30\") # Get master file as numpy array of Strings\n",
    "    fp_ids = np.unique(fp_all[:, 0]) # Trim duplicate filename rows, store unique filenames\n",
    "\n",
    "    # Construct empty Pandas dataframe of correct size.\n",
    "    # Number of rows is equal to the number of unique molecules (found in fp_ids).\n",
    "    substituents = pd.DataFrame(0, index = fp_ids, columns=range(BITS), dtype=int)\n",
    "\n",
    "    # Populate the dataframe using each molecule's filename to place data in the correct row.\n",
    "    for row in fp_all:\n",
    "        substituents.at[row[0], int(row[1])] = int(row[2])\n",
    "\n",
    "    # Convert populated dataframe into a numpy array for use as output by neural networks.\n",
    "    \n",
    "    np_matrix = substituents\n",
    "    return np_matrix\n",
    "\n",
    "# Load the names of all substituents included in the correct order\n",
    "# This is used for boxplots, when performance metrics for individual substituent are calculated.\n",
    "def load_substituent_legend(path):\n",
    "    substituent_legend = []\n",
    "    # Open file containing substituent names.\n",
    "    with open(path, 'r') as f:\n",
    "        # Add each name to the list of substituent names.\n",
    "        lines = list(islice(f, 0, None))\n",
    "        for line in lines:\n",
    "            substituent_legend.append(line[:-1])\n",
    "    return substituent_legend\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below methods create and train various neural networks when provided with valid input and output data. They allow for specifying the number of epochs the network is to be trained for and, in some cases, the learning rate. Trained models are returned and can be used to predict on test data and thereby be evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Sotchastic Gradient Descent object from Keras to allow for tweaking its learning rate.\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# A simplified spectrum-fingeprrint encoder.\n",
    "# Structure: (Input)1000-500-200-2098(Output)\n",
    "def simplified_substituent_model(x_train_spectra, x_train_substituents, epochs=100):\n",
    "    # Create input based on the provided x_train data structure.\n",
    "    input_layer = Input(shape=(x_train_spectra.shape[1],))\n",
    "    # Since output is not the same as input, we obtain its shape separately.\n",
    "    output_dims = x_train_substituents.shape[1]\n",
    "    print(output_dims)\n",
    "    # Create the encoding layers using functional API.\n",
    "    l = input_layer\n",
    "    l = Dense(500, activation='relu')(l)\n",
    "    \n",
    "    # Linear activation ensures that values can be negative (necessary for sigmoid to function)\n",
    "    l = Dense(200, activation='linear')(l)\n",
    "    \n",
    "    # Save reference to latent space\n",
    "    latent_space = l\n",
    "    \n",
    "    # Sigmoid activation to get outputs between 0 and 1. This is done because the output fingerprint is a set of bits (0 or 1).\n",
    "    l2 = Dense(output_dims, activation='sigmoid')(l)\n",
    "    \n",
    "    #Reference for output layer\n",
    "    out_layer = l2\n",
    "\n",
    "    auto_model = Model(input=input_layer, output=out_layer)\n",
    "    \n",
    "    # Set SGD learning rate = 0.05 and compile model with binary_crossentropy as loss function.\n",
    "    sgd = SGD(lr=0.05)\n",
    "    auto_model.compile(loss='binary_crossentropy', optimizer=sgd)\n",
    "    \n",
    "    # Train the model for the specified number of epochs, using the specified validation fraction.\n",
    "    autoencoder_train = auto_model.fit(x_train_spectra, x_train_substituents, shuffle=False, validation_split = 0.1, epochs=epochs)\n",
    "    \n",
    "    # Loss Plots\n",
    "    plot_loss(autoencoder_train, epochs)\n",
    "    \n",
    "    return auto_model # Return model, now trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes as input a trained neural network model and extracts its history variable.\n",
    "# It then uses it to graph the model's loss and validation loss over the training epochs\n",
    "# The epochs paramter is used for plotting the x axis.\n",
    "def plot_loss(fitted_model, epochs):\n",
    "    # Extract loss values for the training and validation sets.\n",
    "    loss = fitted_model.history['loss']\n",
    "    val_loss = fitted_model.history['val_loss']\n",
    "    # Create x axis variables.\n",
    "    epochs_label = epochs\n",
    "    epochs = range(epochs)\n",
    "\n",
    "    #Plot both losses.\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "    plt.plot(epochs, val_loss,'r', label='Validation loss')\n",
    "    plt.title('Training and Validation Loss for ' + str(epochs_label) + ' epochs')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "# Takes a actual and predicted fingerprint values and computes the area under the Roc curve for each substructure.\n",
    "# For each subtructure, also calculates AUC when the actual values are scrambled.\n",
    "# Return two numpy arrays: one containing AUC metrics for all susbtructures, one containing each permutation's\n",
    "# AUC scores for each susbtructure.\n",
    "from sympy.utilities.iterables import multiset_permutations\n",
    "def compute_auc(bits, true, pred, num_samples=0, permutations=500):\n",
    "    val_start_index = int(num_samples-(num_samples*val_fraction)-1) # Index where validation samples begin.\n",
    "    \n",
    "    num_permutations = permutations  # Number of permutations to compute AUC scores for. \n",
    "    \n",
    "    # Create structured array to hold statistics for each fingerprint.\n",
    "    dtype = [('fp_id', int),('nonzeros', int), ('auc', float), ('auc_percent', float)]\n",
    "    mol_stats = np.zeros((bits,), dtype=dtype)\n",
    "\n",
    "    # Create array to hold permutation AUC scores for plotting.\n",
    "    perm_scores = np.zeros((bits, num_permutations))\n",
    "    val_start_index = 0\n",
    "    for fp_id in range(true.shape[1]): # For every substructure\n",
    "        nonzero_vals = np.count_nonzero(true[val_start_index:, fp_id]) # Count number of nonzero values\n",
    "        if nonzero_vals > 0 and nonzero_vals < true[val_start_index:, fp_id].size:  # If there are no 1s or no 0s, can't compute.\n",
    "            # Compute actual AUC score using only the validation fraction of the dataset.\n",
    "            fp_true = true[val_start_index:, fp_id]\n",
    "            fp_pred = pred[val_start_index:, fp_id]\n",
    "            score = metrics.roc_auc_score(fp_true, fp_pred)\n",
    "\n",
    "            # Compute AUC scores for permutations and compare to actual.\n",
    "            counter = 0         \n",
    "            for i in range(num_permutations):\n",
    "                permutation = np.random.permutation(fp_true)\n",
    "                perm_score = metrics.roc_auc_score(permutation, fp_pred)\n",
    "                perm_scores[fp_id, i] = perm_score\n",
    "                # Count how many permutations have a higer AUC score than actual data.\n",
    "                if perm_score >= score:\n",
    "                    counter = counter + 1\n",
    "            # Calculate % of scrambled values with higher AUC score than actual AUC\n",
    "            percentage = (counter/num_permutations)*100\n",
    "        # Update structured array with data or non values if no AUC could be calculated.\n",
    "            mol_stats[fp_id] = fp_id, nonzero_vals, score, percentage\n",
    "        else:\n",
    "            mol_stats[fp_id] = (fp_id, nonzero_vals, 0, 100)\n",
    "        \n",
    "    # Permutations take a while, print statement to say when finished.\n",
    "    print(\"Compute AUC Done\")\n",
    "    return mol_stats, perm_scores\n",
    "\n",
    "\n",
    "# Takes a set of AUC scores and permutation AUC scores (normally output by compute_auc above) an uses them\n",
    "# to draw boxplots for specified susbtructures. Actual AUC is plotted as a coloured dot.\n",
    "plt.rcParams['figure.dpi'] = default_dpi*2.2\n",
    "def boxplots(real_stats, perm_stats, sample_fps):\n",
    "    index = sample_fps['fp_id']  # Grab id of each substructure to be plotted, used as index in parallel arrays\n",
    "    names = np.array(substituent_names)[index]  # Grab name of each susbtructure to be plotted.\n",
    "\n",
    "    plt.rcParams.update({'font.size': 6})\n",
    "    plt.figure()\n",
    "    plt.boxplot(perm_stats[index].T, vert=False, labels = names) # Boxplot permutation AUC scores\n",
    "    plt.scatter(real_stats[index]['auc'], range(1, len(index)+1)) # Scatter plot actual AUC scores for substructures in colour.\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "# Takes a set of AUC scores and permutation AUC scores and uses them to draw boxplots for specified substructures\n",
    "# Actual AUC is plotted as a coloured dot. A separate set of AUC scores\n",
    "# computed for prediction from a different model is also plotted for comparison\n",
    "def tandem_boxplots(real_stats, perm_stats, exp_stats, sample_fps):\n",
    "    index = sample_fps['fp_id']  # Grab id of each substructure to be plotted, used as index in parallel arrays\n",
    "    names = np.array(substituent_names)[index]  # Grab name of each susbtructure to be plotted.\n",
    "  \n",
    "    plt.rcParams.update({'font.size': 6})\n",
    "    plt.figure()\n",
    "    plt.boxplot(perm_stats[index].T, vert=False, labels = names) # Boxplot permutation AUC scores\n",
    "    plt.scatter(real_stats[index]['auc'], range(1, len(index)+1)) # Scatter plot actual AUC scores for substructures\n",
    "    plt.scatter(exp_stats[index]['auc'], range(1, len(index)+1), color = 'r') # Scatter plot AUC scores to be compared to.\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Given the AUC statistics derived from two separate models, it comapres the two models' performance\n",
    "# Creates a bar chart comparing substructures above an AUC threshold and draws boxplots for each model's best and worst\n",
    "# performing substructures.\n",
    "# Usually compares an experimental model's AUC to a baseline (e.g. the basic fingerprint encoder)\n",
    "def evaluate(base_stats, base_perm_scores, exp_stats, exp_perm_scores):\n",
    "    # Sort molecules in ascending order of baseline AUC score, keeping only molecules with AUC scores above 0.5\n",
    "    normal_auc = np.where((base_stats['auc'] > 0.5))\n",
    "    abnormal_auc = np.where((base_stats['auc']) < 0.5)\n",
    "    ordered_base = np.sort(base_stats[normal_auc], order='auc', axis=0)[::-1]\n",
    "    \n",
    "    # Take top 30 and bottom 5 substructures by AUC score to use for boxplots.\n",
    "    sample_fps = ordered_base[:30]\n",
    "    sample_fps = np.append(sample_fps, ordered_base[-5:])\n",
    "    \n",
    "    # Plot number of substructures with AUC scores above 0.7 and above 0.5 for both data sets\n",
    "    base_above_07 = len(np.where((base_stats['auc'] >= 0.7))[0])\n",
    "    exp_above_07 = len(np.where((exp_stats['auc'] >= 0.7))[0])\n",
    "    base_above_05 = len(np.where((base_stats['auc'] >= 0.5))[0])\n",
    "    exp_above_05 = len(np.where((exp_stats['auc'] >= 0.5))[0])\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    index = np.arange(2)\n",
    "    bar_width = 0.35\n",
    "    opacity = 0.5\n",
    "    ax.bar(index, (base_above_05, base_above_07), bar_width, alpha=opacity, color='b', label='Baseline')\n",
    "    ax.bar(index+bar_width, (exp_above_05, exp_above_07), bar_width, alpha=opacity, color='r', label='Experiment')\n",
    "    \n",
    "    ax.set_xlabel('AUC Threshold')\n",
    "    ax.set_ylabel('Number of Substituents')\n",
    "    ax.set_title('AUC Score Comparison')\n",
    "    ax.set_xticks(index + bar_width / 2)\n",
    "    ax.set_xticklabels(('Above 0.5', 'Above 0.7'))\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Boxplots of sample substructures for both data sets\n",
    "    boxplots(base_stats, base_perm_scores, sample_fps)\n",
    "    boxplots(exp_stats, exp_perm_scores, sample_fps)\n",
    "    tandem_boxplots(base_stats, base_perm_scores, exp_stats, sample_fps)\n",
    "    \n",
    "     # Sort molecules in ascending order of experimental AUC score, keeping only molecules with AUC scores above 0.5\n",
    "    normal_auc = np.where((exp_stats['auc'] > 0.5))\n",
    "    abnormal_auc = np.where((exp_stats['auc']) < 0.5)\n",
    "    ordered_exp = np.sort(exp_stats[normal_auc], order='auc', axis=0)[::-1]\n",
    "    \n",
    "    # Take top 30 and bottom 5 substructures by AUC score to use for boxplots.\n",
    "    sample_fps = ordered_exp[:30]\n",
    "    sample_fps = np.append(sample_fps, ordered_exp[-5:])\n",
    "    \n",
    "    boxplots(base_stats, base_perm_scores, sample_fps)\n",
    "    boxplots(exp_stats, exp_perm_scores, sample_fps)\n",
    "    tandem_boxplots(base_stats, exp_perm_scores, exp_stats, sample_fps)\n",
    "    \n",
    "\n",
    "# Given a matrix of layer weights, plots them in a Hinton diagram: each weight is a box\n",
    "# Box size is indicates absolute value, box colour indicates sign (white for positive, black for negative)\n",
    "# Adapted from matplotlib documentation.\n",
    "def hinton(matrix, max_weight=None, ax=None):\n",
    "    ax = ax if ax is not None else plt.gca()\n",
    "    # Find maximum weight in matrix.\n",
    "    if not max_weight:\n",
    "        max_weight = 2 ** np.ceil(np.log(np.abs(matrix).max()) / np.log(2))\n",
    "\n",
    "    ax.patch.set_facecolor('gray')\n",
    "    ax.set_aspect('equal', 'box')\n",
    "    ax.xaxis.set_major_locator(plt.NullLocator())\n",
    "    ax.yaxis.set_major_locator(plt.NullLocator())\n",
    "\n",
    "    # Plot weights as black or white boxes.\n",
    "    for (x, y), w in np.ndenumerate(matrix):\n",
    "        color = 'white' if w > 0 else 'black'\n",
    "        size = np.sqrt(np.abs(w) / max_weight)\n",
    "        rect = plt.Rectangle([x - size / 2, y - size / 2], size, size,\n",
    "                             facecolor=color, edgecolor=color)\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    ax.autoscale_view()\n",
    "    ax.invert_yaxis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following methods are quick ways to train multiple times using different training-validation splits. Used when we want means, error bars and statistical tests for model comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trains the basic substituent encoder using 10 different validation-training splits.\n",
    "# Computes AUC scores for each split and stores them as a separate file.\n",
    "# Takes path to store files in as a parameter, as well as the name of the test.\n",
    "def train_diff_splits(path, name, splits=10):\n",
    "    # Extract permuted indices for dataset.\n",
    "    index_path = \"G:\\\\Dev\\\\Data\\\\GNPS ALL Substituent Validation Split Permutations.txt\"\n",
    "    permuted_indices = np.loadtxt(index_path, dtype=int, delimiter=',')\n",
    "    epochs = 100\n",
    "    path = path + name + \" \"\n",
    "    #List to store AUC scores if we want to use them right away instead of loading from files.\n",
    "    experiment_stats = []\n",
    "    for i in range(splits):\n",
    "        # Create filepath for this training session.\n",
    "        curr_path = path + str(i) + \".txt\"\n",
    "        # Use permuted indices to create permuted array of input data.\n",
    "        x_train_dense = spectra[permuted_indices[:, i]]\n",
    "        x_train_dense = np.log(x_train_dense+1)\n",
    "        x_train_substituents = substituents.values[permuted_indices[:, i]]\n",
    "        # Train a basic model.\n",
    "        enc_basic = simplified_substituent_model(x_train_dense, x_train_substituents, epochs=100)\n",
    "        # Use trained model to compute AUC scores for substructures and save them to disc.\n",
    "        actual = x_train_substituents\n",
    "        predicted = enc_basic.predict(x_train_dense)\n",
    "        base_stats, base_perm_scores = compute_auc(2098, actual, predicted, num_samples=10038)\n",
    "        \n",
    "        np.savetxt(curr_path, base_stats, fmt=['%d', '%d', '%f', '%f'])\n",
    "        experiment_stats.append(base_stats)\n",
    "    \n",
    "    evaluate(base_stats, base_perm_scores, base_stats, base_perm_scores)\n",
    "    \n",
    "    return experiment_stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The below is the basic set up for running any of the methods. It loads the fragment spectra and subtituent terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "spectra = load_master_file(path=classyfire_datapath)\n",
    "substituents = load_substituents_master(synced_substituents_path)\n",
    "substituent_names = load_substituent_legend(filtered_substituents_names_path)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10038, 1000)\n"
     ]
    }
   ],
   "source": [
    "print(spectra.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/simon/.virtualenvs/termnn/lib/python3.7/site-packages/ipykernel_launcher.py:28: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9034 samples, validate on 1004 samples\n",
      "Epoch 1/400\n",
      "9034/9034 [==============================] - 1s 106us/step - loss: 0.4896 - val_loss: 0.1769\n",
      "Epoch 2/400\n",
      "9034/9034 [==============================] - 1s 80us/step - loss: 0.1335 - val_loss: 0.0631\n",
      "Epoch 3/400\n",
      "9034/9034 [==============================] - 1s 80us/step - loss: 0.0865 - val_loss: 0.0534\n",
      "Epoch 4/400\n",
      "9034/9034 [==============================] - 1s 78us/step - loss: 0.0750 - val_loss: 0.0508\n",
      "Epoch 5/400\n",
      "9034/9034 [==============================] - 1s 83us/step - loss: 0.0697 - val_loss: 0.0498\n",
      "Epoch 6/400\n",
      "9034/9034 [==============================] - 1s 89us/step - loss: 0.0665 - val_loss: 0.0492\n",
      "Epoch 7/400\n",
      "9034/9034 [==============================] - 1s 79us/step - loss: 0.0643 - val_loss: 0.0488\n",
      "Epoch 8/400\n",
      "9034/9034 [==============================] - 1s 78us/step - loss: 0.0627 - val_loss: 0.0485\n",
      "Epoch 9/400\n",
      "9034/9034 [==============================] - 1s 79us/step - loss: 0.0614 - val_loss: 0.0483\n",
      "Epoch 10/400\n",
      "9034/9034 [==============================] - 1s 80us/step - loss: 0.0604 - val_loss: 0.0480\n",
      "Epoch 11/400\n",
      "9034/9034 [==============================] - 1s 81us/step - loss: 0.0595 - val_loss: 0.0478\n",
      "Epoch 12/400\n",
      "9034/9034 [==============================] - 1s 80us/step - loss: 0.0587 - val_loss: 0.0476\n",
      "Epoch 13/400\n",
      "9034/9034 [==============================] - 1s 79us/step - loss: 0.0581 - val_loss: 0.0474\n",
      "Epoch 14/400\n",
      "9034/9034 [==============================] - 1s 85us/step - loss: 0.0574 - val_loss: 0.0472\n",
      "Epoch 15/400\n",
      "9034/9034 [==============================] - 1s 87us/step - loss: 0.0569 - val_loss: 0.0470\n",
      "Epoch 16/400\n",
      "9034/9034 [==============================] - 1s 92us/step - loss: 0.0563 - val_loss: 0.0468\n",
      "Epoch 17/400\n",
      "9034/9034 [==============================] - 1s 87us/step - loss: 0.0558 - val_loss: 0.0466\n",
      "Epoch 18/400\n",
      "9034/9034 [==============================] - 1s 84us/step - loss: 0.0553 - val_loss: 0.0464\n",
      "Epoch 19/400\n",
      "9034/9034 [==============================] - 1s 83us/step - loss: 0.0549 - val_loss: 0.0462\n",
      "Epoch 20/400\n",
      "9034/9034 [==============================] - 1s 91us/step - loss: 0.0545 - val_loss: 0.0460\n",
      "Epoch 21/400\n",
      "9034/9034 [==============================] - 1s 85us/step - loss: 0.0540 - val_loss: 0.0458\n",
      "Epoch 22/400\n",
      "9034/9034 [==============================] - 1s 85us/step - loss: 0.0536 - val_loss: 0.0456\n",
      "Epoch 23/400\n",
      "9034/9034 [==============================] - 1s 84us/step - loss: 0.0533 - val_loss: 0.0454\n",
      "Epoch 24/400\n",
      "9034/9034 [==============================] - 1s 83us/step - loss: 0.0529 - val_loss: 0.0453\n",
      "Epoch 25/400\n",
      "9034/9034 [==============================] - 1s 82us/step - loss: 0.0525 - val_loss: 0.0451\n",
      "Epoch 26/400\n",
      "9034/9034 [==============================] - 1s 85us/step - loss: 0.0522 - val_loss: 0.0449\n",
      "Epoch 27/400\n",
      "9034/9034 [==============================] - 1s 85us/step - loss: 0.0519 - val_loss: 0.0448\n",
      "Epoch 28/400\n",
      "9034/9034 [==============================] - 1s 82us/step - loss: 0.0516 - val_loss: 0.0446\n",
      "Epoch 29/400\n",
      "9034/9034 [==============================] - 1s 80us/step - loss: 0.0512 - val_loss: 0.0445\n",
      "Epoch 30/400\n",
      "9034/9034 [==============================] - 1s 79us/step - loss: 0.0510 - val_loss: 0.0443\n",
      "Epoch 31/400\n",
      "9034/9034 [==============================] - 1s 80us/step - loss: 0.0507 - val_loss: 0.0442\n",
      "Epoch 32/400\n",
      "9034/9034 [==============================] - 1s 83us/step - loss: 0.0504 - val_loss: 0.0440\n",
      "Epoch 33/400\n",
      "9034/9034 [==============================] - 1s 81us/step - loss: 0.0502 - val_loss: 0.0439\n",
      "Epoch 34/400\n",
      "9034/9034 [==============================] - 1s 83us/step - loss: 0.0499 - val_loss: 0.0438\n",
      "Epoch 35/400\n",
      "9034/9034 [==============================] - 1s 82us/step - loss: 0.0497 - val_loss: 0.0437\n",
      "Epoch 36/400\n",
      "9034/9034 [==============================] - 1s 83us/step - loss: 0.0494 - val_loss: 0.0435\n",
      "Epoch 37/400\n",
      "9034/9034 [==============================] - 1s 82us/step - loss: 0.0492 - val_loss: 0.0434\n",
      "Epoch 38/400\n",
      "9034/9034 [==============================] - 1s 84us/step - loss: 0.0490 - val_loss: 0.0433\n",
      "Epoch 39/400\n",
      "9034/9034 [==============================] - 1s 84us/step - loss: 0.0488 - val_loss: 0.0432\n",
      "Epoch 40/400\n",
      "9034/9034 [==============================] - 1s 83us/step - loss: 0.0486 - val_loss: 0.0431\n",
      "Epoch 41/400\n",
      "9034/9034 [==============================] - 1s 84us/step - loss: 0.0484 - val_loss: 0.0430\n",
      "Epoch 42/400\n",
      "9034/9034 [==============================] - 1s 82us/step - loss: 0.0482 - val_loss: 0.0429\n",
      "Epoch 43/400\n",
      "9034/9034 [==============================] - 1s 82us/step - loss: 0.0481 - val_loss: 0.0428\n",
      "Epoch 44/400\n",
      "9034/9034 [==============================] - 1s 81us/step - loss: 0.0479 - val_loss: 0.0427\n",
      "Epoch 45/400\n",
      "9034/9034 [==============================] - 1s 81us/step - loss: 0.0477 - val_loss: 0.0426\n",
      "Epoch 46/400\n",
      "9034/9034 [==============================] - 1s 82us/step - loss: 0.0476 - val_loss: 0.0425\n",
      "Epoch 47/400\n",
      "9034/9034 [==============================] - 1s 83us/step - loss: 0.0474 - val_loss: 0.0425\n",
      "Epoch 48/400\n",
      "9034/9034 [==============================] - 1s 83us/step - loss: 0.0472 - val_loss: 0.0424\n",
      "Epoch 49/400\n",
      "9034/9034 [==============================] - 1s 82us/step - loss: 0.0471 - val_loss: 0.0423\n",
      "Epoch 50/400\n",
      "9034/9034 [==============================] - 1s 82us/step - loss: 0.0469 - val_loss: 0.0422\n",
      "Epoch 51/400\n",
      "9034/9034 [==============================] - 1s 82us/step - loss: 0.0468 - val_loss: 0.0421\n",
      "Epoch 52/400\n",
      "9034/9034 [==============================] - 1s 82us/step - loss: 0.0467 - val_loss: 0.0421\n",
      "Epoch 53/400\n",
      "9034/9034 [==============================] - 1s 82us/step - loss: 0.0465 - val_loss: 0.0420\n",
      "Epoch 54/400\n",
      "9034/9034 [==============================] - 1s 82us/step - loss: 0.0464 - val_loss: 0.0419\n",
      "Epoch 55/400\n",
      "9034/9034 [==============================] - 1s 82us/step - loss: 0.0463 - val_loss: 0.0419\n",
      "Epoch 56/400\n",
      "9034/9034 [==============================] - 1s 85us/step - loss: 0.0461 - val_loss: 0.0418\n",
      "Epoch 57/400\n",
      "9034/9034 [==============================] - 1s 86us/step - loss: 0.0460 - val_loss: 0.0417\n",
      "Epoch 58/400\n",
      "9034/9034 [==============================] - 1s 82us/step - loss: 0.0459 - val_loss: 0.0417\n",
      "Epoch 59/400\n",
      "9034/9034 [==============================] - 1s 81us/step - loss: 0.0458 - val_loss: 0.0416\n",
      "Epoch 60/400\n",
      "9034/9034 [==============================] - 1s 82us/step - loss: 0.0457 - val_loss: 0.0415\n",
      "Epoch 61/400\n",
      "9034/9034 [==============================] - 1s 82us/step - loss: 0.0455 - val_loss: 0.0415\n",
      "Epoch 62/400\n",
      "9034/9034 [==============================] - 1s 82us/step - loss: 0.0454 - val_loss: 0.0414\n",
      "Epoch 63/400\n",
      "9034/9034 [==============================] - 1s 82us/step - loss: 0.0453 - val_loss: 0.0413\n",
      "Epoch 64/400\n",
      "9034/9034 [==============================] - 1s 82us/step - loss: 0.0452 - val_loss: 0.0413\n",
      "Epoch 65/400\n",
      "9034/9034 [==============================] - 1s 81us/step - loss: 0.0451 - val_loss: 0.0412\n",
      "Epoch 66/400\n",
      "9034/9034 [==============================] - 1s 82us/step - loss: 0.0450 - val_loss: 0.0412\n",
      "Epoch 67/400\n",
      "9034/9034 [==============================] - 1s 84us/step - loss: 0.0449 - val_loss: 0.0411\n",
      "Epoch 68/400\n",
      "9034/9034 [==============================] - 1s 85us/step - loss: 0.0448 - val_loss: 0.0411\n",
      "Epoch 69/400\n",
      "9034/9034 [==============================] - 1s 85us/step - loss: 0.0447 - val_loss: 0.0410\n",
      "Epoch 70/400\n",
      "9034/9034 [==============================] - 1s 83us/step - loss: 0.0446 - val_loss: 0.0410\n",
      "Epoch 71/400\n",
      "9034/9034 [==============================] - 1s 83us/step - loss: 0.0445 - val_loss: 0.0409\n",
      "Epoch 72/400\n",
      "9034/9034 [==============================] - 1s 83us/step - loss: 0.0444 - val_loss: 0.0409\n",
      "Epoch 73/400\n",
      "9034/9034 [==============================] - 1s 81us/step - loss: 0.0443 - val_loss: 0.0408\n",
      "Epoch 74/400\n",
      "9034/9034 [==============================] - 1s 85us/step - loss: 0.0442 - val_loss: 0.0408\n",
      "Epoch 75/400\n",
      "9034/9034 [==============================] - 1s 86us/step - loss: 0.0441 - val_loss: 0.0407\n",
      "Epoch 76/400\n",
      "9034/9034 [==============================] - 1s 84us/step - loss: 0.0441 - val_loss: 0.0407\n",
      "Epoch 77/400\n",
      "9034/9034 [==============================] - 1s 91us/step - loss: 0.0440 - val_loss: 0.0406\n",
      "Epoch 78/400\n",
      "9034/9034 [==============================] - 1s 88us/step - loss: 0.0439 - val_loss: 0.0406\n",
      "Epoch 79/400\n",
      "9034/9034 [==============================] - 1s 92us/step - loss: 0.0438 - val_loss: 0.0405\n",
      "Epoch 80/400\n",
      "9034/9034 [==============================] - 1s 98us/step - loss: 0.0437 - val_loss: 0.0405\n",
      "Epoch 81/400\n",
      "9034/9034 [==============================] - 1s 96us/step - loss: 0.0436 - val_loss: 0.0404\n",
      "Epoch 82/400\n",
      "9034/9034 [==============================] - 1s 91us/step - loss: 0.0435 - val_loss: 0.0404\n",
      "Epoch 83/400\n",
      "9034/9034 [==============================] - 1s 83us/step - loss: 0.0435 - val_loss: 0.0403\n",
      "Epoch 84/400\n",
      "9034/9034 [==============================] - 1s 83us/step - loss: 0.0434 - val_loss: 0.0403\n",
      "Epoch 85/400\n",
      "9034/9034 [==============================] - 1s 81us/step - loss: 0.0433 - val_loss: 0.0403\n",
      "Epoch 86/400\n",
      "9034/9034 [==============================] - 1s 82us/step - loss: 0.0432 - val_loss: 0.0402\n",
      "Epoch 87/400\n",
      "9034/9034 [==============================] - 1s 82us/step - loss: 0.0432 - val_loss: 0.0402\n",
      "Epoch 88/400\n",
      "9034/9034 [==============================] - 1s 82us/step - loss: 0.0431 - val_loss: 0.0401\n",
      "Epoch 89/400\n",
      "9034/9034 [==============================] - 1s 81us/step - loss: 0.0430 - val_loss: 0.0401\n",
      "Epoch 90/400\n",
      "9034/9034 [==============================] - 1s 81us/step - loss: 0.0429 - val_loss: 0.0401\n",
      "Epoch 91/400\n",
      "9034/9034 [==============================] - 1s 85us/step - loss: 0.0429 - val_loss: 0.0400\n",
      "Epoch 92/400\n",
      "9034/9034 [==============================] - 1s 86us/step - loss: 0.0428 - val_loss: 0.0400\n",
      "Epoch 93/400\n",
      "9034/9034 [==============================] - 1s 84us/step - loss: 0.0427 - val_loss: 0.0400\n",
      "Epoch 94/400\n",
      "9034/9034 [==============================] - 1s 82us/step - loss: 0.0427 - val_loss: 0.0399\n",
      "Epoch 95/400\n",
      "9034/9034 [==============================] - 1s 81us/step - loss: 0.0426 - val_loss: 0.0399\n",
      "Epoch 96/400\n",
      "9034/9034 [==============================] - 1s 81us/step - loss: 0.0425 - val_loss: 0.0399\n",
      "Epoch 97/400\n",
      "9034/9034 [==============================] - 1s 83us/step - loss: 0.0424 - val_loss: 0.0398\n",
      "Epoch 98/400\n",
      "9034/9034 [==============================] - 1s 82us/step - loss: 0.0424 - val_loss: 0.0398\n",
      "Epoch 99/400\n",
      "9034/9034 [==============================] - 1s 83us/step - loss: 0.0423 - val_loss: 0.0398\n",
      "Epoch 100/400\n",
      "9034/9034 [==============================] - 1s 82us/step - loss: 0.0422 - val_loss: 0.0397\n",
      "Epoch 101/400\n",
      "9034/9034 [==============================] - 1s 81us/step - loss: 0.0422 - val_loss: 0.0397\n",
      "Epoch 102/400\n",
      "9034/9034 [==============================] - 1s 85us/step - loss: 0.0421 - val_loss: 0.0397\n",
      "Epoch 103/400\n",
      "9034/9034 [==============================] - 1s 84us/step - loss: 0.0421 - val_loss: 0.0396\n",
      "Epoch 104/400\n",
      "9034/9034 [==============================] - 1s 85us/step - loss: 0.0420 - val_loss: 0.0396\n",
      "Epoch 105/400\n",
      "9034/9034 [==============================] - 1s 82us/step - loss: 0.0419 - val_loss: 0.0396\n",
      "Epoch 106/400\n",
      "9034/9034 [==============================] - 1s 82us/step - loss: 0.0419 - val_loss: 0.0395\n",
      "Epoch 107/400\n",
      "9034/9034 [==============================] - 1s 82us/step - loss: 0.0418 - val_loss: 0.0395\n",
      "Epoch 108/400\n",
      "9034/9034 [==============================] - 1s 81us/step - loss: 0.0417 - val_loss: 0.0395\n",
      "Epoch 109/400\n",
      "9034/9034 [==============================] - 1s 81us/step - loss: 0.0417 - val_loss: 0.0395\n",
      "Epoch 110/400\n",
      "9034/9034 [==============================] - 1s 81us/step - loss: 0.0416 - val_loss: 0.0394\n",
      "Epoch 111/400\n",
      "9034/9034 [==============================] - 1s 83us/step - loss: 0.0416 - val_loss: 0.0394\n",
      "Epoch 112/400\n",
      "9034/9034 [==============================] - 1s 83us/step - loss: 0.0415 - val_loss: 0.0394\n",
      "Epoch 113/400\n",
      "9034/9034 [==============================] - 1s 83us/step - loss: 0.0414 - val_loss: 0.0394\n",
      "Epoch 114/400\n",
      "9034/9034 [==============================] - 1s 86us/step - loss: 0.0414 - val_loss: 0.0393\n",
      "Epoch 115/400\n",
      "9034/9034 [==============================] - 1s 83us/step - loss: 0.0413 - val_loss: 0.0393\n",
      "Epoch 116/400\n",
      "9034/9034 [==============================] - 1s 82us/step - loss: 0.0413 - val_loss: 0.0393\n",
      "Epoch 117/400\n",
      "9034/9034 [==============================] - 1s 82us/step - loss: 0.0412 - val_loss: 0.0393\n",
      "Epoch 118/400\n",
      "9034/9034 [==============================] - 1s 82us/step - loss: 0.0412 - val_loss: 0.0392\n",
      "Epoch 119/400\n",
      "9034/9034 [==============================] - 1s 83us/step - loss: 0.0411 - val_loss: 0.0392\n",
      "Epoch 120/400\n",
      "9034/9034 [==============================] - 1s 81us/step - loss: 0.0411 - val_loss: 0.0392\n",
      "Epoch 121/400\n",
      "9034/9034 [==============================] - 1s 81us/step - loss: 0.0410 - val_loss: 0.0392\n",
      "Epoch 122/400\n",
      "9034/9034 [==============================] - 1s 83us/step - loss: 0.0409 - val_loss: 0.0391\n",
      "Epoch 123/400\n",
      "9034/9034 [==============================] - 1s 85us/step - loss: 0.0409 - val_loss: 0.0391\n",
      "Epoch 124/400\n",
      "9034/9034 [==============================] - 1s 83us/step - loss: 0.0408 - val_loss: 0.0391\n",
      "Epoch 125/400\n",
      "9034/9034 [==============================] - 1s 85us/step - loss: 0.0408 - val_loss: 0.0391\n",
      "Epoch 126/400\n",
      "9034/9034 [==============================] - 1s 83us/step - loss: 0.0407 - val_loss: 0.0391\n",
      "Epoch 127/400\n",
      "9034/9034 [==============================] - 1s 83us/step - loss: 0.0407 - val_loss: 0.0390\n",
      "Epoch 128/400\n",
      "9034/9034 [==============================] - 1s 82us/step - loss: 0.0406 - val_loss: 0.0390\n",
      "Epoch 129/400\n",
      "9034/9034 [==============================] - 1s 82us/step - loss: 0.0406 - val_loss: 0.0390\n",
      "Epoch 130/400\n",
      "9034/9034 [==============================] - 1s 81us/step - loss: 0.0405 - val_loss: 0.0390\n",
      "Epoch 131/400\n",
      "9034/9034 [==============================] - 1s 82us/step - loss: 0.0405 - val_loss: 0.0389\n",
      "Epoch 132/400\n",
      "9034/9034 [==============================] - 1s 84us/step - loss: 0.0404 - val_loss: 0.0389\n",
      "Epoch 133/400\n",
      "9034/9034 [==============================] - 1s 86us/step - loss: 0.0404 - val_loss: 0.0389\n",
      "Epoch 134/400\n",
      "9034/9034 [==============================] - 1s 84us/step - loss: 0.0403 - val_loss: 0.0389\n",
      "Epoch 135/400\n",
      "9034/9034 [==============================] - 1s 84us/step - loss: 0.0403 - val_loss: 0.0389\n",
      "Epoch 136/400\n",
      "9034/9034 [==============================] - 1s 84us/step - loss: 0.0402 - val_loss: 0.0389\n",
      "Epoch 137/400\n",
      "9034/9034 [==============================] - 1s 81us/step - loss: 0.0402 - val_loss: 0.0388\n",
      "Epoch 138/400\n",
      "9034/9034 [==============================] - 1s 85us/step - loss: 0.0401 - val_loss: 0.0388\n",
      "Epoch 139/400\n",
      "9034/9034 [==============================] - 1s 82us/step - loss: 0.0401 - val_loss: 0.0388\n",
      "Epoch 140/400\n",
      "9034/9034 [==============================] - 1s 84us/step - loss: 0.0400 - val_loss: 0.0388\n",
      "Epoch 141/400\n",
      "9034/9034 [==============================] - 1s 84us/step - loss: 0.0400 - val_loss: 0.0388\n",
      "Epoch 142/400\n",
      "9034/9034 [==============================] - 1s 83us/step - loss: 0.0399 - val_loss: 0.0387\n",
      "Epoch 143/400\n",
      "9034/9034 [==============================] - 1s 85us/step - loss: 0.0399 - val_loss: 0.0387\n",
      "Epoch 144/400\n",
      "9034/9034 [==============================] - 1s 82us/step - loss: 0.0398 - val_loss: 0.0387\n",
      "Epoch 145/400\n",
      "9034/9034 [==============================] - 1s 82us/step - loss: 0.0398 - val_loss: 0.0387\n",
      "Epoch 146/400\n",
      "9034/9034 [==============================] - 1s 84us/step - loss: 0.0397 - val_loss: 0.0387\n",
      "Epoch 147/400\n",
      "9034/9034 [==============================] - 1s 83us/step - loss: 0.0397 - val_loss: 0.0387\n",
      "Epoch 148/400\n",
      "9034/9034 [==============================] - 1s 81us/step - loss: 0.0396 - val_loss: 0.0387\n",
      "Epoch 149/400\n",
      "9034/9034 [==============================] - 1s 82us/step - loss: 0.0396 - val_loss: 0.0386\n",
      "Epoch 150/400\n",
      "9034/9034 [==============================] - 1s 83us/step - loss: 0.0395 - val_loss: 0.0386\n",
      "Epoch 151/400\n",
      "9034/9034 [==============================] - 1s 84us/step - loss: 0.0395 - val_loss: 0.0386\n",
      "Epoch 152/400\n",
      "9034/9034 [==============================] - 1s 84us/step - loss: 0.0394 - val_loss: 0.0386\n",
      "Epoch 153/400\n",
      "9034/9034 [==============================] - 1s 83us/step - loss: 0.0394 - val_loss: 0.0386\n",
      "Epoch 154/400\n",
      "9034/9034 [==============================] - 1s 83us/step - loss: 0.0393 - val_loss: 0.0386\n",
      "Epoch 155/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9034/9034 [==============================] - 1s 80us/step - loss: 0.0393 - val_loss: 0.0385\n",
      "Epoch 156/400\n",
      "9034/9034 [==============================] - 1s 80us/step - loss: 0.0393 - val_loss: 0.0385\n",
      "Epoch 157/400\n",
      "9034/9034 [==============================] - 1s 80us/step - loss: 0.0392 - val_loss: 0.0385\n",
      "Epoch 158/400\n",
      "9034/9034 [==============================] - 1s 81us/step - loss: 0.0392 - val_loss: 0.0385\n",
      "Epoch 159/400\n",
      "9034/9034 [==============================] - 1s 81us/step - loss: 0.0391 - val_loss: 0.0385\n",
      "Epoch 160/400\n",
      "9034/9034 [==============================] - 1s 80us/step - loss: 0.0391 - val_loss: 0.0385\n",
      "Epoch 161/400\n",
      "9034/9034 [==============================] - 1s 81us/step - loss: 0.0390 - val_loss: 0.0385\n",
      "Epoch 162/400\n",
      "9034/9034 [==============================] - 1s 82us/step - loss: 0.0390 - val_loss: 0.0385\n",
      "Epoch 163/400\n",
      "9034/9034 [==============================] - 1s 83us/step - loss: 0.0389 - val_loss: 0.0384\n",
      "Epoch 164/400\n",
      "9034/9034 [==============================] - 1s 82us/step - loss: 0.0389 - val_loss: 0.0384\n",
      "Epoch 165/400\n",
      "9034/9034 [==============================] - 1s 81us/step - loss: 0.0389 - val_loss: 0.0384\n",
      "Epoch 166/400\n",
      "9034/9034 [==============================] - 1s 82us/step - loss: 0.0388 - val_loss: 0.0384\n",
      "Epoch 167/400\n",
      "9034/9034 [==============================] - 1s 82us/step - loss: 0.0388 - val_loss: 0.0384\n",
      "Epoch 168/400\n",
      "9034/9034 [==============================] - 1s 82us/step - loss: 0.0387 - val_loss: 0.0384\n",
      "Epoch 169/400\n",
      "9034/9034 [==============================] - 1s 82us/step - loss: 0.0387 - val_loss: 0.0384\n",
      "Epoch 170/400\n",
      "9034/9034 [==============================] - 1s 81us/step - loss: 0.0386 - val_loss: 0.0383\n",
      "Epoch 171/400\n",
      "9034/9034 [==============================] - 1s 80us/step - loss: 0.0386 - val_loss: 0.0383\n",
      "Epoch 172/400\n",
      "9034/9034 [==============================] - 1s 80us/step - loss: 0.0385 - val_loss: 0.0383\n",
      "Epoch 173/400\n",
      "9034/9034 [==============================] - 1s 81us/step - loss: 0.0385 - val_loss: 0.0383\n",
      "Epoch 174/400\n",
      "9034/9034 [==============================] - 1s 81us/step - loss: 0.0385 - val_loss: 0.0383\n",
      "Epoch 175/400\n",
      "9034/9034 [==============================] - 1s 83us/step - loss: 0.0384 - val_loss: 0.0383\n",
      "Epoch 176/400\n",
      "9034/9034 [==============================] - 1s 84us/step - loss: 0.0384 - val_loss: 0.0383\n",
      "Epoch 177/400\n",
      "9034/9034 [==============================] - 1s 82us/step - loss: 0.0383 - val_loss: 0.0383\n",
      "Epoch 178/400\n",
      "9034/9034 [==============================] - 1s 81us/step - loss: 0.0383 - val_loss: 0.0383\n",
      "Epoch 179/400\n",
      "9034/9034 [==============================] - 1s 80us/step - loss: 0.0382 - val_loss: 0.0382\n",
      "Epoch 180/400\n",
      "9034/9034 [==============================] - 1s 82us/step - loss: 0.0382 - val_loss: 0.0382\n",
      "Epoch 181/400\n",
      "9034/9034 [==============================] - 1s 82us/step - loss: 0.0382 - val_loss: 0.0382\n",
      "Epoch 182/400\n",
      "9034/9034 [==============================] - 1s 81us/step - loss: 0.0381 - val_loss: 0.0382\n",
      "Epoch 183/400\n",
      "9034/9034 [==============================] - 1s 81us/step - loss: 0.0381 - val_loss: 0.0382\n",
      "Epoch 184/400\n",
      "9034/9034 [==============================] - 1s 81us/step - loss: 0.0380 - val_loss: 0.0382\n",
      "Epoch 185/400\n",
      "9034/9034 [==============================] - 1s 80us/step - loss: 0.0380 - val_loss: 0.0382\n",
      "Epoch 186/400\n",
      "9034/9034 [==============================] - 1s 81us/step - loss: 0.0380 - val_loss: 0.0382\n",
      "Epoch 187/400\n",
      "9034/9034 [==============================] - 1s 82us/step - loss: 0.0379 - val_loss: 0.0382\n",
      "Epoch 188/400\n",
      "9034/9034 [==============================] - 1s 83us/step - loss: 0.0379 - val_loss: 0.0382\n",
      "Epoch 189/400\n",
      "9034/9034 [==============================] - 1s 83us/step - loss: 0.0378 - val_loss: 0.0381\n",
      "Epoch 190/400\n",
      "9034/9034 [==============================] - 1s 84us/step - loss: 0.0378 - val_loss: 0.0381\n",
      "Epoch 191/400\n",
      "9034/9034 [==============================] - 1s 82us/step - loss: 0.0377 - val_loss: 0.0381\n",
      "Epoch 192/400\n",
      "9034/9034 [==============================] - 1s 80us/step - loss: 0.0377 - val_loss: 0.0381\n",
      "Epoch 193/400\n",
      "9034/9034 [==============================] - 1s 83us/step - loss: 0.0377 - val_loss: 0.0381\n",
      "Epoch 194/400\n",
      "9034/9034 [==============================] - 1s 82us/step - loss: 0.0376 - val_loss: 0.0381\n",
      "Epoch 195/400\n",
      "9034/9034 [==============================] - 1s 83us/step - loss: 0.0376 - val_loss: 0.0381\n",
      "Epoch 196/400\n",
      "9034/9034 [==============================] - 1s 81us/step - loss: 0.0375 - val_loss: 0.0381\n",
      "Epoch 197/400\n",
      "9034/9034 [==============================] - 1s 81us/step - loss: 0.0375 - val_loss: 0.0381\n",
      "Epoch 198/400\n",
      "9034/9034 [==============================] - 1s 80us/step - loss: 0.0375 - val_loss: 0.0381\n",
      "Epoch 199/400\n",
      "9034/9034 [==============================] - 1s 82us/step - loss: 0.0374 - val_loss: 0.0380\n",
      "Epoch 200/400\n",
      "9034/9034 [==============================] - 1s 82us/step - loss: 0.0374 - val_loss: 0.0380\n",
      "Epoch 201/400\n",
      "9034/9034 [==============================] - 1s 84us/step - loss: 0.0373 - val_loss: 0.0380\n",
      "Epoch 202/400\n",
      "9034/9034 [==============================] - 1s 83us/step - loss: 0.0373 - val_loss: 0.0380\n",
      "Epoch 203/400\n",
      "9034/9034 [==============================] - 1s 84us/step - loss: 0.0373 - val_loss: 0.0380\n",
      "Epoch 204/400\n",
      "9034/9034 [==============================] - 1s 80us/step - loss: 0.0372 - val_loss: 0.0380\n",
      "Epoch 205/400\n",
      "9034/9034 [==============================] - 1s 81us/step - loss: 0.0372 - val_loss: 0.0380\n",
      "Epoch 206/400\n",
      "9034/9034 [==============================] - 1s 81us/step - loss: 0.0371 - val_loss: 0.0380\n",
      "Epoch 207/400\n",
      "9034/9034 [==============================] - 1s 83us/step - loss: 0.0371 - val_loss: 0.0380\n",
      "Epoch 208/400\n",
      "9034/9034 [==============================] - 1s 81us/step - loss: 0.0371 - val_loss: 0.0380\n",
      "Epoch 209/400\n",
      "9034/9034 [==============================] - 1s 81us/step - loss: 0.0370 - val_loss: 0.0380\n",
      "Epoch 210/400\n",
      "9034/9034 [==============================] - 1s 81us/step - loss: 0.0370 - val_loss: 0.0379\n",
      "Epoch 211/400\n",
      "9034/9034 [==============================] - 1s 81us/step - loss: 0.0369 - val_loss: 0.0379\n",
      "Epoch 212/400\n",
      "9034/9034 [==============================] - 1s 80us/step - loss: 0.0369 - val_loss: 0.0379\n",
      "Epoch 213/400\n",
      "9034/9034 [==============================] - 1s 81us/step - loss: 0.0369 - val_loss: 0.0379\n",
      "Epoch 214/400\n",
      "9034/9034 [==============================] - 1s 85us/step - loss: 0.0368 - val_loss: 0.0379\n",
      "Epoch 215/400\n",
      "9034/9034 [==============================] - 1s 84us/step - loss: 0.0368 - val_loss: 0.0379\n",
      "Epoch 216/400\n",
      "9034/9034 [==============================] - 1s 84us/step - loss: 0.0367 - val_loss: 0.0379\n",
      "Epoch 217/400\n",
      "9034/9034 [==============================] - 1s 82us/step - loss: 0.0367 - val_loss: 0.0379\n",
      "Epoch 218/400\n",
      "9034/9034 [==============================] - 1s 81us/step - loss: 0.0367 - val_loss: 0.0379\n",
      "Epoch 219/400\n",
      "9034/9034 [==============================] - 1s 81us/step - loss: 0.0366 - val_loss: 0.0379\n",
      "Epoch 220/400\n",
      "9034/9034 [==============================] - 1s 82us/step - loss: 0.0366 - val_loss: 0.0379\n",
      "Epoch 221/400\n",
      "9034/9034 [==============================] - 1s 83us/step - loss: 0.0365 - val_loss: 0.0379\n",
      "Epoch 222/400\n",
      "9034/9034 [==============================] - 1s 83us/step - loss: 0.0365 - val_loss: 0.0379\n",
      "Epoch 223/400\n",
      "9034/9034 [==============================] - 1s 82us/step - loss: 0.0365 - val_loss: 0.0378\n",
      "Epoch 224/400\n",
      "9034/9034 [==============================] - 1s 86us/step - loss: 0.0364 - val_loss: 0.0378\n",
      "Epoch 225/400\n",
      "9034/9034 [==============================] - 1s 85us/step - loss: 0.0364 - val_loss: 0.0378\n",
      "Epoch 226/400\n",
      "9034/9034 [==============================] - 1s 84us/step - loss: 0.0363 - val_loss: 0.0378\n",
      "Epoch 227/400\n",
      "9034/9034 [==============================] - 1s 86us/step - loss: 0.0363 - val_loss: 0.0378\n",
      "Epoch 228/400\n",
      "9034/9034 [==============================] - 1s 84us/step - loss: 0.0363 - val_loss: 0.0378\n",
      "Epoch 229/400\n",
      "9034/9034 [==============================] - 1s 81us/step - loss: 0.0362 - val_loss: 0.0378\n",
      "Epoch 230/400\n",
      "9034/9034 [==============================] - 1s 80us/step - loss: 0.0362 - val_loss: 0.0378\n",
      "Epoch 231/400\n",
      "9034/9034 [==============================] - 1s 81us/step - loss: 0.0361 - val_loss: 0.0378\n",
      "Epoch 232/400\n",
      "9034/9034 [==============================] - 1s 84us/step - loss: 0.0361 - val_loss: 0.0378\n",
      "Epoch 233/400\n",
      "9034/9034 [==============================] - 1s 82us/step - loss: 0.0361 - val_loss: 0.0378\n",
      "Epoch 234/400\n",
      "9034/9034 [==============================] - 1s 85us/step - loss: 0.0360 - val_loss: 0.0378\n",
      "Epoch 235/400\n",
      "9034/9034 [==============================] - 1s 86us/step - loss: 0.0360 - val_loss: 0.0378\n",
      "Epoch 236/400\n",
      "9034/9034 [==============================] - 1s 85us/step - loss: 0.0359 - val_loss: 0.0377\n",
      "Epoch 237/400\n",
      "9034/9034 [==============================] - 1s 85us/step - loss: 0.0359 - val_loss: 0.0377\n",
      "Epoch 238/400\n",
      "9034/9034 [==============================] - 1s 84us/step - loss: 0.0359 - val_loss: 0.0377\n",
      "Epoch 239/400\n",
      "9034/9034 [==============================] - 1s 83us/step - loss: 0.0358 - val_loss: 0.0377\n",
      "Epoch 240/400\n",
      "9034/9034 [==============================] - 1s 87us/step - loss: 0.0358 - val_loss: 0.0377\n",
      "Epoch 241/400\n",
      "9034/9034 [==============================] - 1s 90us/step - loss: 0.0357 - val_loss: 0.0377\n",
      "Epoch 242/400\n",
      "9034/9034 [==============================] - 1s 92us/step - loss: 0.0357 - val_loss: 0.0377\n",
      "Epoch 243/400\n",
      "9034/9034 [==============================] - 1s 87us/step - loss: 0.0357 - val_loss: 0.0377\n",
      "Epoch 244/400\n",
      "9034/9034 [==============================] - 1s 85us/step - loss: 0.0356 - val_loss: 0.0377\n",
      "Epoch 245/400\n",
      "9034/9034 [==============================] - 1s 88us/step - loss: 0.0356 - val_loss: 0.0377\n",
      "Epoch 246/400\n",
      "9034/9034 [==============================] - 1s 86us/step - loss: 0.0356 - val_loss: 0.0377\n",
      "Epoch 247/400\n",
      "9034/9034 [==============================] - 1s 85us/step - loss: 0.0355 - val_loss: 0.0377\n",
      "Epoch 248/400\n",
      "9034/9034 [==============================] - 1s 84us/step - loss: 0.0355 - val_loss: 0.0377\n",
      "Epoch 249/400\n",
      "9034/9034 [==============================] - 1s 85us/step - loss: 0.0354 - val_loss: 0.0377\n",
      "Epoch 250/400\n",
      "9034/9034 [==============================] - 1s 84us/step - loss: 0.0354 - val_loss: 0.0377\n",
      "Epoch 251/400\n",
      "9034/9034 [==============================] - 1s 85us/step - loss: 0.0354 - val_loss: 0.0376\n",
      "Epoch 252/400\n",
      "9034/9034 [==============================] - 1s 83us/step - loss: 0.0353 - val_loss: 0.0376\n",
      "Epoch 253/400\n",
      "9034/9034 [==============================] - 1s 87us/step - loss: 0.0353 - val_loss: 0.0376\n",
      "Epoch 254/400\n",
      "9034/9034 [==============================] - 1s 89us/step - loss: 0.0352 - val_loss: 0.0376\n",
      "Epoch 255/400\n",
      "9034/9034 [==============================] - 1s 87us/step - loss: 0.0352 - val_loss: 0.0376\n",
      "Epoch 256/400\n",
      "9034/9034 [==============================] - 1s 85us/step - loss: 0.0352 - val_loss: 0.0376\n",
      "Epoch 257/400\n",
      "9034/9034 [==============================] - 1s 83us/step - loss: 0.0351 - val_loss: 0.0376\n",
      "Epoch 258/400\n",
      "9034/9034 [==============================] - 1s 83us/step - loss: 0.0351 - val_loss: 0.0376\n",
      "Epoch 259/400\n",
      "9034/9034 [==============================] - 1s 83us/step - loss: 0.0350 - val_loss: 0.0376\n",
      "Epoch 260/400\n",
      "9034/9034 [==============================] - 1s 85us/step - loss: 0.0350 - val_loss: 0.0376\n",
      "Epoch 261/400\n",
      "9034/9034 [==============================] - 1s 89us/step - loss: 0.0350 - val_loss: 0.0376\n",
      "Epoch 262/400\n",
      "9034/9034 [==============================] - 1s 95us/step - loss: 0.0349 - val_loss: 0.0376\n",
      "Epoch 263/400\n",
      "9034/9034 [==============================] - 1s 94us/step - loss: 0.0349 - val_loss: 0.0376\n",
      "Epoch 264/400\n",
      "9034/9034 [==============================] - 1s 90us/step - loss: 0.0349 - val_loss: 0.0376\n",
      "Epoch 265/400\n",
      "9034/9034 [==============================] - 1s 86us/step - loss: 0.0348 - val_loss: 0.0376\n",
      "Epoch 266/400\n",
      "9034/9034 [==============================] - 1s 88us/step - loss: 0.0348 - val_loss: 0.0376\n",
      "Epoch 267/400\n",
      "9034/9034 [==============================] - 1s 87us/step - loss: 0.0347 - val_loss: 0.0376\n",
      "Epoch 268/400\n",
      "9034/9034 [==============================] - 1s 85us/step - loss: 0.0347 - val_loss: 0.0376\n",
      "Epoch 269/400\n",
      "9034/9034 [==============================] - 1s 85us/step - loss: 0.0347 - val_loss: 0.0375\n",
      "Epoch 270/400\n",
      "9034/9034 [==============================] - 1s 83us/step - loss: 0.0346 - val_loss: 0.0375\n",
      "Epoch 271/400\n",
      "9034/9034 [==============================] - 1s 83us/step - loss: 0.0346 - val_loss: 0.0375\n",
      "Epoch 272/400\n",
      "9034/9034 [==============================] - 1s 84us/step - loss: 0.0345 - val_loss: 0.0375\n",
      "Epoch 273/400\n",
      "9034/9034 [==============================] - 1s 84us/step - loss: 0.0345 - val_loss: 0.0375\n",
      "Epoch 274/400\n",
      "9034/9034 [==============================] - 1s 87us/step - loss: 0.0345 - val_loss: 0.0375\n",
      "Epoch 275/400\n",
      "9034/9034 [==============================] - 1s 102us/step - loss: 0.0344 - val_loss: 0.0375\n",
      "Epoch 276/400\n",
      "9034/9034 [==============================] - 1s 86us/step - loss: 0.0344 - val_loss: 0.0375\n",
      "Epoch 277/400\n",
      "9034/9034 [==============================] - 1s 86us/step - loss: 0.0344 - val_loss: 0.0375\n",
      "Epoch 278/400\n",
      "9034/9034 [==============================] - 1s 87us/step - loss: 0.0343 - val_loss: 0.0375\n",
      "Epoch 279/400\n",
      "9034/9034 [==============================] - 1s 85us/step - loss: 0.0343 - val_loss: 0.0375\n",
      "Epoch 280/400\n",
      "9034/9034 [==============================] - 1s 85us/step - loss: 0.0342 - val_loss: 0.0375\n",
      "Epoch 281/400\n",
      "9034/9034 [==============================] - 1s 83us/step - loss: 0.0342 - val_loss: 0.0375\n",
      "Epoch 282/400\n",
      "9034/9034 [==============================] - 1s 82us/step - loss: 0.0342 - val_loss: 0.0375\n",
      "Epoch 283/400\n",
      "9034/9034 [==============================] - 1s 84us/step - loss: 0.0341 - val_loss: 0.0375\n",
      "Epoch 284/400\n",
      "9034/9034 [==============================] - 1s 102us/step - loss: 0.0341 - val_loss: 0.0375\n",
      "Epoch 285/400\n",
      "9034/9034 [==============================] - 1s 89us/step - loss: 0.0341 - val_loss: 0.0375\n",
      "Epoch 286/400\n",
      "9034/9034 [==============================] - 1s 89us/step - loss: 0.0340 - val_loss: 0.0375\n",
      "Epoch 287/400\n",
      "9034/9034 [==============================] - 1s 85us/step - loss: 0.0340 - val_loss: 0.0375\n",
      "Epoch 288/400\n",
      "9034/9034 [==============================] - 1s 84us/step - loss: 0.0339 - val_loss: 0.0374\n",
      "Epoch 289/400\n",
      "9034/9034 [==============================] - 1s 85us/step - loss: 0.0339 - val_loss: 0.0374\n",
      "Epoch 290/400\n",
      "9034/9034 [==============================] - 1s 85us/step - loss: 0.0339 - val_loss: 0.0374\n",
      "Epoch 291/400\n",
      "9034/9034 [==============================] - 1s 85us/step - loss: 0.0338 - val_loss: 0.0374\n",
      "Epoch 292/400\n",
      "9034/9034 [==============================] - 1s 84us/step - loss: 0.0338 - val_loss: 0.0374\n",
      "Epoch 293/400\n",
      "9034/9034 [==============================] - 1s 87us/step - loss: 0.0337 - val_loss: 0.0374\n",
      "Epoch 294/400\n",
      "9034/9034 [==============================] - 1s 87us/step - loss: 0.0337 - val_loss: 0.0374\n",
      "Epoch 295/400\n",
      "9034/9034 [==============================] - 1s 87us/step - loss: 0.0337 - val_loss: 0.0374\n",
      "Epoch 296/400\n",
      "9034/9034 [==============================] - 1s 87us/step - loss: 0.0336 - val_loss: 0.0374\n",
      "Epoch 297/400\n",
      "9034/9034 [==============================] - 1s 87us/step - loss: 0.0336 - val_loss: 0.0374\n",
      "Epoch 298/400\n",
      "9034/9034 [==============================] - 1s 84us/step - loss: 0.0336 - val_loss: 0.0374\n",
      "Epoch 299/400\n",
      "9034/9034 [==============================] - 1s 85us/step - loss: 0.0335 - val_loss: 0.0374\n",
      "Epoch 300/400\n",
      "9034/9034 [==============================] - 1s 85us/step - loss: 0.0335 - val_loss: 0.0374\n",
      "Epoch 301/400\n",
      "9034/9034 [==============================] - 1s 84us/step - loss: 0.0334 - val_loss: 0.0374\n",
      "Epoch 302/400\n",
      "9034/9034 [==============================] - 1s 85us/step - loss: 0.0334 - val_loss: 0.0374\n",
      "Epoch 303/400\n",
      "9034/9034 [==============================] - 1s 86us/step - loss: 0.0334 - val_loss: 0.0374\n",
      "Epoch 304/400\n",
      "9034/9034 [==============================] - 1s 88us/step - loss: 0.0333 - val_loss: 0.0374\n",
      "Epoch 305/400\n",
      "9034/9034 [==============================] - 1s 93us/step - loss: 0.0333 - val_loss: 0.0374\n",
      "Epoch 306/400\n",
      "9034/9034 [==============================] - 1s 86us/step - loss: 0.0333 - val_loss: 0.0374\n",
      "Epoch 307/400\n",
      "9034/9034 [==============================] - 1s 86us/step - loss: 0.0332 - val_loss: 0.0374\n",
      "Epoch 308/400\n",
      "9034/9034 [==============================] - 1s 83us/step - loss: 0.0332 - val_loss: 0.0374\n",
      "Epoch 309/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9034/9034 [==============================] - 1s 84us/step - loss: 0.0331 - val_loss: 0.0374\n",
      "Epoch 310/400\n",
      "9034/9034 [==============================] - 1s 84us/step - loss: 0.0331 - val_loss: 0.0374\n",
      "Epoch 311/400\n",
      "9034/9034 [==============================] - 1s 87us/step - loss: 0.0331 - val_loss: 0.0374\n",
      "Epoch 312/400\n",
      "9034/9034 [==============================] - 1s 86us/step - loss: 0.0330 - val_loss: 0.0374\n",
      "Epoch 313/400\n",
      "9034/9034 [==============================] - 1s 88us/step - loss: 0.0330 - val_loss: 0.0374\n",
      "Epoch 314/400\n",
      "9034/9034 [==============================] - 1s 86us/step - loss: 0.0330 - val_loss: 0.0374\n",
      "Epoch 315/400\n",
      "9034/9034 [==============================] - 1s 86us/step - loss: 0.0329 - val_loss: 0.0373\n",
      "Epoch 316/400\n",
      "9034/9034 [==============================] - 1s 90us/step - loss: 0.0329 - val_loss: 0.0373\n",
      "Epoch 317/400\n",
      "9034/9034 [==============================] - 1s 91us/step - loss: 0.0328 - val_loss: 0.0373\n",
      "Epoch 318/400\n",
      "9034/9034 [==============================] - 1s 92us/step - loss: 0.0328 - val_loss: 0.0373\n",
      "Epoch 319/400\n",
      "9034/9034 [==============================] - 1s 89us/step - loss: 0.0328 - val_loss: 0.0373\n",
      "Epoch 320/400\n",
      "9034/9034 [==============================] - 1s 88us/step - loss: 0.0327 - val_loss: 0.0373\n",
      "Epoch 321/400\n",
      "9034/9034 [==============================] - 1s 87us/step - loss: 0.0327 - val_loss: 0.0373\n",
      "Epoch 322/400\n",
      "9034/9034 [==============================] - 1s 85us/step - loss: 0.0327 - val_loss: 0.0373\n",
      "Epoch 323/400\n",
      "9034/9034 [==============================] - 1s 86us/step - loss: 0.0326 - val_loss: 0.0373\n",
      "Epoch 324/400\n",
      "9034/9034 [==============================] - 1s 84us/step - loss: 0.0326 - val_loss: 0.0373\n",
      "Epoch 325/400\n",
      "9034/9034 [==============================] - 1s 83us/step - loss: 0.0325 - val_loss: 0.0373\n",
      "Epoch 326/400\n",
      "9034/9034 [==============================] - 1s 86us/step - loss: 0.0325 - val_loss: 0.0373\n",
      "Epoch 327/400\n",
      "9034/9034 [==============================] - 1s 85us/step - loss: 0.0325 - val_loss: 0.0373\n",
      "Epoch 328/400\n",
      "9034/9034 [==============================] - 1s 86us/step - loss: 0.0324 - val_loss: 0.0373\n",
      "Epoch 329/400\n",
      "9034/9034 [==============================] - 1s 87us/step - loss: 0.0324 - val_loss: 0.0373\n",
      "Epoch 330/400\n",
      "9034/9034 [==============================] - 1s 85us/step - loss: 0.0324 - val_loss: 0.0373\n",
      "Epoch 331/400\n",
      "9034/9034 [==============================] - 1s 84us/step - loss: 0.0323 - val_loss: 0.0373\n",
      "Epoch 332/400\n",
      "9034/9034 [==============================] - 1s 83us/step - loss: 0.0323 - val_loss: 0.0373\n",
      "Epoch 333/400\n",
      "9034/9034 [==============================] - 1s 83us/step - loss: 0.0322 - val_loss: 0.0373\n",
      "Epoch 334/400\n",
      "9034/9034 [==============================] - 1s 84us/step - loss: 0.0322 - val_loss: 0.0373\n",
      "Epoch 335/400\n",
      "9034/9034 [==============================] - 1s 86us/step - loss: 0.0322 - val_loss: 0.0373\n",
      "Epoch 336/400\n",
      "9034/9034 [==============================] - 1s 85us/step - loss: 0.0321 - val_loss: 0.0373\n",
      "Epoch 337/400\n",
      "9034/9034 [==============================] - 1s 87us/step - loss: 0.0321 - val_loss: 0.0373\n",
      "Epoch 338/400\n",
      "9034/9034 [==============================] - 1s 86us/step - loss: 0.0321 - val_loss: 0.0373\n",
      "Epoch 339/400\n",
      "9034/9034 [==============================] - 1s 85us/step - loss: 0.0320 - val_loss: 0.0373\n",
      "Epoch 340/400\n",
      "9034/9034 [==============================] - 1s 82us/step - loss: 0.0320 - val_loss: 0.0373\n",
      "Epoch 341/400\n",
      "9034/9034 [==============================] - 1s 84us/step - loss: 0.0319 - val_loss: 0.0373\n",
      "Epoch 342/400\n",
      "9034/9034 [==============================] - 1s 82us/step - loss: 0.0319 - val_loss: 0.0373\n",
      "Epoch 343/400\n",
      "9034/9034 [==============================] - 1s 83us/step - loss: 0.0319 - val_loss: 0.0373\n",
      "Epoch 344/400\n",
      "9034/9034 [==============================] - 1s 84us/step - loss: 0.0318 - val_loss: 0.0373\n",
      "Epoch 345/400\n",
      "9034/9034 [==============================] - 1s 83us/step - loss: 0.0318 - val_loss: 0.0373\n",
      "Epoch 346/400\n",
      "9034/9034 [==============================] - 1s 84us/step - loss: 0.0318 - val_loss: 0.0373\n",
      "Epoch 347/400\n",
      "9034/9034 [==============================] - 1s 81us/step - loss: 0.0317 - val_loss: 0.0373\n",
      "Epoch 348/400\n",
      "9034/9034 [==============================] - 1s 80us/step - loss: 0.0317 - val_loss: 0.0373\n",
      "Epoch 349/400\n",
      "9034/9034 [==============================] - 1s 81us/step - loss: 0.0316 - val_loss: 0.0373\n",
      "Epoch 350/400\n",
      "9034/9034 [==============================] - 1s 80us/step - loss: 0.0316 - val_loss: 0.0373\n",
      "Epoch 351/400\n",
      "9034/9034 [==============================] - 1s 82us/step - loss: 0.0316 - val_loss: 0.0373\n",
      "Epoch 352/400\n",
      "9034/9034 [==============================] - 1s 81us/step - loss: 0.0315 - val_loss: 0.0373\n",
      "Epoch 353/400\n",
      "9034/9034 [==============================] - 1s 81us/step - loss: 0.0315 - val_loss: 0.0373\n",
      "Epoch 354/400\n",
      "9034/9034 [==============================] - 1s 81us/step - loss: 0.0315 - val_loss: 0.0373\n",
      "Epoch 355/400\n",
      "9034/9034 [==============================] - 1s 81us/step - loss: 0.0314 - val_loss: 0.0373\n",
      "Epoch 356/400\n",
      "9034/9034 [==============================] - 1s 82us/step - loss: 0.0314 - val_loss: 0.0373\n",
      "Epoch 357/400\n",
      "9034/9034 [==============================] - 1s 84us/step - loss: 0.0314 - val_loss: 0.0373\n",
      "Epoch 358/400\n",
      "9034/9034 [==============================] - 1s 83us/step - loss: 0.0313 - val_loss: 0.0373\n",
      "Epoch 359/400\n",
      "9034/9034 [==============================] - 1s 83us/step - loss: 0.0313 - val_loss: 0.0373\n",
      "Epoch 360/400\n",
      "9034/9034 [==============================] - 1s 81us/step - loss: 0.0312 - val_loss: 0.0373\n",
      "Epoch 361/400\n",
      "9034/9034 [==============================] - 1s 81us/step - loss: 0.0312 - val_loss: 0.0373\n",
      "Epoch 362/400\n",
      "9034/9034 [==============================] - 1s 81us/step - loss: 0.0312 - val_loss: 0.0373\n",
      "Epoch 363/400\n",
      "9034/9034 [==============================] - 1s 81us/step - loss: 0.0311 - val_loss: 0.0373\n",
      "Epoch 364/400\n",
      "9034/9034 [==============================] - 1s 82us/step - loss: 0.0311 - val_loss: 0.0373\n",
      "Epoch 365/400\n",
      "9034/9034 [==============================] - 1s 82us/step - loss: 0.0311 - val_loss: 0.0373\n",
      "Epoch 366/400\n",
      "9034/9034 [==============================] - 1s 81us/step - loss: 0.0310 - val_loss: 0.0373\n",
      "Epoch 367/400\n",
      "9034/9034 [==============================] - 1s 82us/step - loss: 0.0310 - val_loss: 0.0373\n",
      "Epoch 368/400\n",
      "9034/9034 [==============================] - 1s 82us/step - loss: 0.0310 - val_loss: 0.0373\n",
      "Epoch 369/400\n",
      "9034/9034 [==============================] - 1s 83us/step - loss: 0.0309 - val_loss: 0.0373\n",
      "Epoch 370/400\n",
      "9034/9034 [==============================] - 1s 83us/step - loss: 0.0309 - val_loss: 0.0373\n",
      "Epoch 371/400\n",
      "9034/9034 [==============================] - 1s 84us/step - loss: 0.0308 - val_loss: 0.0373\n",
      "Epoch 372/400\n",
      "9034/9034 [==============================] - 1s 81us/step - loss: 0.0308 - val_loss: 0.0373\n",
      "Epoch 373/400\n",
      "9034/9034 [==============================] - 1s 81us/step - loss: 0.0308 - val_loss: 0.0373\n",
      "Epoch 374/400\n",
      "9034/9034 [==============================] - 1s 81us/step - loss: 0.0307 - val_loss: 0.0373\n",
      "Epoch 375/400\n",
      "9034/9034 [==============================] - 1s 81us/step - loss: 0.0307 - val_loss: 0.0373\n",
      "Epoch 376/400\n",
      "9034/9034 [==============================] - 1s 81us/step - loss: 0.0307 - val_loss: 0.0373\n",
      "Epoch 377/400\n",
      "9034/9034 [==============================] - 1s 80us/step - loss: 0.0306 - val_loss: 0.0373\n",
      "Epoch 378/400\n",
      "9034/9034 [==============================] - 1s 83us/step - loss: 0.0306 - val_loss: 0.0373\n",
      "Epoch 379/400\n",
      "9034/9034 [==============================] - 1s 81us/step - loss: 0.0306 - val_loss: 0.0373\n",
      "Epoch 380/400\n",
      "9034/9034 [==============================] - 1s 83us/step - loss: 0.0305 - val_loss: 0.0373\n",
      "Epoch 381/400\n",
      "9034/9034 [==============================] - 1s 84us/step - loss: 0.0305 - val_loss: 0.0373\n",
      "Epoch 382/400\n",
      "9034/9034 [==============================] - 1s 85us/step - loss: 0.0305 - val_loss: 0.0373\n",
      "Epoch 383/400\n",
      "9034/9034 [==============================] - 1s 81us/step - loss: 0.0304 - val_loss: 0.0373\n",
      "Epoch 384/400\n",
      "9034/9034 [==============================] - 1s 83us/step - loss: 0.0304 - val_loss: 0.0373\n",
      "Epoch 385/400\n",
      "9034/9034 [==============================] - 1s 81us/step - loss: 0.0303 - val_loss: 0.0373\n",
      "Epoch 386/400\n",
      "9034/9034 [==============================] - 1s 81us/step - loss: 0.0303 - val_loss: 0.0373\n",
      "Epoch 387/400\n",
      "9034/9034 [==============================] - 1s 81us/step - loss: 0.0303 - val_loss: 0.0373\n",
      "Epoch 388/400\n",
      "9034/9034 [==============================] - 1s 81us/step - loss: 0.0302 - val_loss: 0.0373\n",
      "Epoch 389/400\n",
      "9034/9034 [==============================] - 1s 80us/step - loss: 0.0302 - val_loss: 0.0373\n",
      "Epoch 390/400\n",
      "9034/9034 [==============================] - 1s 81us/step - loss: 0.0302 - val_loss: 0.0373\n",
      "Epoch 391/400\n",
      "9034/9034 [==============================] - 1s 82us/step - loss: 0.0301 - val_loss: 0.0373\n",
      "Epoch 392/400\n",
      "9034/9034 [==============================] - 1s 82us/step - loss: 0.0301 - val_loss: 0.0373\n",
      "Epoch 393/400\n",
      "9034/9034 [==============================] - 1s 84us/step - loss: 0.0301 - val_loss: 0.0373\n",
      "Epoch 394/400\n",
      "9034/9034 [==============================] - 1s 84us/step - loss: 0.0300 - val_loss: 0.0373\n",
      "Epoch 395/400\n",
      "9034/9034 [==============================] - 1s 81us/step - loss: 0.0300 - val_loss: 0.0373\n",
      "Epoch 396/400\n",
      "9034/9034 [==============================] - 1s 81us/step - loss: 0.0300 - val_loss: 0.0373\n",
      "Epoch 397/400\n",
      "9034/9034 [==============================] - 1s 81us/step - loss: 0.0299 - val_loss: 0.0373\n",
      "Epoch 398/400\n",
      "9034/9034 [==============================] - 1s 82us/step - loss: 0.0299 - val_loss: 0.0373\n",
      "Epoch 399/400\n",
      "9034/9034 [==============================] - 1s 81us/step - loss: 0.0299 - val_loss: 0.0373\n",
      "Epoch 400/400\n",
      "9034/9034 [==============================] - 1s 80us/step - loss: 0.0298 - val_loss: 0.0373\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1320x880 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Train 1 time\n",
    "\n",
    "epochs = 400\n",
    "x_train_spectra = np.log(spectra+1)\n",
    "x_train_substituents = substituents\n",
    "\n",
    "enc = simplified_substituent_model(x_train_spectra, x_train_substituents.values, epochs=epochs)\n",
    "\n",
    "actual = x_train_substituents.values\n",
    "predicted = enc.predict(x_train_spectra)\n",
    "\n",
    "# print(compute_auc(444, actual, predicted, num_samples=10038))\n",
    "    \n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing base scores with experiment scores (to get the bar chart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(baseline_stats, baseline_perm_scores, exp_stats, exp_perm_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the results in (substituent_term auc_score) format to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(substituents_auc_results_path, 'w') as f:\n",
    "    for index, auc in enumerate(exp_stats):\n",
    "        f.write(substituent_names[index] + \" \" + str(auc[2]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method to calculate how many matches are there i.e. true label == predicted label. Write to result to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "substituent_dict = {}\n",
    "true_label_counter = {}\n",
    "substituent_correct_counter = {}\n",
    "\n",
    "for name in substituent_names:\n",
    "    true_label_counter[name] = 0\n",
    "    substituent_correct_counter[name] = 0\n",
    "\n",
    "# save the substituent terms indexes to a dict for each id\n",
    "# e.g. {CCMSLIB0000001 : [10, 15, 103], ...}\n",
    "with open(substituents_path, 'r') as f:\n",
    "    for line in f:\n",
    "        id, index, value = line.split(\" \")\n",
    "        if id not in substituent_dict:\n",
    "            substituent_dict[id] = []\n",
    "        substituent_dict[id].append(index)\n",
    "\n",
    "# get the index of element that has highest probability\n",
    "# for each substituent terms in the gnps id, store the max\n",
    "# in the end, go through each and see if the index is in the true labels\n",
    "# if matches, that term's counter increase\n",
    "for index, probabilities in enumerate(predicted):\n",
    "    temp = copy.deepcopy(probabilities).tolist()\n",
    "    true_labels = []\n",
    "    max_indexes = []\n",
    "    for substituent_index in substituent_dict[substituents.index[index]]:\n",
    "        true_labels.append(substituent_index)\n",
    "        max_index = temp.index(max(temp))\n",
    "        max_indexes.append(max_index)\n",
    "        temp.remove(temp[max_index])\n",
    "        true_label_counter[substituent_names[int(substituent_index)]] += 1\n",
    "    for index in max_indexes:\n",
    "        if index in true_labels:\n",
    "            substituent_correct_counter[substituent_names[int(index)]] += 1\n",
    "                    \n",
    "prediction_comparison_report_path = \"G:\\\\Dev\\\\Data\\\\substituent_prediction_comparison_report.txt\"\n",
    "with open(prediction_comparison_report_path, 'w') as f:\n",
    "    f.write(\"substituent,matched,actual,proportion\\n\")\n",
    "\n",
    "with open(prediction_comparison_report_path, 'a') as f:\n",
    "    for substituent in substituent_correct_counter:\n",
    "        matched = substituent_correct_counter[substituent]\n",
    "        actual = true_label_counter[substituent]\n",
    "        proportion = 0\n",
    "        if actual != 0:\n",
    "            proportion = matched / actual\n",
    "        f.write(substituent + \",\" + str(matched) + \",\" + str(actual) + \",\" + str(proportion*100) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method to generate validation split files to be used when training different splits 10 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnps_all_substituent_perm_file_path = \"G:\\\\Dev\\\\Data\\\\GNPS ALL Substituent Validation Split Permutations.txt\"\n",
    "\n",
    "# Create and store 10 randomly permuted indices for 10038 samples\n",
    "sample_size = 10038\n",
    "att = np.arange(sample_size)\n",
    "\n",
    "att = np.random.permutation(att)\n",
    "\n",
    "index_permutations = np.zeros((sample_size, 0), dtype=int)\n",
    "# Add each permutation to a numpy array of indices\n",
    "for i in range(10):\n",
    "    perm = np.random.permutation(np.arange(sample_size, dtype=int))\n",
    "    index_permutations = np.column_stack((index_permutations, perm))\n",
    "\n",
    "# Verify numpy array has correct shape (should be 5770 for each column)\n",
    "for i in range(10):\n",
    "    print(np.unique(index_permutations[:, i]).size)\n",
    "\n",
    "# Save index permutations to file\n",
    "np.savetxt(gnps_all_substituent_perm_file_path, index_permutations, delimiter=',', fmt='%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of training 10 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HI\n"
     ]
    }
   ],
   "source": [
    "# Train 10 times (may take awhile)\n",
    "\n",
    "path = \"G:\\\\Dev\\\\Data\\\\Substituents Experiments\\\\\"\n",
    "train_diff_splits(path, \"GNPS ALL Substituents\")\n",
    "\n",
    "process_average_substituent_path = \"G:\\\\Dev\\\\Data\\\\Substituents Experiments\\\\filtered_average_result.txt\"\n",
    "substituents_legend_path = \"G:\\\\Dev\\\\Data\\\\Classyfire Taxanomy\\\\GNPS_substituents_legend.txt\"\n",
    "substituents_term_occurences_path = \"G:\\\\Dev\\\\Data\\\\Substituent Terms Occurences\\\\substituents_terms_occurences.txt\"\n",
    "variables = [\"GNPS ALL Substituents\"]\n",
    "data = []\n",
    "substituent_occurences_dict = {}\n",
    "\n",
    "# to calculate occurences\n",
    "with open(substituents_legend_path, 'r') as f:\n",
    "    content = f.readlines()\n",
    "\n",
    "with open(substituents_term_occurences_path, 'r') as f:\n",
    "    for line in f:\n",
    "        substituent, occurences = line.split(\"\\t\")\n",
    "        substituent_occurences_dict[substituent] = int(float(occurences[:-1]))\n",
    "\n",
    "for i in range(10):\n",
    "    filepath = path + variables[0] + \" \" + str(i) + \".txt\"\n",
    "    stats_one = np.loadtxt(filepath, dtype=float)\n",
    "    print(stats_one[:, 2])\n",
    "    data.append(stats_one[:, 2])\n",
    "\n",
    "print(\"Average\")\n",
    "result = np.mean(data, axis=0)\n",
    "with open(process_average_substituent_path, 'w') as f:\n",
    "    for index, probability in enumerate(result):\n",
    "        f.write(content[index][:-1] + \"\\t\" + str(probability) + \"\\t\" + str(substituent_occurences_dict[content[index][:-1]]) + \"%\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving model to h5 format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "filepath = datapath+os.sep+'models' + os.sep + 'AUC0.7ANDAUC0.6OCC0.5/saved_substituents_classifier.h5'\n",
    "# filepath = \"G:\\\\Dev\\\\Data\\\\saved_substituents_classifier_after_parameters_400e_model.h5\"\n",
    "enc.save(filepath)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

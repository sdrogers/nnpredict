{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.layers import Input, Dense, Lambda, concatenate, Conv1D, Concatenate, Flatten, MaxPooling1D\n",
    "from keras.models import Model\n",
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file_dir = \"G:\\\\Dev\\\\trees_gnps\"\n",
    "formula_occurences_path = \"G:\\\\Dev\\\\fragment_occurences.csv\"\n",
    "saved_intensities_path = \"G:\\\\Dev\\\\saved_intensities.csv\"\n",
    "fingerprints_names_path = \"G:\\\\Dev\\\\Data\\\\1000\\\\GNPS Python Master\\\\Fingerprint Legend.txt\"\n",
    "formula_occurences = {}\n",
    "num_formula = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4616\n"
     ]
    }
   ],
   "source": [
    "with open(formula_occurences_path, 'r') as f:\n",
    "    content = f.readlines()\n",
    "    num_formula = len(content)\n",
    "    \n",
    "print(num_formula)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filenames = [file[:-5] for file in os.listdir(json_file_dir)]\n",
    "\n",
    "formula = [formula.split(\",\")[0] for formula in content]\n",
    "intensities = pd.DataFrame(0.0, index = filenames, columns=formula, dtype=float)\n",
    "\n",
    "for index, file in enumerate(filenames):\n",
    "    filepath = os.path.join(json_file_dir, file + \".json\")\n",
    "    with open(filepath, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "        for fragment in data['fragments']:\n",
    "            if fragment[\"molecularFormula\"] in formula and \"intensity\" in fragment:\n",
    "                intensities.at[file, fragment[\"molecularFormula\"]] = float(fragment[\"intensity\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intensities.sort_index(inplace=True)\n",
    "train, validate, test = np.split(intensities.sample(frac=1), [int(.6*len(intensities)), int(.8*len(intensities))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intensities.to_csv(saved_intensities_path, encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_intensities = pd.read_csv(saved_intensities_path, sep=\",\")\n",
    "saved_intensities.index = filenames\n",
    "print(saved_intensities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a master file containing CDK fingerprints for all molecules.\n",
    "# Each molecules CDK bit set is added as a 320 element array to a Pandas dataframe.\n",
    "def load_fingerprints_master(path, number_of_rows=0):\n",
    "    BITS = 307  # Total number of bits in fingerprint\n",
    "\n",
    "    fp_all = np.loadtxt(path, dtype=\"U25\", skiprows=number_of_rows) # Get master file as numpy array of Strings\n",
    "    fp_ids = np.unique(fp_all[:, 0]) # Trim duplicate filename rows, store unique filenames\n",
    "\n",
    "    # Construct empty Pandas dataframe of correct size.\n",
    "    # Number of rows is equal to the number of unique molecules (found in fp_ids).\n",
    "    fingerprints = pd.DataFrame(0, index = fp_ids, columns=range(BITS), dtype=int)\n",
    "\n",
    "    # Populate the dataframe using each molecule's filename to place data in the correct row.\n",
    "    for row in fp_all:\n",
    "        fingerprints.at[row[0], int(row[1])] = int(row[2])\n",
    "\n",
    "    # Convert populated dataframe into a numpy array for use as output by neural networks.\n",
    "    return fingerprints\n",
    "\n",
    "# Load the names of all substructures included in the CDK fingerprint in the correct order\n",
    "# This is used for boxplots, when performance metrics for individual substructures are calculated.\n",
    "def load_fingerprint_legend():\n",
    "    fingerprint_legend = []\n",
    "    # Open file containing substructure names.\n",
    "    with open(fingerprints_names_path, 'r') as f:\n",
    "        # Add each name to the list of substructure names.\n",
    "        lines = list(islice(f, 0, None))\n",
    "        for line in lines:\n",
    "            fingerprint_legend.append(line)\n",
    "    return fingerprint_legend\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model,Sequential\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.optimizers import SGD\n",
    "    \n",
    "def baseline_model(x_train_formula, x_train_fingerprints):\n",
    "    class_model = Sequential()\n",
    "    class_model.add(Dense(2000, input_dim=x_train_formula.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "    class_model.add(Dense(1000,kernel_initializer='normal',activation = 'relu'))\n",
    "    class_model.add(Dense(500,kernel_initializer='normal',activation = 'relu'))\n",
    "    class_model.add(Dense(x_train_fingerprints.shape[1],kernel_initializer='normal',activation = 'sigmoid'))\n",
    "    class_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return class_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes as input a trained neural network model and extracts its history variable.\n",
    "# It then uses it to graph the model's loss and validation loss over the training epochs\n",
    "# The epochs paramter is used for plotting the x axis.\n",
    "def plot_loss(fitted_model, epochs):\n",
    "    # Extract loss values for the training and validation sets.\n",
    "    loss = fitted_model.history['loss']\n",
    "    val_loss = fitted_model.history['val_loss']\n",
    "    # Create x axis variables.\n",
    "    epochs_label = epochs\n",
    "    epochs = range(epochs)\n",
    "\n",
    "    #Plot both losses.\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "    plt.plot(epochs, val_loss,'r', label='Validation loss')\n",
    "    plt.title('Training and Validation Loss for ' + str(epochs_label) + ' epochs')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Takes a trained autoencoder model and input data. Uses the model to predit on the provided data\n",
    "# It then plots the input data vs. the model's prediction for a specified sample in the data.\n",
    "def plot_input_output(auto_model, data, sample=0):\n",
    "    # set font size\n",
    "    matplotlib.rcParams.update({'font.size': 14})\n",
    "    # Use model to make predictions for the provided input data.\n",
    "    decoded_data = auto_model.predict(data)\n",
    "    # Create plot\n",
    "    fig, ax = plt.subplots()\n",
    "    for i, d in enumerate(data[sample]):\n",
    "        # For each datapoint in the specified sample, plot a vertical line equal to its intensity value.\n",
    "        if d>0.1:\n",
    "            ax.plot([i,i],[0, d], color='g')\n",
    "        # Do the same for the corresponding prediction but using negative values, to create a \"mirror\" plot\n",
    "        if decoded_data[sample][i]>0.1:\n",
    "            ax.plot([i,i],[0,-decoded_data[sample][i]], color='r')\n",
    "    # Plot invisible line on far end of spectrum so all 1000 mass bins are shown even if absent.\n",
    "    ax.plot([999,999], [0,0])\n",
    "    ax.set_xlabel(\"Mass Bin(Da)\")\n",
    "    ax.set_ylabel(\"Relative Abundance\")\n",
    "    plt.show()\n",
    "    \n",
    "# Takes actual and predicted fingerprint bit sets and plots a representative graph\n",
    "# Similar to the fragment spectrum plotting.\n",
    "def plot_fingerprint_output(actual, predicted, sample=0):\n",
    "    fig, ax = plt.subplots()\n",
    "    for i, d in enumerate(actual[sample]):\n",
    "        # For each substructure in fingerprint, plot a vertical line if it is present.\n",
    "        ax.plot([i,i],[0,d], color='g')\n",
    "        # DO the same for prediction, using the predicted probability of the susbtructure's presence.\n",
    "        ax.plot([i,i],[0, -predicted[sample][i]], color = 'r')\n",
    "    # Plot invisible line on far end of fingerprint.\n",
    "    ax.plot([307,307], [0,0])\n",
    "    ax.set_xlabel(\"Substructure (Index in CDK)\")\n",
    "    ax.set_ylabel(\"Probability of Presence\")\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "# Takes a actual and predicted fingerprint values and computes the area under the Roc curve for each substructure.\n",
    "# For each subtructure, also calculates AUC when the actual values are scrambled.\n",
    "# Return two numpy arrays: one containing AUC metrics for all susbtructures, one containing each permutation's\n",
    "# AUC scores for each susbtructure.\n",
    "def compute_auc(bits, true, pred, permutations=500):\n",
    "    num_permutations = permutations  # Number of permutations to compute AUC scores for. \n",
    "    \n",
    "    # Create structured array to hold statistics for each fingerprint.\n",
    "    dtype = [('fp_id', int),('nonzeros', int), ('auc', float), ('auc_percent', float)]\n",
    "    mol_stats = np.zeros((bits,), dtype=dtype)\n",
    "    # Create array to hold permutation AUC scores for plotting.\n",
    "    perm_scores = np.zeros((bits, num_permutations))\n",
    "    val_start_index = 0\n",
    "    for fp_id in range(true.shape[1]): # For every substructure      \n",
    "        nonzero_vals = np.count_nonzero(true[val_start_index:, fp_id]) # Count number of nonzero values\n",
    "        if nonzero_vals > 0 and nonzero_vals < true[val_start_index:, fp_id].size:  # If there are no 1s or no 0s, can't compute.\n",
    "            # Compute actual AUC score using only the validation fraction of the dataset.\n",
    "            fp_true = true[val_start_index:, fp_id]\n",
    "            fp_pred = pred[val_start_index:, fp_id]\n",
    "            score = metrics.roc_auc_score(fp_true, fp_pred)\n",
    "\n",
    "            # Compute AUC scores for permutations and compare to actual.\n",
    "            counter = 0         \n",
    "            for i in range(num_permutations):\n",
    "                permutation = np.random.permutation(fp_true)\n",
    "                perm_score = metrics.roc_auc_score(permutation, fp_pred)\n",
    "                perm_scores[fp_id, i] = perm_score\n",
    "                # Count how many permutations have a higer AUC score than actual data.\n",
    "                if perm_score >= score:\n",
    "                    counter = counter + 1\n",
    "            # Calculate % of scrambled values with higher AUC score than actual AUC\n",
    "            percentage = (counter/num_permutations)*100\n",
    "        # Update structured array with data or non values if no AUC could be calculated.\n",
    "            mol_stats[fp_id] = fp_id, nonzero_vals, score, percentage\n",
    "        else:\n",
    "            mol_stats[fp_id] = (fp_id, nonzero_vals, 0, 100)\n",
    "        \n",
    "    # Permutations take a while, print statement to say when finished.\n",
    "    print(\"Done\")\n",
    "    return mol_stats, perm_scores\n",
    "\n",
    "\n",
    "# Given the AUC statistics derived from two separate models, it comapres the two models' performance\n",
    "# Creates a bar chart comparing substructures above an AUC threshold and draws boxplots for each model's best and worst\n",
    "# performing substructures.\n",
    "# Usually compares an experimental model's AUC to a baseline (e.g. the basic fingerprint encoder)\n",
    "def evaluate(base_stats, base_perm_scores, exp_stats, exp_perm_scores, names):\n",
    "    # Sort molecules in ascending order of baseline AUC score, keeping only molecules with AUC scores above 0.5\n",
    "    normal_auc = np.where((base_stats['auc'] > 0.5))\n",
    "    abnormal_auc = np.where((base_stats['auc']) < 0.5)\n",
    "    ordered_base = np.sort(base_stats[normal_auc], order='auc', axis=0)[::-1]\n",
    "    \n",
    "    # Take top 30 and bottom 5 substructures by AUC score to use for boxplots.\n",
    "    sample_fps = ordered_base[:30]\n",
    "    sample_fps = np.append(sample_fps, ordered_base[-5:])\n",
    "    \n",
    "    # Plot number of substructures with AUC scores above 0.7 and above 0.5 for both data sets\n",
    "    base_above_07 = len(np.where((base_stats['auc'] >= 0.7))[0])\n",
    "    exp_above_07 = len(np.where((exp_stats['auc'] >= 0.7))[0])\n",
    "    base_above_05 = len(np.where((base_stats['auc'] >= 0.5))[0])\n",
    "    exp_above_05 = len(np.where((exp_stats['auc'] >= 0.5))[0])\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    index = np.arange(2)\n",
    "    bar_width = 0.35\n",
    "    opacity = 0.5\n",
    "    ax.bar(index, (base_above_05, base_above_07), bar_width, alpha=opacity, color='b', label='Baseline')\n",
    "    ax.bar(index+bar_width, (exp_above_05, exp_above_07), bar_width, alpha=opacity, color='r', label='Experiment')\n",
    "    \n",
    "    ax.set_xlabel('AUC Threshold')\n",
    "    ax.set_ylabel('Number of Substructures')\n",
    "    ax.set_title('AUC Score Comparison')\n",
    "    ax.set_xticks(index + bar_width / 2)\n",
    "    ax.set_xticklabels(('Above 0.5', 'Above 0.7'))\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Boxplots of sample substructures for both data sets\n",
    "    boxplots(base_stats, base_perm_scores, sample_fps, names)\n",
    "    boxplots(exp_stats, exp_perm_scores, sample_fps, names)\n",
    "    tandem_boxplots(base_stats, base_perm_scores, exp_stats, sample_fps, names)\n",
    "    \n",
    "     # Sort molecules in ascending order of experimental AUC score, keeping only molecules with AUC scores above 0.5\n",
    "    normal_auc = np.where((exp_stats['auc'] > 0.5))\n",
    "    abnormal_auc = np.where((exp_stats['auc']) < 0.5)\n",
    "    ordered_exp = np.sort(exp_stats[normal_auc], order='auc', axis=0)[::-1]\n",
    "    \n",
    "    # Take top 30 and bottom 5 substructures by AUC score to use for boxplots.\n",
    "    sample_fps = ordered_exp[:30]\n",
    "    sample_fps = np.append(sample_fps, ordered_exp[-5:])\n",
    "    \n",
    "    boxplots(base_stats, base_perm_scores, sample_fps, names)\n",
    "    boxplots(exp_stats, exp_perm_scores, sample_fps, names)\n",
    "    tandem_boxplots(base_stats, exp_perm_scores, exp_stats, sample_fps, names)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fingerprints_path = \"G:\\\\Dev\\\\Data\\\\1000\\\\GNPS Python Master\\\\Final Fingerprints.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fingerprints = load_fingerprints_master(fingerprints_path)\n",
    "fingerprints.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "x_train_formula = np.log(train.values+1)\n",
    "x_validate_formula = np.log(validate.values+1)\n",
    "\n",
    "x_train_fingerprints = fingerprints[fingerprints.index.isin(train.index)].values\n",
    "x_validate_fingerprints = fingerprints[fingerprints.index.isin(validate.index)].values\n",
    "\n",
    "N,M = x_train_formula.shape\n",
    "shuffle_order = np.random.permutation(N)\n",
    "\n",
    "print(x_train_formula.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = baseline_model(x_train_formula, x_train_fingerprints)\n",
    "history = mod.fit(x_train_formula[shuffle_order,:],x_train_fingerprints[shuffle_order],epochs=epochs,validation_data=(x_validate_formula,x_validate_fingerprints),verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = fingerprints[fingerprints.index.isin(test.index)].values\n",
    "predicted = mod.predict(np.log(test.values+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_stats, baseline_perm_scores = compute_auc(307, actual, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fingerprint_names = load_fingerprint_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(baseline_stats, baseline_perm_scores, baseline_stats, baseline_perm_scores, fingerprint_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(baseline_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
